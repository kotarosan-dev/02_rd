実務で役立つ

ログ 教科書

基礎知識から収集方法 分析手法 トラブルシューティング・ パフォーマンス最適化・機械学習での活用まで

増井 敏克

マルウェア感染

内部不正対策 パフォーマンス解析 顧客分析 フォレンジック

この1冊ですべてわかる 武雄市図書館 決定版 112947544

知 バッグログ バケットキャプチャ セスログ | 統合ログ管理ツール Fluenta Logstash | 相関分析 ディング | インデックス | RFM分析数| AIログ分析 etc.

SHOEISHA

こんな人におすすめ! サーバー管理者 セキュリティ担当者 情報システム担当者 Webサイト運営者 etc.

SE

ログを読み解けば システムの裏側が見える。 基礎知識やしくみから 設定ツールまでを網羅!

こんな人におすすめ

ログをきちんと学んだことのない人 各種ログの基本的な知識を整理したい人 システムの不安定な挙動の原因を特定したい人 ログの分析結果をビジネスに活かせていない人 日常の運用で異常な兆候を見逃したくない人 効率的なログの検索テクニックを身につけたい人 コンプライアンスや監査対応を強化したい人 最新のログ分析技術に興味がある人

実務で役立つ

ログの

教科書

増井 敏克

基礎知識から収集方法 分析手法 トラブルシューティング・ パフォーマンス最適化・機械学習での活用まで

SE

SHOESHA

はじめに

パソコンやスマートフォンを日常的に使っている私たちは、普段、 ログが出力されていることをあまり意識しません。 しかし、 実際に はアプリの起動や終了、 さまざまな操作のたびに、目に見えないと ころで多くのログが記録されています。 これらのログは、 システム がこっそりと書き残している 「日記」 のようなもので、普段は目立 たない存在ですが、 システムの安定運用や問題解決において重要な 役割を果たしています。

ログは、コンピュータが動作している間に起こった出来事を時系列 で記録したものです。 たとえば、パソコンがどのように操作されたのか、 Web サイトに誰がいつアクセスしたのか、 データベースがどのような命令を受けて 動作したのかといった情報がログとして保存されています。 一見するとただの 文字の羅列に見えるかもしれませんが、 その中にはシステムの状態や動作を詳 細に知るための貴重なヒントが詰まっています。

ログが特に必要とされるのは、 なんらかのトラブルが発生したときです。 た とえば、 突然パソコンが動かなくなったり、 Webアプリケーションが利用でき なくなったりした場合、 原因を調査するためにログは欠かせません。 ログを確 認することで、どのタイミングでどのようなエラーが発生したのか、 どんな操 作が問題の引き金になったのかを特定できます。 もしログがなければ、原因の 特定は非常に困難となり、 復旧にかかる時間も大幅に増えてしまうでしょう。

ログは1種類だけでなく、複数の種類があります。 システム内部の動作を記 録する 「システムログ｣、 Webサイトへのアクセス状況を記録する 「アクセス ログ」、 データベースの操作内容を記録する 「クエリログ」 などが代表例です。 それぞれが異なる視点からシステムの様子を記録しており、これら複数のログ を組み合わせて分析することで、より正確かつ多角的にシステムの状態を把握 できます。

このようにログは、 開発者やシステム管理者にとって欠かせない情報源です。 システムを設計・構築する際には、どのようなログを、 どのタイミングで記録 するかをあらかじめ計画しておく必要があります。 ログが少なすぎるのも問題 ですが、 ログを出しすぎると情報の管理や分析が煩雑になり、かえって運用が

2

難しくなることもあります。 そのため、 必要な情報を適切に選び、 使いやすい 形で記録することが求められます。

さらに、ログはトラブル対応だけでなく、 日々のシステム運用やサービス改 善にも役立ちます。 定期的にログをチェックし、 分析することでシステムのパ フォーマンスを向上させたり、 利用者の行動を把握してサービスの質を高めた りできます。 たとえばアクセスログを分析すれば、 どのページがよく見られて いるのか、どのような操作が多いのかがわかり、 ユーザーにとってより使いや すいサービスの設計に役立ちます。

また、セキュリティの観点からもログは不可欠です。 たとえば、 不正アクセ スの検知や監査のために、適切にログが記録されていることが求められていま す。 ログが適切に管理されていれば、 万が一インシデントが発生しても迅速に 対応でき、 被害を最小限に抑えられます。

本書では、まず第1章でログの基礎知識を丁寧に解説します。 続く第2章では ログの記録と収集方法、 第3章ではログの分析手法について触れています。 さ らに第4章ではセキュリティ面からのログ活用、 第5章ではビジネス面でのログ の役割と活用例を紹介します。 そして最後の第6章では、AI技術を活用したロ グ分析の最新動向と応用について解説しています。

ログはシステムの 「日記」 であり、 システムが発する 「声」 であり、 時には 「証言者」としての役割も果たします。 その声に耳を傾け、 ログを正しく読み解 くことで、 システムの調子を把握し、 問題を早期に発見し、よりよいシステム 運用を実現できます。 ログを通じてシステムの裏側にある 「見えない世界」に 目を向けることは、 ITエンジニアとしての視野を広げる大きなチャンスでもあ ります。

本書が皆さまのスキルアップや仕事の充実に少しでも役立つことを願ってい ます。

2025年11月 増井敏克

3

CONTENTS

はじめに

読者特典データのご案内

ログの基礎知識

1-1 ログの役割

2

8

ログに出力する内容

■ システム管理におけるログの使用

セキュリティにおけるログの使用

ビジネスにおけるログの使用

1-2 ログの種類

パソコンに記録されているログ

■ Web サーバーに記録されるログ

22222332 235 10 10 20 26 29

データベースに記録されるログ その他のログ

1-3 ログの取得

37

優先順位を意識して取得する パソコンによるログの取得方法 ■ ネットワークでのログの取得 ■ WebビーコンやWebセンサーによるログの取得方法

39

42

44 #

56

第2章

ログの記録と収集

2-1 | ログの収集方法、 ツールの紹介

■ログの出力

Fluentd Logstash.

47

64

■プログラムからログを出力する

ログ収集・管理ツールの種類

64

70

NA

72

77

44

222222 92 92 96

87

2-2 インポートとエクスポート

■インポート

■エクスポート

2-3 | エージェント型とエージェントレス型

■エージェントの基本概念と役割

Zabbix

Nagios

■ログを長く保存する

87

90

2-4 | ログの保存と圧縮、 アーカイブ

103

111

■ ログ圧縮とアーカイブ

ログの閲覧や改ざんを防ぐ

3-2 | 主要な分析手法

111

115

117

124

■ grepやテキストエディタでの検索 124 ■高速に検索するための準備 133 ログ検索ツールの具体例 136

151

■ 異常検知の考え方

162

162

第

ログの分析手法

3-1 | ログの検索

章

■集計による概要の把握 ■ フィルタリングによるログの抽出 ■直感的な把握につながる可視化 ■複数の分析手法の組み合わせ 151 154 156 159

3-3 異常検知とフォレンジック

■ フォレンジックの考え方

164

第4章 4

セキュリティ面での活用方法

4-1| モニタリング、 アラートの発出

168

緊急性の高い事案のリアルタイム検知 168 ■ システム間の連鎖障害を防ぐログの相関分析 ■根本原因分析による再発防止策の検討 174 177

4-2 セキュリティインシデントの検出

180

■不審なログイン試行やアクセスを自動検出する ■ ログによるマルウェア感染の兆候の発見

4-3 | 内部不正対策

180

185

191

■社内システムの操作ログから異常な操作を検知する ログの改ざんを検出し、 証拠を保全する

191

196

第5章

ビジネス面での活用方法

5-1 アクセスログの解析

■ アクセス解析に使われる指標 ■ アクセスログを使った高度な分析

200

5-2 ■必要なデータを計測する

200

パフォーマンスの最適化

208

225

パフォーマンスの改善策を実施する

5-3 顧客の行動分析

第

機械学習の基礎知識

225

顧客行動分析とは何か?

233

239

顧客の行動分析における課題

6 AIによるログ分析

239

242

6-1 | 機械学習にて未知の攻撃パターンを検出する

246

246

257

266

■代表的なモデルとアルゴリズム

6-2 | 障害予測と根本原因分析への応用

■ 根本原因分析とは?

268

障害予測とは?

6-3| 行動分析とパターン認識

266

272

利用者の行動を把握する

6-4| AI ログ分析の自動化

■ログ分析自動化の目的

272

■AIログ分析の課題と対策

276

279

276

COLUMN log という単語 ライフログ 14 32 ビーコンとは 59 ログを出力するときに意識すること 71 ITAM, ITSM, ITOM 72 Logstashgrok] 86 バックアップと監査のためのデータ出力の自動化 91 Prometheus Grafana 110 コンプライアンスを意識したログ管理 122 日時での検索 131 オープンソースか商用ツールか 150 生成AIを用いたログの分析 166 相関分析を実施するための注意点 177 Web ブラウザ側での改善 235 ボットによるアクセスを除外する 244

あとがき INDEX

282

283

読者特典データのご案内

本書の読者限定!

実務にさらに役立つ操作動画を特別提供いたします。 ・Linux環境を準備する ・Windows環境でのログを確認する

本書読者の皆様に、操作の具体的な流れがわかる動画を提供いたします。 一連の流れが 動画で確認できますので、ぜひ参考にしてみてください。

読者特典データは、以下のサイトからダウンロードして入手いただけます。

https://www.shoeisha.co.jp/book/present/9784798192123

読者特典データのファイルは圧縮されています。 ダウンロードしたファイルをダブル クリックすると、 ファイルが解凍され、 利用いただけます。

●注意

※読者特典データのダウンロードには、SHOEISHA iD (翔泳社が運営する無料の会員制度) への会員登録 が必要です。 詳しくは、Webサイトをご覧ください。

※読者特典データに関する権利は著者および株式会社翔泳社が所有しています。 許可なく配布したり、Web サイトに転載することはできません。

読者特典データの提供は予告なく終了することがあります。あらかじめご了承ください。

※図書館利用者の方もダウンロード可能です。

@免責事項

※読者特典データの記載内容は、2025年9月現在の法令等に基づいています。

※読者特典データに記載されたURL等は予告なく変更される場合があります。

読者特典データの提供にあたっては正確な記述につとめましたが、 著者や出版社などのいずれも、その内 容に対してなんらかの保証をするものではなく、内容やサンプルに基づくいかなる運用結果に関してもい っさいの責任を負いません。

読者特典データに記載されている会社名、製品名はそれぞれ各社の商標および登録商標です。

第

1 ログの基礎知識

章

システムやアプリケーションを運用・保守するときに 「ログ」 を 取得し、 管理することは欠かせません。 このログにどのような役 割や種類があるのか、 どうやって取得すればよいのかについて解 説します。

ログの役割

役立つログに出力されている内容を知りたい ログが必要になる場面を知りたい セキュリティやビジネスでの活用方法を知りたい

のはこんなとき

まずはログがなぜ必要なのか、その理由や役割について考えることから始め ましょう。 普段ログの存在を意識することはあまりありませんが、 ログがない と困ることがあるのです。

ログに出力する内容

私たちは仕事で打ち合わせをすると議事録を作成しますし、 何か学ぶときに はノートに記録します。 経理担当者は簿記として入出金を記録しますし、 病院 では診察をしたときにカルテとして残します。 プライベートで普段から日記を 書いている人も多いでしょう。

個人では記録を残さなくても、会社員であれば組織として記録をつけること を求められることがあります。 なぜこのような記録を残すのか、 そしてログと の関係について考えてみましょう。

記録として残す意味

多くの人が作業内容などの記録を残す目的として、次のようなことが挙げら れます。

10

あとから振り返り、 改善するため

・思考を整理し、 新たな発想につなげるため

●自分が持つ知識を第三者と共有するため

作業の進捗状況を把握するため

● トラブル時に証拠として残すため

統計的な処理をするため

自分が忘れたときに備えるため

問題が起きたときに原因を調査するため

コンピュータで動作するプログラムについても、似たようなことを意識して 作業内容を記録することを考えます。 たとえば、パソコンを使っているときに、 いつ、どのソフトウェアを使ったのか、 どのファイルを開いたのかなどの情報 が記録されていると、あとからその履歴を調べられます。

このため、プログラムの開発者は、そのプログラムが実行中にある処理が動 作した経緯や、その状況についての情報を記録することを考えています。 この ような記録をログ (log) といいます。 つまり、 ログとはコンピュータに対す る操作や動作状況、 ネットワークの通信履歴などを指します (図1-1)。

図1-1 ログ

USBメモリの 接続 エラーの 発生 アクセスの 記錄 操作 通信 ログイン ファイルの アプリの コピー 攻撃の 検知 起動

I

-1 ログの役割

1

11

どのタイミングで、 どのような項目をログとして出力するかはプログラムの開 発者が決めているため、その内容や粒度はプログラムによって異なります。 し かし、トラブルが発生したときに、その原因を解明するために記録を残そうと いう考えは同じです。 「いつ」 「どこで」 「誰が」 「何を」 操作したのかわからな いと、プログラムが動いていたときの状況がつかめませんし、 同じ問題が再発 するかもしれないからです。

このような背景があり、 プログラムの開発者の多くはあらゆる操作に対して ログを出力できるようにしています。 パソコンの起動や終了、 ソフトウェアの 起動や終了、 ファイルの操作、 キーボードやマウスの接続、キー入力、マウス 操作、ネットワークでの通信の発生など、 例を挙げるとキリがありません。

そして、これはパソコンだけではありません。 スマートフォンやタブレット 端末、ネットワーク上のルーターやハブ、 loT機器、 サーバーなどあらゆる電 子機器でログを出力しています。 当然、テレビやエアコン、冷蔵庫などの家電、 街中で見かける自動改札や自動販売機、 信号機など、 あらゆる機器がログを出 力しているでしょう。 一般の利用者がそのログを目にすることは基本的にあり ませんが、それぞれに膨大なログが出力されています。

トラブル対応以外のログの目的

問題が発生したときには、 管理者がログを調べることで原因を調査できます。 パソコンやスマートフォンであれば、ログが保存されている場所さえわかれば、 利用者がログを確認することもできます。

しかし、ログは問題が発生したときに出力されるだけではありません。 正常 時にも「問題なく動作していること」を示すログを出力しておかないと、どの ような状況が正常なのかを判断できないからです。

そして、この「問題なく動作していること」 を示すログを他の目的で使うこ ともあります。 たとえば、パソコンやスマートフォンを使って利用者がWeb ブ

1 ログを出力するために設定が必要な場合もある。

1章 ログの基礎知識

m 第

12

ラウザでWebサイトを閲覧したときのことを考えましょう。 このとき、 誰がい つどのページを開いたのかがWebサーバー側にアクセスログとして記録され ています。 このアクセスログを使うと、 利用者の閲覧履歴を調べられるのです (図1-2)。

図1-2 Web サーバーのアクセスログ

接続元の 接続元の IPアドレス OS、Webブラウザ Webブラウザ Web サーバー リクエスト 直前に 閲覧していた ページ アクセス日時 URL

閲覧履歴から利用者の行動を把握できるのであれば、それをマーケティング などに活かせないかと考える人がいます。 つまり、 アクセスログはデータ分析 にも活用されています。

Web サイトのアクセス解析では、 Web サーバーに残ったアクセスログを使 って、どのページから入ってきて、どのページで離脱したか、どの地域の人が どれくらいアクセスしたかなどさまざまな分析が行われます。 その分析結果は、 人が興味を持つようなページ作りや、見せたいページへ人をどのように導いて いくか、いわゆる導線の張り方を考える材料などに用いられたりします。 このように、ログは大きく分けて次の3つの目的で使われています。

1) システム管理: システムの利用状況を調べる、 トラブルの原因を調べる 2) セキュリティ 攻撃の兆候を調べる、 不正を抑止する 3) ビジネス: 利用状況を調べる

この1と2は主にITエンジニアが利用しますが、 3については営業やマーケテ

13

-

1-1 ログの役割

ィングを担当する部署が利用することが多いものです。 ITエンジニアはトラブ ルに備える目的でログを出力していても、ビジネスの視点では利用状況を把握 するために使うなど、 その目的があとから変わることもあります。

COLUMN log という単語

そくてい ログは英語で 「log」 という単語です。 この単語には「ログハウス」のよ うに丸太や材木を表す意味のほか、 「測程儀」 という船の速度を測る器具や、 「航海 (航路) 日誌」 という意味もあります。

昔は船の速度を測るとき、 木片を海に浮かべてその通過にかかった時間を 測ったり、船から紐を垂らして一定の時間内に流れ出た紐の量で船の速度を 測定したそうです。 紐には一定の間隔で結び目をつけていたことから、 日本 語で 「結び目」 という意味がある 「ノット」 という言葉が船の速度の単位と して使われています。 そして、この木片が丸太であり、 航海日誌 (logbook) という言葉にもログが使われています。

このように、単に記録 (record) ではなく、時間経過を表す継続的な記録 を指すときはログという言葉が使われます。 GPSもない時代には安全な航海 において航海日誌が貴重な情報だったことがうかがえます。 なお、個人的 な日記を継続的に記録するときは journal という言葉が使われることもあり、 コンピュータでのログの記録にも 「journal」 が使われることが増えています。

コログに何が出力されているか

開発者がログを出力するときに考えていることは、ログを見ることであとか ら調査できるようにすることです。 このために、 「5W1H」 の視点で記録する ことを意識していることが多いです。 「5W1H」 は次の6つの英単語の頭文字を 取ったものです。

When (いつ)

基本的にはログを出力した時刻を記録しますが、 その用途によって出力され

1

章ログの基礎知識

14

る精度は異なります。 たとえば、 Web サーバーへのアクセスログなど多くの場 合は 「秒」 まで記録するだけで十分です。 しかし、データベースでは多くの利 用者がほぼ同時に更新する可能性もあるため、 処理の順序を厳密に把握するた め 「ミリ秒」まで記録することもあります。 また、 プログラムの処理時間を測 定する目的や、リアルタイム性が求められるゲームでの記録であれば 「マイク 口秒」 まで必要なこともあります。

Where (どこで)

ログがどこから出力されたのか、 その場所を特定できる内容を記録します。 Webサーバーへのアクセスログでは、そのファイルの場所を示すURLがわか りやすいでしょう。 プログラムでエラーが発生したときにエラーログを出力す るのであれば、エラーが発生した位置 (ソースコードの行番号など) が出力さ れます。

Who (誰が)

ログの出力が必要な操作をした人や端末などを特定できる内容を記録します。 Webアプリであればアクセスしている利用者のIPアドレスや User-Agent、ロ グインしていれば利用者のユーザーIDやセッションIDなどが考えられます。 社 内システムであれば、パソコンの端末IDやMACアドレス、 社員番号などが記 録されることもあります。 これを使って利用者の行動分析や、 不正対策、 トラ ブルへの対応などをするため、 その目的に応じて必要な情報が記録されていま す。

What (何を)

どんなデータを操作したか、どんな情報がネットワーク経由で送信されたか を記録します。 WebアプリではWebブラウザからWebサーバーへのリクエス トの内容、 デスクトップアプリであれば処理中のメモリ上に残っているデータ をそのままダンプ (加工せずに出力) することもあります。 出力する内容によ っては容量が多くなることが想定されるため、 必要な情報に限定して出力され

1

1 ログの役割

15

ます。

Why(なぜ)

ログとして出力が必要な理由を記録します。 エラーであればエラーコードや エラーメッセージ、 正常であれば問題ないことがわかるようにログの種類 (警 告、 エラー、 正常などのステータス) が出力されます。

How (どうやって)

どうやってその操作がされたかを記録します。 「どのボタンを押したか」 「ど のリンクをクリックしたか」「どのプログラムから呼び出されたか」 といった利 用者の操作を識別できるように出力します。

ログの出力時に注意すること

ログには操作の内容が細かく記録されていることがわかりました。 それなら、 ログにあらゆる情報を出力しておけばよいと感じる開発者がいるかもしれませ ん。

しかし、 形式などを考慮せずになんでもログに出力してしまうと、 さまざま な問題が発生します。 具体的にどのような問題があるのか、そしてどのような 形式で出力されていると使いやすいのかを考えます。

日付や時刻の形式

ログを出力するとき、日時の出力は必須です。 しかし、 この出力形式はプロ グラミング言語やライブラリによって異なります。 日時の標準的な形式とし てISO 8601 という世界標準の規格があり、 「20250801T123456+0900」 や 「2025-08-01T12:34:56+09:00」 といった形式が考えられます。しかし、単に 「2025/08/01 12:34:56」 のように人間が読みやすい形式で出力されることも 多いものです。 また、 Unixtimeと呼ばれる1970年1月1日0時0分0秒からの経 過秒数を使うこともあります。

第1章 ログの基礎知識

文字コード

ログは特定のソフトウェアでしか読み込めないようなバイナリ形式で出力す ることもできますが、 多くのソフトウェアで扱えるようにテキスト形式で出力 されることは多いものです。

このようなテキスト形式でログを出力するときに注意しなければならないの が文字コードです。 アルファベットや数字などASCIIコードの範囲内でログ を出力しているときはあまり気にする必要はありませんが、 日本語や記号、 絵 文字などを含むときには、 文字コードを意識しないと文字化けします。 最近は UTF-8で出力することが多いものの、 古いシステムではShift_JISやEUC-JP な どが使われることもあります。 このようなログを他のシステムと連携するとき には、 文字コードの変換が必要になります。

ログの出力先

ログを出力するディレクトリはOSやプログラムによって異なります。 この 出力先として、 OS が用意している標準的なディレクトリで管理するか、 プロ グラムで自由に変更できるようにするかを検討します。 ソフトウェアの利用者 ( システム管理者) が設定ファイルなどで指定できるような機能があれば便利で しょう。

なお、 ログの用途によって、 出力先を複数のファイルに分けることもありま す。 たとえば、アクセスログとエラーログは用途が違うため、 分割しておくと 集計や分析が便利です。

保存期間と退避

長期間にわたってログを出力すると、 それだけログの容量が大きくなり、デ ィスクの空き容量が減ります。 これにより本来のサービスに必要なデータを保 存できなくなると困ります。 このため、 ログを保存する期間を決めておきます。 組織の業務内容によっては、 内部統制などのルールにより保存期間が定められ ているログもあるため、 要件を確認することが必要です。

1 ログの役割

17

また、ログのサイズが大きすぎると解析にも時間がかかるため、一定の期間 でログを退避します。 これをローテート2やローテーションといいます。 日次 (1日1回) や月次 (月に1回) といった間隔でローテートすることが多いですが、 一定のサイズを超えたらローテートする、 数時間ごとにローテートする、 とい った方法も考えられます。

圧縮と分割

直近のデータは分析や調査に使うことが多いため、 すぐにアクセスできる場 所に保存する必要があります。 しかし、 過去のデータはそれほど使うことはな く、問題が発生したときに調査できれば十分でしょう。

ログはテキスト形式であることが多く、 圧縮することでディスクの使用量を 大幅に減らせます。 ただし、 巨大なファイルを圧縮すると、 圧縮の処理にCPU を使うことで本来のサービスに影響が出る可能性があります。 このため、ファ イルを細かく分割するか、 夜間などCPUの負荷が低い時間帯に圧縮処理を実行 します。

粒度と種類

ログが細かく出力されていると詳細な分析ができる一方で、 過剰にログを出力 するとシステムのパフォーマンス低下やストレージの圧迫につながります。こ のため、ログをどの程度の粒度で出力するのか、 種類 (正常、 情報、 警告、 エ ラーなど) をどうするのか調整します。 また、 開発環境ではすべての種類のロ グを出力し、 本番環境では警告やエラーなど重要な種類だけを出力するように 切り替える設定を用意することもあります。

ログに残してはいけない情報

あとから分析するためには細かな情報をログに出力しておくと便利ではあり

2 一定の期間が過ぎたログを定期的に他の場所に移したり消したりすること

1章 ログの基礎知識

第

18

ますが、 セキュリティの観点から考えるとログに記録するべきでない情報があ ります。 たとえば、 顧客の個人情報や企業が持つ秘密情報があります。

顧客の個人情報

顧客の住所や氏名、電話番号などの情報はデータベースに保存されているこ とが多いです。 このデータベースにアクセスしたプログラムが、 読み込んだ内 容をログに出力してしまうと、ログに個人情報が残ります。 一般に、 個人情報 の管理においてログへの出力は想定されていないため、 記録として残ってしま うことは問題です。

どのような情報を記録してよいかは組織によって異なるため、 組織のセキュ リティポリシーなどで定義を確認します。 そのうえで、必要に応じてログとし て出力するときには 「*」 や 「■｣といった記号に置き換えます。

企業が持つ秘密情報

企業の売上や人事情報、 設計図などの情報を特定の部署以外が閲覧できてし まっては問題になることがあります。 個別の値は出力しなくても、計算した結 果をデバッグ情報として出力していることがありますが、その内容によっては ログとして保存されると問題になることがあります。

開発環境のみで出力し、 本番環境では出力しないように設定するなど、 その 管理には注意が必要です。

第2章で解説するように、 他のシステムにログを転送して一元管理するよう な技術を使用している場合は、 出力されたログが複数のシステムに記録される 可能性があるため、 特に注意が必要です。

なお、ログには開発者がプログラムの動作を確認するために出力するものと、 現場の運用担当者が使うために出力するものがあります。 このため、 開発者が 独自に判断するのではなく、 運用担当者と認識を合わせておく必要があります。

ログの役割

1

19

システム管理におけるログの使用

ログに出力されている内容を、 どのような目的で使っているのかについて業 務ごとに解説します。 まずはシステム管理など、 組織におけるコンピュータを 管理する業務について考えます。

システム管理の仕事とは

従業員が使うパソコンや、 情報共有に使うサーバーなど、 さまざまなコンピ ュータを安定して稼働させるために、ハードウェアやソフトウェアを運用、管 理する仕事をシステム管理といいます。 組織で利用するIT関連の機器の導入や 設定、 運用、 保守、 障害対応、 社内インフラの整備、 従業員のIT教育など、 組 織の業務を円滑に遂行できるように管理する業務を指します (図1-3)。

図1-3 システム管理者の仕事 ネットワークの 構築 一般の システム 教育 利用者 管理者 問い合わせ 調整 端末の設定 サーバーの管理 ベンダーの 担当者

企業が使う情報システムには、組織の運営に欠かせない機密情報が含まれて いるため、適切に管理しないとセキュリティ上の問題が発生することもありま

20

1章 ログの基礎知識

す。 たとえば、 外部からの不正アクセスやウイルス感染、 内部からの情報の不 正な持ち出しなど情報漏えいにつながる事態が起こりかねません。

そこでシステム管理の業務では、問題が発生したときの調査、 解決などにロ グを使います。 問題が発生していなくても、 コンピュータの買い替えなどの時 期の判断に、CPUやメモリ、ストレージなどの使用率などの情報を使いたいこ ともあります。

トラブルシューティングにおけるログの必要性

コンピュータを使っていて、 「エラーメッセージが表示される」 「アプリケー ションが起動しない」 「キーボードの入力に反応しない」といった問題が起きた とき、 その原因を特定し、 解決に導くことをトラブルシューティングといいま す。このトラブルシューティングの作業において、 何が起きたのか原因を調べ るためには、時系列に沿って記録されているログは有効な手がかりになります。

システムが停止したタイミングの前後で出力されたログを確認すると、いつ、 どのような操作によって問題が発生したのかを把握できる可能性があります。 ロ グが出力されていないと、 曖昧な情報しか得られず、 どの時点まで問題なかっ たのかを正確に把握するのは困難です。

このとき、システム管理者の立場として、 ログにどのような情報が出力され ていればよいのかを考えます。

タイムスタンプ

問題が発生したタイミングを調べるためには、日時を表す情報が必要です。 こ れは問題が発生したときだけでなく、正常に動作していることを確認するため にも必要なので、 どのログにも必須です。

さらに重要なのは、複数のシステムが動作しているときに、時刻が同期してい ることです。 コンピュータ内部の時計が他のコンピュータとずれていると、シ ステムから出力されたログを見比べたとき、どれが先に発生したのかを把握で きません。

1

ログの役割

21

エラーメッセージ

エラーが発生したのであれば、そのエラーの内容を人が理解できるように出 力されていることはもちろん、 どのような情報が出力されているかが重要です。 たとえば、あるアプリがメモリ不足で異常終了したとします。 このとき、単に 「メモリ不足」 といってもその種類はさまざまです。 該当のアプリが大量のメモ リを使用したことによってメモリ不足に陥ったのか、 他のアプリが大量のメモ リを使用したことによってメモリ不足に陥ったのかによって、 対応は変わって くるためです。

このため、 表1-1 のような内容が出力されていると助かります。

表1-1 トラブル対応に必要な情報の例

項目 内容 システムの状態 メモリ使用量、 CPU 使用率、 ネットワークの状態など 実行された操作 利用者の操作、 内部処理など 操作した人 利用者のアカウントなど エラーが発生した場所 ソースコードにおける行数など

これらの情報を総合的に判断して、 トラブルシューティングを実施します。

ブログを確認したあとの対応

ログが出力されてさえいれば、その内容を見てすぐに対応できると思う人が いるかもしれません。 しかし、現代のシステムは複雑に組み合わさって構成さ れており、ログを確認しただけで原因を特定できるとは限りません。

このため、現象を再現することが必要です。 たとえば、 なんらかの操作をし たときに問題が発生したのであれば、 再度同じ操作を実施して、 同じ問題が発 生するか、 同じログが出力されるかを確認します。 もし同じ結果が得られれば、

-

第1章 ログの基礎知

22

その操作による問題の可能性が高いと判断できます。

たとえば、 「エラーメッセージが表示される」 「アプリケーションが起動しな 「い」といった場合は、そのタイミングの前後で出力されているログを重点的に 確認します。 そして、似たような操作を繰り返して、現象が起きるパターンや 条件を把握することから始めます。

ネットワークに関連するトラブルの場合は、パソコンのログだけでなくルー ターのログも調べる必要があります。 ログはシステムごとに出力されているた め、複数のログを統合して時系列順に並べ替える作業が必要になることもあり ます。

ログを整理できれば、そこに表示されているエラーメッセージをもとに、異 常な状態が発生したときに出力される記述を使って原因を絞り込みます。 たと えば、タイムスタンプで並べ替え、 必要に応じて検索します。 「Error」 (エラ - ) や 「Exception」 (例外) などのメッセージを含むログを検索すると、欲し い情報だけを抽出できます。

効率よく調べるために、第2章で解説するログ管理ツールを使うこともありま す。 そして、 ログを漠然と眺めるのではなく、 「なぜエラーが発生したのか?」 という疑問を持ち、 考えられる原因を頭の中に思い浮かべながら探すことが大 切です。 そして、 追加でログを収集したり、 実際の動作をシミュレーションし たりしながら、仮説を検証します。

原因を特定できれば、その問題を解消する対策を講じたり、回避する操作を 検討したりします。 その後もログを監視することで、同様のエラーが再発しな いかを確認します (1-4)。

■1-4 トラブルシューティングの流れ

「トラブルの発生 ログの確認 現象の再現 対策の実施 エラーメッセ ージの表示 ・正しく動作し ・発生時刻や 内容の確認 問題点の把 ・同じ操作で 再現するか の確認 ない 他の原因の 調查 プログラムの 修正、設定 ファイルの変 更 ・再発しない ことの確認

ログの役割

23

パフォーマンス解析におけるログの必要性

システム管理ではトラブルシューティングだけでなく、 コンピュータの買い 替えなどの根拠にするためにログを使うこともあります。 長くシステムを使っ ていると、利用者数やデータ量の増加、 システム改修によるプログラムの複雑 化などによって、 性能が低下することがあります。 このとき、 より性能のよい ハードウェアに交換することで解決する方法が考えられます。

ただし、 新しいハードウェアを購入するときも、どのような性能が必要なのか を判断しなければなりません。 たとえば、利用者から「遅い」という声があが っていたとして、それが一時的なものなのか、 恒常的なものなのかによって必 要な性能は変わります。 体感的な 「遅い」 という言葉だけでは判断できず、 上 司に報告や提案をするときには、 数値的な根拠が必要です。

そこで、速度や応答性、 信頼性を保っているかを評価するために、 パフォーマ ンス解析を実施します。 Windows には 「パフォーマンスモニター」(図1-5) や「タスクマネージャー」 (図1−6 ) macOS には「アクティビティモニタ」 (図1-7)という機能があり、リアルタイムにCPU使用率やメモリ使用量など を確認できます。

図1-5 パフォーマンスモニター

モニター ファイ スパフォーマンス X/ モニターツール パフォーマンスモニター コレクタ 100- 175344 175955 18:00:05 18.00 15 18:00.25 140035 11943 表示 カラー 39- カウンター インスタンス

第1章 ログの基礎知識

24

図1-6 タスクマネージャー

m タスクマネージャー パフォーマンス 中 プロセス + パフォーマンス 5 アプリの履歴 ? スタートアップアプリ 83 ユーザー The CPU CPU 29% 1.45GHz メモリ 7.8/157GB (50%) ディスク 0 (D) SSD (NVM) 0% ディスク1 (C:) SD (SD) 0% サービス 設定 Wi-Fi Wi-Fi 1602010 GPU 0 Intel UHD Graphics 新しいタスクを Intel(R) N100 100% 0.80 GH 有助 384KB 20MB 6.0MB

図1-7 アクティビティモニタ

アクティビティモニタ CPU メモリ エネルギー ディスク ネットワーク すべてのプロセス プロセス& % CPU- CPURE スレッド アイドル % GPU OPUR PID ユーザ Windows_ 11.4 6:05.87 20 131 Apple 48 60.77 376 windowserver Spotlight 6.1 2.12 16 Apple 0.0 0.00 714 ap kernel_ta. 48 323.27 542 1602 Apple 0.0 0.00 mediaans 3.4 35.40 " Apple 0.00 645 mamuipes アクティー 2.7 4.99 6 0.00 3647 Eagle Hel 2.5 1.53.70 16 64 Apple 0.0 6.00 1265 masuipeo window. 1.9 12.09 7 2 Apple 0.0 0.00 562 mas 1.6 1:15.99 4 0.0 0.00 414 1.4 0. 7 Арв 0.0 0.00 0.9 9.50 7 Apple 0.0 0.00 361 root 13817 AM システム) 2.61% スレッド 2,454 プロセス・ 501 ユーザ アイドル 82.87%

このようなCPU使用率やメモリ使用量、 応答時間などさまざまなデータをリ アルタイムで把握するだけでなくログに記録していると、 現状の性能を把握す る情報源となります。

ストレージの空き容量程度であれば、 その値を月単位で出力し、 グラフにす

ログの役割

25

ることで容量不足に陥る前に増設することもできるでしょう。 このくらいであ ればログを使う必要はないかもしれません。

しかし、ログを収集することで、パフォーマンスに影響を与えている原因を 特定できることもあります。 たとえば、 常にメモリ使用量が100%に近い状態 なのであれば、メモリを増設することで解消できるかもしれません。

特定のアプリケーションを利用しているときだけCPU使用率が高い状態にな るのであれば、コンピュータを買い替えても効果がなく、 アプリケーションを 改修する必要があるかもしれません。

このように、原因を特定するためにログが使われることもありますし、根本的 な問題であればコンピュータを買い替える根拠として使うこともあります。 い ずれにしても、改善策を実施したあとは、 再度ログを解析し、 変更前後でのパ フォーマンスの変化や効果を測定します。

セキュリティにおけるログの使用

セキュリティにおいては、 「事後対応」 「予兆検知」 「不正抑止」 という3つの 目的でログが使われます。 それぞれの目的について、 どのような考え方でログ を使うのかについて解説します。

事後対応

ニュースで 「不正アクセス」 や 「サイバー攻撃」 といった言葉を聞かない日 がないくらい、 多くの企業がインターネット経由でさまざまな攻撃を受けてい ます。 脆弱性を狙った攻撃のほか、 ウイルス (マルウェア) を使った攻撃が使 われることもあります。

組織が攻撃を受けたときには、どのような被害があったのかを調べる必要が あります。 一時的に攻撃が落ち着いたとしても、根本的な解決を行わなければ、 同じ手口で攻撃されて再び被害が発生します。

たとえば、脆弱性を狙って攻撃されたのであれば、 プログラムの修正プログ

第1章 ログの基礎知識

26

ラム (セキュリティパッチ) などを使って脆弱性を解消します。 ウイルスに感 染したのであれば、そのウイルスを除去しなければなりません。

また、従業員による誤操作で情報漏えいが発生した、 従業員が不正に情報を 持ち出した、といった事案が発生すると、その影響範囲を調べなければなりま せん。

いずれにしても、 被害状況を調査することから始めます。 サイバー攻撃が成 功した場合でも、 攻撃者による痕跡がシステムに残っていることが多く、 ログ を解析することが考えられます。 どのような経路を通じて侵入したのか、どの システムやデータが狙われたのかといった詳細な情報を特定できます。

たとえば、ネットワーク経由での不正アクセスが確認された場合、 攻撃者が 利用したIPアドレス、 アクセスされたファイル、 実行されたコマンドなどがロ グに残ります。 これを分析することで、攻撃者の手口を特定できます。

内部からの情報漏えいであっても、どのファイルがどのような経路で流出し たのかをログから把握できる可能性があります。

このように、攻撃を受けた、 情報漏えいが発生した、という事案のあとから ログを分析するため、 事後対応と呼ばれます。

予兆検知

攻撃者がシステムへ侵入するときは、 1度だけのアクセスで攻撃が成立する ことはほとんどなく、 さまざまな手法を試行します。 このため、 通常とは異な るポート番号へのアクセスや大量のアクセス、 不自然なログインの試行など普 段発生しないような異常が発生することが多いです。

これらはログに記録されるため、リアルタイムでログを監視していれば、速 やかに異常を検知できます。 たとえば、 短期間に大量のログイン失敗のログが 記録されると、「ブルートフォース攻撃」 が疑われます。 このときは速やかにア カウントをロックする、 アクセス制御を強化するといった対策が考えられます。

また、Web サーバーへのアクセスログを監視していると、 特定のURLへの アクセスが増えることがあります。 WordPressをはじめとした有名なフレーム

1 ログの役割

27

ワークでは、脆弱性のあるバージョンが使われていることがあり、 その脆弱性 を狙った攻撃が繰り返される可能性が高いものです。

WAF3などを導入している場合は、特定のパターンを検出するようなルール を設定し、 定期的にログをレビューすることで、 攻撃を受けるリスクを軽減で きます。 そのためには、問題が起きる前の段階で、 ログを定期的に確認するこ とが求められます。 被害が発生する前に攻撃の予兆を調べるために実施するこ とから予兆検知と呼ばれます。

これは攻撃に限った話ではありません。 普段から使っているパソコンやスマ ートフォンといった端末、 ネットワークなどから発せられるログを確認してい ると、 故障前に警告が出ていることがあります。

こういったログを定期的に解析し、エラーの兆候やパターンを早期に発見す ることで、組織全体として大規模な障害が発生する前に対策を講じることがで きます。 これにより、 システム運用の信頼性が高まり、 従業員にとっても安心 して仕事ができることを意味します。

不正抑止

会社などの組織からの情報漏えいは、外部からの攻撃によって情報が盗まれ るだけではありません。 管理ミスや誤操作による情報漏えいもありますが、 従 業員による不正な持ち出しも挙げられます。

つまり、従業員が悪意を持って組織内にある機密情報を持ち出すことがあり ます。 会社のパソコンにUSBメモリなどを接続してデータをコピーする手法の ほか、 メールなどへの添付、 オンラインストレージへのアップロードなどさま ざまな手法が使われます。

これを止める技術的な手法としてDLP4がありますが、 DLPの製品を導入す るだけですべての不正を阻止できるわけではありません。 漏えいリスクの低い

3 Web Application Firewallの略で、 Webアプリケーションへの典型的な攻撃を防ぐためのツール。 4 Data Loss Prevention の略で、機密情報のコピーや転送、 使用などのタイミングで検知して阻止する技術。

第1章 ログの基礎知識

-

28

データまで保護対象にすると、 業務効率が大幅に低下してコストばかりがかか ります。 漏えいリスクの高いデータだけに限定したとしても、漏えいの可能性 をゼロにすることはできません。

そこで、技術的な対策以外に、 ログを活用した不正抑止について考えます。つ まり、ログを監視していることを従業員に伝えることで、不正をしたときに見 つかるリスクが高いと考えて不正行為を躊躇することが想定されます。

この監視体制を伝えるだけでなく、 ログを確認した結果を通知します。 たと えば、アクセスログやファイルの操作ログを調べると、 通常の業務で大容量の ファイルを転送されている記録が発見されることがあります。 このとき、どの ような目的で操作したのかを問い合わせる、 といった対応です。 もちろん、通 常の業務内で問題ない操作なのかもしれませんが、 実際に不正につながりかね ない操作をしたときに問い合わせることで、 従業員にとっては「見られている」 という意識が働きます。 これにより、 早期に不正行為を抑制できる可能性が高 まります。

■ビジネスにおけるログの使用

最近では、「ビッグデータ」や「データサイエンス」といったキーワードが注 目されており、 膨大なデータが記録されているログがデータ分析の対象として 使われることが増えています。 これまでのようにシステムの安定稼働だけでな く、マーケティング担当者や人事担当者など、幅広い部門でログが使われるよ うになっているのです。

■ アクセス解析

システム管理者やプログラムの開発者のようなIT担当者以外がログを使う代 表的な業務として、これまでにも解説したマーケティング担当者によるアクセ ス解析があります。 企業の多くはインターネット上にWebサイトを公開してい ますが、 そのWebサイトにどの程度のアクセスがあるのかを調べるものです。

-

1

ログの役割

1

29

効率よくアクセス解析をするための 「アクセス解析ツール」がいくつか提供 されており、代表的なツールとして Google Analytics や Adobe Analytics が あります。 このようなツールの基本的な考え方は、 Web サーバーに残ったログ を分析して、マーケティング担当者が把握しやすいように表示することです。

Webサーバーに残ったログの時刻情報をもとに集計し、 時間別、 曜日別、月 別などで集計したグラフとして出力することでマーケティング担当者などはど のページへのアクセス数が多いのか、 どのような利用者が多いのか、といった 情報を確認できます (図1-8、 図1-9)。

図1-8 時間別のアクセスログの例

ページビュー

800000

700000

600000

500000

400000

300000

200000

100000

0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 B

図1-9 曜日別のアクセスログの例

ページビュー

3000000

2500000

最大值

2000000

1500000

1000000

2765046

平均值

2343329.7

500000

0

日月火水木 日曜日

金土

第1章 ログの基礎知識

30

また、IPアドレスの範囲を調べることで、 アクセス元の国名などを特定でき ることもあります。 正確な情報が得られるとは限りませんが、参考情報として 使うことはできます。

一般に、マーケティング担当者は非ITエンジニアであることがほとんどです。 このため、ログそのものを見ることはなく、グラフとして可視化されたデータ だけを使っていることが多いものです。

問い合わせ対応への活用

ログというと決まったフォーマットで記録されているデータをイメージする 人が多いですが、 最近は決まった構造ではないデータ (非構造化データ) も増 えています。 「ビッグデータ」 という言葉が 「3つのV」、つまりVolume (量)、 Velocity (速度) Variety (多様性) を表すようにも、単に量が多いだけでな く多様なデータを扱うことが求められているのです。

その代表的な例が 「チャットログ」 や 「電話応対ログ」 です。 顧客からの問 い合わせや、 電話での応対記録を残すとき、 文章としてメモのように記録して います。 これまでは、日本語のような自然言語で記録された文章は分析できず、 問い合わせの件数などを集計する程度でした。

しかし、 「自然言語処理」 という技術が発達し、 日本語や英語で書かれた文章 から、その顧客の感情を分析したり、 内容で分類したりできるようになりまし た。これにより、FAQを整理したり、 今後の対応品質の向上につなげたりする 対応が考えられます。

生成AIをはじめとしたAIの進化もあり、 応対の担当者が短時間でこれまで の履歴を把握できるようにログの内容を要約する、 といった使い方も当たり前 になるかもしれません。

労働状況の管理

人事担当者がログを使う例として、 従業員の勤務状況の把握があります。 単

-

1 ログの役割

1

1

31

に出退勤の記録だけであれば、勤怠システムなどで登録する方法が一般的です。 しかし、従業員がどのような時間に仕事をしているのかをより詳細に把握する ために、パソコンにログオン ログオフした時刻を記録する方法が考えられま す。

パソコンに残ったログを使えば、 アプリケーションごとの操作時間、 USBメ モリの接続や取り外しなどの時刻も取得できるため、 どういった作業に時間が かかってしまっているのかを把握できるようになり、 労働状況の改善にもつな がります。

Windows 11では 「Recall」 という機能が追加され、 動作要件を満たした機種 では、利用者がパソコンで表示した画面や実施した操作が記録されており、自 然言語で検索できるようになりました。 あらゆる操作が記録されていることで、 過去の作業内容を振り返ることもできるようになってきたのです。

COLUMN ライフログ

企業で使うログ以外にも、 個人の日常生活に関するあらゆるデータを記録 したものとしてライフログがあります。 食べたもの、 行ったところ、 会った 人などの行動だけでなく、 体重や体脂肪率、 心拍数のような健康情報、 感情 や気分、 目標やタスクなどあらゆることを記録していることから、 膨大なデ ータを分析できます。

初期のライフログといえば、日記や手帳によるアナログな記録が多かった ものですが、最近ではスマートフォンアプリやウェアラブル端末などを使っ たデジタルの記録が一般的です。

単に記録するだけでなく、 生活習慣の改善や、 感情の振り返り、 時間やタ スクの可視化など、 自己管理や成長のために使っている人が多いものです。

最近では、日記のように記録を作成する 「ブログ」、 映像で記録する 「 Vlog (Video blog) 」 のように 「ログ」 という言葉が使われる単語も増えています。

第1章 ログの基礎知識

32

ログの種類

役立つのはこんなとき

パソコンに記録されているログの種類を知りたい

> Webサーバーに記録されているログの種類を知りたい

データベースなどに記録されているログの種類を知りたい

ログが果たす役割について解説しましたが、 どのようなときに、どのような ログが出力されているのか、その種類と内容をシステムごとに紹介します。

パソコンに記録されているログ

ログは出力されるシステムやその出力内容などによって分類されます。 ここ では、パソコンに記録されているログを、システムログ、 アプリケーションロ グ、イベントログ、クラッシュログに分けて解説します。

システムログ

パソコンやスマートフォンなどを動作させるには、 OS (基本ソフト) が必須 です。 さまざまなアプリケーションを実行するためには、OSが動作している ことが前提です。

このOSが動作しているときに、システム全体の動作や状態を記録するため にOSが出力するログをシステムログといいます。 システムログには、OSの起 動から停止、ハードウェアの状態、 サービスの動作など多岐にわたる情報が記 録されています。 OSによってログの形式や保存場所は異なりますが、 次のよ

1-2 ログの種類

33

うな情報が記録されています (表 1-2)。

表1-2 システムログの内容

項目 内容 タイムスタンプ イベントが発生した日時 レベル 「重大」 「警告」 「詳細」 「エラー」 「情報」 のように重要度 に応じた分類 プロセスやサービス イベントを発生させたプログラムやコンポーネントの名前 の名前 エラーメッセージや コード エラーの内容やエラーを識別する番号 ユーザー システムにログインしているユーザー

なお、Windows では、レベルとして 表1-3 の内容が定義されています。

表1-3 Windows でのログのレベル

ログのレベル 概要 重大 (Critical) システム管理者が最優先で対応すべき重大なエラー。 Windows が突然停止するなどの問題 警告 (Warning) エラーではないが、注意が必要な状況を事前に警告したも の 詳細 (Verbose) 進行状況や成功などの状態 エラー (Error) 一般的なエラー (問題)の発生 情報 (Informational) 操作に対する正常な動作、 参考程度の情報

アプリケーションログ

一般の利用者はOSが起動したあと、 さまざまなアプリケーションを使用し ます。 これらのアプリケーションを実行しているときに出力されるログをアプ

第1章 ログの基礎知識

34

リケーションログといいます。 システムログがOSレベルで動作を記録するの に対し、 アプリケーションログはアプリケーション固有の処理や利用者の操作 履歴など、 アプリケーション内部の動作を記録しています。

アプリケーションログは、それぞれのアプリケーションに対する利用者の操 作などによって、予期せぬエラーなどが発生したときに出力されるため、その 原因の特定や対応策の検討に役立ちます。

イベントログ

システムログやアプリケーションログのように、重要なイベントを包括的に 記録したものをイベントログといいます。 Windows では、 システムログやア プリケーションログはイベントログに含まれます。

これは、利用者による操作やシステム状態の変化などを 「イベント」と考え たもので、 次に挙げるような内容があります。

ファイル操作のログ

パソコンに保管されたファイルに対し、 作成、 閲覧、 更新、削除といった操 作をしたログです。 誰が、 いつ、どのファイルを、 どのように操作したかがわ かるため、 不正な閲覧や書き換え、 コピーなどの記録が残ります。

外部ストレージのログ

USBメモリや外付けハードディスクなど、 パソコンに外部ストレージを接続 したログです。 接続した時刻や取り外した時刻、 読み取ったファイルなどの記 録が残ります。

印刷ログ

パソコンから書類を印刷したときのログです。 書類を印刷した利用者やファ イル名、印刷日時、印刷した端末 (コンピュータ名)、 印刷枚数などが記録され ています。 機密情報が印刷して持ち出されていないか、 必要のない印刷をして

35

2 ログの種類

1

いないかなどを確認できます。

設定変更ログ

ソフトウェアの設定を変更したときのログです。 設定変更の権限を持つ担当 者が管理者権限を変更したり、 権限のあるユーザーが設定を変更したりすると 記録されます。 設定を変更して証拠隠滅を図ろうとしても記録として残るため、 不正が発生したときの調査資料になります。

認証ログ

パソコンにログイン(ログオン)、ログアウト (ログオフ) したときのログで す。 この時刻を調べることで、コンピュータをどのくらいの時間使っていたの か、利用状況を確認できます。 ログインに失敗した回数を調べると、不正なロ グインなどの攻撃を受けている可能性も調査できます。

クラッシュログ

システムやアプリケーションが予期しない異常終了(クラッシュ)を起こし たときに生成されるログをクラッシュログといいます。 エラーが発生したとき の詳細な情報を記録しているもので、 システムや実行環境から自動的に生成さ れます。

エラーメッセージだけでなく、 メモリダンプ5や実行環境、 実行中のプロセ スの情報など、 異常終了したときの状態を詳細に記録していることが特徴です。 エラーに至るまでのプログラム内での呼び出し履歴などが記録されるため、 プ ログラムのソースコード中で、 どの部分が問題になったのか、 またその前後に どのような処理が実行されていたのかがわかります。

これをプログラムの修正などに役立てることで、問題の根本原因を特定し、再 発防止策を講じることができます。 使っているOSのバージョン、プログラム

5 実行中のメモリの中身をそのままファイルとして出力したもの。

第1章 ログの基礎知識

36

の設定内容、 ハードウェアの情報、 実行中のライブラリのバージョンなど、シ ステム環境に関する情報も出力されることが多いため、 特定の環境でしか発生 しない問題を特定する手助けになります。

Web サーバーに記録されるログ

Web サーバーは大きく分けて2種類のログを出力しています。 マーケティン グ担当者がよく使うアクセスログだけでなく、 システム担当者が使うエラーロ グです。 それぞれについて詳しく解説します。

アクセスログ

Webサイトを配置している Web サーバーに対して利用者からアクセスがあっ たときに記録されるログです。 記録されている項目として、 一般的には 表1-4 のようなものが挙げられます。

表1-4 Web サーバーのアクセスログに記録される内容

項目 内容 タイムスタンプ アクセス元のIPアドレス リクエストの種類 URL リクエストの日時 利用者のIPアドレス GET POST など パスとクエリ文字列 ステータスコード 正常、 エラーなど (表1-5) 転送量 リファラー ファイルのサイズなど リンク元のURL ユーザーエージェント 利用者のOS、Webブラウザなど

1-2 ログの種類

37

これらを記録するときの書式はWebサーバーのソフトウェアや設定によって 異なり、 NCSA combined 形式やW3C 拡張ログ形式などがよく使われます。

このアクセスログに記録されているのは、利用者が Web ブラウザでWebサイ トを閲覧したときに、 そのWebサイトを置いているWebサーバーに残るリク エストです。 これはHTMLファイルだけでなく、 画像やスタイルシート (CSS)、 JavaScriptのプログラムなどすべてのリクエストが含まれます。 当然、 Ajaxの ような仕組みを使っていて、1つのHTMLファイル内に含まれる JavaScriptの プログラムで次のリクエストを要求したときも、 それぞれのリクエストとその 応答の概要が記録されています。

いずれにしても、 リクエストされたURLが記録されているため、 どのURL に多くアクセスされているのかはわかります。 また、ユーザーエージェントを 確認すると、 どのようなOSでどのようなブラウザを使っている人が多いのか も集計できます。

ただし、表1-4 項目を見ると、 個人を特定する情報はありません。 IPアド レスやユーザーエージェントはありますが、 これだけでは同じ会社で同じOS、 同じWebブラウザを使っている利用者は識別できません。 同じ利用者がパソコ ンとスマートフォンなど異なるOS を使って同じページにアクセスすることも あります。つまり、 アクセス数はカウントできたとしても、単純なアクセスロ グだけでは訪問した人数は調べられないのです。

最近では、ユーザーエージェントの情報として固定した内容を出力する Web ブラウザもあり、 OSやWeb ブラウザのバージョンアップでも値が変わらない ことから、参考程度として使うことが多いものです。 また、 リファラーについ ても送信されないWebサイトが増えています。

このため、 多くのアクセス解析ツールはWebサーバーに残るアクセスログで はなく、JavaScriptのプログラムで独自に記録する方法を使っています。 詳し くは後述します。

6 Apache や nginx で標準的な形式。 7 IISで標準的な形式。

の基礎知識

-

38

エラーログ

利用者がリクエストを送信したものの、 そのリクエストに応答しようしたと きにWebサーバーでエラーが発生すると、 エラーログが記録されます。 代表的なエラーとして、ファイルが存在しないときの 「404 (Not Found)」

をはじめとして、 表1-5 のようなものがあります。

表1-5 代表的なエラーのステータスコード

ステータスコード 内容 301 (Moved Permanently) 要求されたファイルが恒久的に別の場所に移動した 401 (Unauthorized) 認証が必要だった 403 (Forbidden) 404 (Not Found) 500 (Internal Server Error) 503 (Service Unavailable) 要求が拒否された ファイルが見つからなかった 不具合などによりサーバー側のプログラムが動かな かった Web サーバーが過負荷で処理できなかった

このエラーログを見て、 必要な対応を実施します。 たとえば、 「404 (Not Found)」のエラーが出ているときは、そのリンク元のファイルでURLが間違 っていることがあります。 この場合は、リンク先のURLを変更します。

また、「500(Internal Server Error)」 が出ているときは、Webアプリの不 具合である可能性が高いため、 プログラムの修正が必要です。

■ データベースに記録されるログ

組織ではデータをデータベースに記録していますが、 このデータベース製品 にもさまざまなログが記録されています。 その目的に応じて形式が異なり、 求 められる管理方法も変わります。

-

2 ログの種類

1

39

トランザクションログ

データベース管理システム (DBMS) における操作 (INSERT、 UPDATE、 DELETE、 その他の変更操作) の実行履歴や、その操作による更新件数などを 記録するための仕組みがトランザクションログです。 多くの DBMSは障害が発 生したときや誤操作によるデータ消失などに備え、 復旧や復元の機能を備えて います。

特に RDBMSがトランザクションを処理するときには、 「ACID 特性」 を満た すことが求められます。 これは、表1-6 の4つの特性の頭文字を取ったもので す。

表1-6 ACID 特性

特性 日本語訳 特徵 Atomicity 原子性 完全に実行されるか、完全に実行されないかのいずれか Consistency 一貫性 データの整合性が保たれている Isolation 独立性 他のトランザクションによる影響を受けない Durability 耐久性 データが永続的に保存される

これにより、 複数のトランザクションが並行して実行されていても、 データ の整合性の確保が保証されます。 そして、それぞれのトランザクションの開始 から終了、 そしてコミットやロールバックといった処理を記録することで、障 害が発生してもデータを保護できます。

また、システム障害や電源断、 その他の予期しない事態が発生しても、保存 されているトランザクションログを使って、 RDBMSを障害発生前の正常な状 態まで戻せます。 このため、ログに記録された操作履歴は、データの修復にお いても不可欠です。

-

1章 ログの基礎知識

第

40

監査ログ

データベースにおける操作履歴を記録しているログが監査ログです。 誰が、 い つ、どのデータにアクセスし、変更したのかを記録しています。 主に不正アク セスや誤操作の検出、 問題発生時の原因分析に使われるログで、内部監査での 証跡に使われることもあります。

監査ログは大きく次の2種類に分けられます。

● アクセス 監査ログ

誰がいつどのデータにアクセスしたかを記録したログ。 SELECT文による 読み取り操作が中心。

● 変更監査ログ

INSERT、 UPDATE DELETEなどデータの変更操作を記録したログ。

多くのRDBMS (Oracle、 SQL Server, MySQLなど) は監査ログ機能を標 準で備えており、 有効化することで監査ログを自動で取得できます。 また、 テ ーブルにトリガーを設定し、 テーブルのデータが変更されたときに監査用のテ ーブルへ操作内容を記録する方法もあります。

クエリログ

DBMSが受け取ったSQLクエリの実行履歴を記録するログをクエリログとい います。 いつ、どのようなクエリが実行されたかを記録しているため、 データ ベースのパフォーマンスの監視やトラブルシューティング、セキュリティ監査 などに役立ちます。

8 MySQL Enterprise Edition は監査ログ機能を標準で備えている。 MySQL Community Edition では含ま れていない。

41

-

2 ログの種類

1

たとえば、 インデックスの最適化やSQL文の改善をするためには、 データベ ースの応答速度の測定が必須です。 これにより、 どのクエリに問題があるのか を調査できます。 特に、設定した閾値を超えて実行したようなクエリを記録し たものをスロークエリログといいます。 このように、 パフォーマンス上の問題 があるクエリに絞って分析できると、 効率よく問題を解決できます。

また、セキュリティ面でも、 不正アクセスや不正な操作の痕跡を追跡するた めにクエリログが使われます。 誰がいつどのようなデータにアクセスしたかを 確認できるため、情報漏えいや不正利用の対象範囲を特定するときに役立ちま す。

その他のログ

パソコンやサーバー以外にも、さまざまなログが出力されています。

通信や通話のログ

ルーターなどのネットワーク機器では、 端末とサーバー間の通信内容をログ で確認できます。 通信内容だけでなく、 通信量や通信時間などが記録されてい ると、その混雑状況を把握できます。 通信エラーの内容を把握することで、サ ービス向上に役立つこともあります。

通話のログは発信や着信、 不在着信などの履歴を確認できるログです。 いつ、 誰と通話したのかが記録されるため、 電話での情報漏えいが発覚した場合でも、 通話ログを把握すれば情報流出の経路を確かめられます。

入退室ログ

建物や部屋への 「入室」 と 「退室」 の記録を時系列で管理するために、その 時間や人物を記録したログを入退室ログといいます。ICチップを搭載した社員 証やドアのセンサー、 監視カメラなどから収集した情報によって、いつ誰が入

第1章 ログの基礎知識

42

退室したのかが記録されています。

不審者の出入りや、 従業員による機密情報の持ち出しなどの不正が起きたと きのようにセキュリティ面で使われるだけでなく、 勤怠管理や業務効率化など の目的で使われることもあります。

デバッグログ

プログラムを開発するとき、 そのプログラム内で使う値を 「変数」 と呼ばれ る名前で指定できるメモリ上の領域に格納します。 プログラマが想定している 値が、 この変数に格納されているかを確認する場合、 手軽に使える方法として、 その変数の値をログや画面に出力する方法があります。

変数に格納されている値はメモリ上に記録されているだけなので目に見えま せんが、 ログや画面に出力することでプログラマが確認できるのです。 プログ ラムから標準出力に出力する方法が手軽で、 「print」 や 「printf」 といった関 数が使われるため 「print デバッグ」 などと呼ばれます。

また、プログラムがどこまで進んだのかを調べるために、 処理状況やエラー メッセージなどを出力するために使われることがあります。

このprint デバッグはすぐに結果を確認できて便利ですが、 実行中にしか確 認できないため、 基本的には開発時の簡易的なチェックとして使われます。

しかし、本番環境やパッケージシステムなどで標準出力に出力すると、 利用 者にとって不要な情報が出力されることになります。 また、 過剰に使うとパフ オーマンスが低下することにつながります。

そこで、開発者が確認するために、 ログに出力する方法があり、 デバッグログ と呼ばれています。 出力される内容は似ていますが、 解析しやすいように一定の フォーマットで出力されており、ログレベル DEBUG INFO、 WARNING) などによって出力を制御できるようにしていることが一般的です。

9 プログラムが結果を出力するときのデフォルトの出力先で、 通常はターミナルやコンソールと呼ばれる画 面上の場所を指す。

1

2 ログの種類

43

ログの取得

3

凸 役立つのはこんなとき

ログを取得するときの考え方を知りたい パソコンでのログの取得方法を知りたい ネットワークでのログの取得方法を知りたい

ログの重要性を理解でき、 ログにはさまざまな種類があることがわかったと ころで、どのようにログを取得、 管理するかを考えます。 特に問題になるのは、 ログの量が膨大であるだけでなく、 出力するシステムが多岐にわたること、そ してその形式に一貫性がないことです。

特定のシステムだけであればそれほど問題なく管理できたとしても、組織と して出力されるログを体系的に管理しようとすると、その管理体制を構築する ことは難しいものです。

優先順位を意識して取得する

ログを取得するときに考えることとして優先順位の設定があります。 すべて のログを確認することは現実的でないため、リスクの高い場所や機器を中心に ログを確認します。

ログを取得する環境の構築にはコストや工数が必要

セキュリティを担当する部署は「コスト部門」 だといわれることがあります。 これは、セキュリティを強化しても売上が増えることはなく、 コストだけが増

第1章 ログの基礎知識

44

えるためです。 もちろん、 情報漏えいなどの事案が発生すると、 膨大な損失が 発生することから重要性は理解していても、どうしてもコストだと感じる人が 多いのです。

これはログ管理でも同じです。 ログの重要性は理解していて、 どれだけ効率 よく管理しても、工夫に時間をかけるほどコストが増えます。

ログを管理するためには、大量のログを収集し保存する必要があるため、 適 切なストレージやネットワークインフラの整備が必要です。 それに加えて、ロ グの可視化や解析を行うためのツールやシステムの導入も必要です。 これらの 導入や運用といった作業には専門知識や経験が必要であり、 それに伴うコスト や工数が発生します。

また、システムの規模や複雑さによっては、ログ管理システムの構築や運用 に必要な人員やリソースが増加し、さらなるコストや工数が必要となる場合も あります。 そのため、ログ管理システムを導入する際には、これらのコストや 工数を考慮し、適切な計画と予算を立てる必要があります。

■ ログ管理だけでは最新の脅威に対応できない

セキュリティを意識した場合、ログを管理することは必須ですが、 ログを管 理しているだけでは最新の脅威には対応できません。 ログはあくまでも「過去 の出来事」 を記録しているだけで、リアルタイムに狙われている攻撃に対処す るのには限界があります。

攻撃が発生してからログを分析し、 対応を開始するまでには時間がかかるた 止め、即座に対応することはできません。 また、 ログに記録されている情報だけ で対応できるわけではなく、ログに記録されていない攻撃を検知するためには 他の対策も必要です。

さらに、攻撃者によってログが改ざんされている可能性があると、そのロ グが信用できなくなってしまいます。 このため、ログの管理はセキュリティで 重要な対策の1つであり、IDS (Intrusion Detection System; 侵入検知シス テム) によって他のセキュリティイベントをリアルタイムで検知したり、 IPS

3 ログの取得

1

45

(Intrusion Prevention System; 侵入防御システム) によって自動的に防御し たりするなど、複数の手法を組み合わせた対策が必要です。

ログの管理方法を考える

ログを運用するとき、それぞれの会社が独自のルールを作成するのは大変で す。 そこで、公的な機関などが定めたガイドラインに沿って運用する方法があ ります。

たとえば、ログ管理に関するガイドラインとして、 NIST (米国国立標準技 術研究所)が発行しているSP800-92 (Guide to Computer Security Log Management) という文書を参照する方法があります。 これは日本のIPA (独 立行政法人情報処理推進機構) が 「コンピュータセキュリティログ管理ガイド」 として日本語訳を公開 10 しています。 こういった資料を参考にする方法もある でしょう。

ただし、 ガイドラインはあくまでも参照するためのもので、 その運用は人が 考えなければなりません。 実際には難易度が高く、現実的には中小企業では対 応が難しいこともあります。

そこで、一般的にはログ管理システムを使います。 ログ管理システムを導入す ると、ログデータの収集や保管、分析やモニタリングといったことを一元管理 できます。 しかし、 複数のシステムから出力されるログを一括で管理できる統 合型のログ管理システムなど、さまざまなシステムが提供されているため、自 社に合ったものを選ばなければなりません。

これらのシステムの選定や導入方法、使い方については、第2章以降で解説 するものとして、この章では、それぞれのシステムから出力されるログを取得 する方法について紹介します。

10 https://www.ipa.go.jp/security/reports/oversea/nist/ug65p90000019cp4-att/ begoj9000000bfnm.pdf

1章 ログの基礎知識

第

46

パソコンによるログの取得方法

まずは一般的なパソコンから出力されるログを確認することについて考えま す。その中でも、OSが出力するシステムログを取得する方法について解説し ます。

多くのOSでは、システムログは専用のディレクトリやファイルに保存され ています。 そして、 記録すべきイベントが発生すると、リアルタイムに書き込 まれます。

Windows 環境でのログ

Windows では、 「イベントビューアー」 というソフトウェアが標準で用意さ れています。 イベントビューアーを開くには、スタートメニューから検索欄に 「イベントビューアー」 と入力するほか、スタートボタンを右クリックして「コ ンピュータの管理」 を開き、 「システムツール」の中にある「イベントビューア -」 を選択する方法もあります (図1-10)。

図1-10 イベントビューアー

コンピューターの ファイル) 操作(A) 表示M ヘルプ コンピューターーカル) 名前 種類 イベント サイズ 操作 ーク | イベントビューアー カスタムビュー Application セキュリティ 23685 26.406 2000MB Setup 使用可能 2,39 100 MB Windows 7 システム 15.901 8.07 MB Application Forwarded Events 使用可能 バイト システム Forwarded Event アプリケーションサービン サブスクリプション 有フォルダー ローカル ユーザーとグループ パフォーマンス デバイスマネージャー ディスクの管 サービスとアプリケーシ 保存されたログを カスタムビュー作 カスタムインボー 表示 ヘル プロバイ ヘルプ

3 ログの取得

1

47

このイベントビューアーでは、システムログやアプリケーションログのほか、 セキュリティログなども確認できます。 たとえば、 イベントビューアーを開い た中にある 「Windows ログ」 の 「システム」 がシステムログです。 また、ア プリケーションログやセキュリティログは、 それぞれ 「Application」 や 「セ キュリティ」 と表示されています。

これらのログの実体は、表1-7 に保存されています。

|表1-7 Windows のログが記録されている場所

ログの種類 場所 システム %SystemRoot%\System32\winevt\Logs System.evtx アプリケーション %SystemRoot%\System32\winevt Logs\Application.evtx セキュリティ その他 %SystemRoot%\System32\winevt Logs Security.evtx %SystemRoot%\System32\winevt\Logs\xxxxxx.evtx

この「%SystemRoot%」 は通常、 「C:\Windows」 という場所を指します。なお、 これらのファイルはバイナリ形式で保存されているため、 ファイルをテキスト エディタなどで開くのではなく、 通常は上記の 「イベントビューアー」などの 専用ツールで閲覧します。

UNIX環境でのログ

LinuxなどのUNIX環境では 「/var/log/」 というディレクトリ内にログファ イルが格納されています。 代表的なログファイルとして syslog や dmesg があ ります。

syslog は、Sendmailというメールサーバーでログを出力する機能として開 発されました。 その後、他のアプリケーションでも導入され、標準的に使われ るログのプロトコルとしてRFC 3163で定義されました。

ログを収集する機能も持ち、 複数のサーバーからのログ情報を1台の専用サー

第1章 ログの基礎知識

48

バーで集中的に管理できます。 図1-11 のように複数のサーバーから syslog にロ グ情報を渡すと、それをログサーバーに転送します。 ログサーバー内のsyslog は、その情報を確認してそれぞれのアプリケーション用のログファイルに書き 込みます。

図1-11 syslog での転送

Web ログを転送 サーバー データベース サーバー ファイル サーバー syslog サーバー ログを収集

なお、単に syslog といっても、現在は syslog を改善した rsyslog や syslog- ng といったプログラムが使われています(表1-8 )。

表1-8 従来の syslog と rsyslog/syslog-ng の主な違い

特徵 拡張性 従来の syslog rsyslog syslog-ng 基本的なログ収集・ 転送のみ プラグインで機能を拡張 できる ネットワーク機能 制限あり 处理性能 シングルスレッド 暗号化やマルチキャスト などのプロトコルに対応 マルチスレッド フィルタリングやルーティング 限定的 高度な条件分岐が可能

ここでは、rsyslogを前提に解説します。 rsyslog は伝統的なsyslogと同じく、

49

3 ログの取得

1

ログをテキスト形式のファイルに出力します。 保存先としてCentOS系の Linux では「/var/log/messages」、 Ubuntu 系の Linux では 「/var/log/syslog」 が標 準設定として指定されており、 ここに出力されます 11。

このファイルに出力された内容を閲覧するには、 「tail」 コマンドを使うこと が一般的です。 たとえば、次のように実行すると、 「/var/log/messages」 とい うファイルの末尾10行を表示できます。

# tail -f /var/log/messages

なお、 現代のLinuxシステムでは、rsyslogだけでなくJournalというログ管 理方法も使われます。 Journal を併用しているディストリビューションもあり、 この場合は 「/var/log/journal/」というディレクトリにログを保存していま す12。 このログはバイナリ形式で保存されているので、 人間が読みやすいテキス トログとして扱うために、rsyslogのデーモン (rsyslogd) Journalのデーモ ソン (journald)からログを受け取り、従来のテキストログファイルに書き出し ています (図1-12)。

図1-12

Journalとrsyslogの連携 さまざまなサービス ログ出力 転送 Journal rsyslog 保存 保存

11 /etc/rsyslog.conf という設定ファイルで変更できる。 12 Raspberry Pi の Raspbian では journal のみが有効になっていることもある。

-

50

このJournal で保存されているバイナリログを検索、 閲覧するには 「journalct1」 というコマンドを使います。 具体的には、次のようにオプション を指定せずに起動すると、 データベース内のすべてのログをまとめて出力でき ます。

# journalctl

最新のログだけを閲覧したいときは、「-f」 オプションを指定します。

# journalctl -f

特定のサービスのログだけを見るときは、 「-u」 オプションでサービス名を指 定します。 たとえば、 「sshd」 のログを確認するには、次のようなコマンドを実 行します。

3 ログの取得

# journalctl -u sshd.service

macOSでのログ

macOS はUNIX系OSのため、一般的なUNIX と同様に 「/var/log」 ディレ クトリにログを出力しています。 たとえば、 システムログは 「/var/log/system. log」 というファイルに出力されています。 このディレクトリに出力されるファ イルは基本的にテキストデータであるため、 テキストエディタなどで確認でき ます。

また、macOS Sierra 以降では、 「ULS (Unified Logging System)」という ログシステムがあります。 このULSでは、 ログデータがデータベースに保存さ れており、コンソールアプリや 「log」 コマンドでアクセスできます (図1-13)。

1

51

図1-13 macOSのコンソールアプリ

[デバイス <-Mac mini レポート クラッシュレポー スピンレポート コンソール メ ค c Q ッセージ アクティビティ 再読み込み メッセージ エラーと ログレポート メッセージなし レポート ログメッセージをストリーミングすると、システムのパフォーマンスに影響します。 Macデータ system.log ストリーミングを開始

たとえば、ターミナルで次のコマンドを使うと、プロセスを指定して最新の ログを確認できます。

$ log show --predicate 'process == "対象のプロセス名"'

オフィスソフトのWordについてのログを見たい場合は、次のように実行し ます。

$ log show --predicate 'process == "Microsoft Word"

必要に応じて、 フィルタ条件や期間などを指定して詳細なログを取得するこ ともできます。

iOSでのログ

iOS では sysdiagnose という機能があります。 iOS11以上でログを取得する には、音量ボタン2つを押しながら、 電源ボタンを1.5秒ほど押します。 これに より、 iPhoneが震えますが、 画面には特に表示されません。

その後、「設定」→「プライバシーとセキュリティ」 → 「解析と改善」 ->

第1章 ログの基礎知識

52

「解析データ」とたどると、 さまざまなログが出力されています。 この中に、 「sysdiagnose_*****」という項目が出力されており、 tar.gz 形式で圧縮された ファイルを確認できます (図1-14)。 このファイルを共有ボタンからiCloudな どに保存し、それをMacなど他の端末に移します。

Macなどで次のコマンドで展開すると、ログやディスクの使用状況を含めて さまざまな情報を確認できます。

$ tar xzvf sysdiagnose_★★★★.tar.gz

図1-14 iOSのsysdiagnose の内容

Accessibility AccessibilityPrefs ACLogs crashes_and_spins AFK AppSupport atcrtcomm AVConference Baseband BatteryBDC BatteryHealth Batteryintelligence com.apple.b__M-meta plist D comaopie M-stace pl al version="1. encoding="UTF- DOCTYPE plus PUBLIC //Apple//DTD PLIST 1.8// Br "http://www.apple.com/ DTD/ PropertyList-1. list version="1.0" dict keySavedgeStatenv ceACANCoreAl cheppersistentState. Aging State.</ /real read95583884 CloudSubscriptionFeatures com.apple.batteryintelligen ce batteryalgM-state.plist プロパティリスト37KB ckksctl status.txt codeccti txt fleproviderci_check.log fleproviderctl dump.log leproviderct.log hpmDiagnose.txt jetsam priority.csv jetsam_priority.txt kbdebug txt BatteryUIPlist Bluetooth CalendarPreferences FOR GenerativeExperiences IOSADiagnose itunes stored keyboards MCState MobileActivation MobileAsset MobileBackup 変更日 2025年3月31日 月曜日 13:12 5月31日 月曜日 13:12 タグ タグを

ASPSnapshots

現代ではコンピュータをネットワークに接続して使うことが当たり前になり ました。 インターネットにつながらないと仕事ができない職種も多く、プライ ベートでも常に接続している状況です。

そんな中、 ネットワークにおけるトラブルが発生すると大きな影響が出ます。 そして、その原因を調査するためにもログは不可欠です。

aphs_stats.txt

summaries

RunningBoard

|TimezoneDB

■ ネットワークでのログの取得

disks bet

Preferences

Personalization

hidutil.plist

Blogs

loreg

WiFi

errors

13ログの取得

bretl

53

ネットワーク機器での取得

ネットワークの構成は有線LANや無線LANなどさまざまですが、そのログ を取得するためには、パソコンやスマートフォンのような端末ではなく、 ルー ターやハブといった通信機器のログを確認する方法があります。

家庭用の無線LANルーターでも、プロバイダとの接続設定や接続している端 末の確認、 ファームウェアのアップデートなどができる管理画面が用意されて います。 そして、この管理画面にログインすると、 通信のログを確認できるこ とが多いものです (図1-15)。

図1-15 ルーターのログの例

ログの すべて TY コントローラー システムログ Dコントローラー 95 Q 1. LED コントローラー 20141220 LED コントローラー 2015.20 LED コントローラー Sato WAN_ON Start for WAND_OFF LAN_OFF 1522LEDコントローラー] SWAN_OFF WANO ON Start to run LAN OFF 225-04-02 15:22:01 LED Start to run WANT OFF TR LED コントローラー WANO_OFF 15220 LED コントローラー JS LAN_OFF [Z-03-16170145 LED コントローラー WAN_N [200-05-161701 LED コントローラー LED コントローラー WANO_OFF LAN OFF すべてクリア

また、業務用のルーターであれば、より細かくログを取得できます。

パケットキャプチャとは

ネットワーク上を流れる通信データ (パケット) を収集し、 解析する技術と してパケットキャプチャがあります。 ネットワークのトラブルシューティング だけでなく、 攻撃の有無を確認するセキュリティ面での監視、 通信速度などの パフォーマンス評価など、さまざまな目的で利用されています。

パケットには、 送信元や宛先のIPアドレス、 ポート番号、使うプロトコル、 データ本体など多種多様な情報が含まれています。 これらを細かく分析するこ とで、ネットワーク上の通信状況を詳細に把握できます。

第1章 ログの基礎知識

54

このようなパケットをキャプチャするときは、パケットキャプチャツールを パソコンにインストールして使うことが一般的です。

パケットキャプチャツールの利用

パケットキャプチャにはさまざまなツールがありますが、ここではGUIで撮 作でき、 Windows や macOS などで動作する Wireshark を使って、 具体的な操 作方法を紹介します。 Wireshark は公式サイト13からダウンロードしてインス トールします。

インストールが完了してWireshark を起動し、 キャプチャするネットワーク インターフェイスを指定します。 すると、そのネットワークインターフェイス を使った通信のキャプチャが始まります。 ネットワークの通信に応じて 1-16 のように画面上部にそれぞれの通信の時刻や発信元、 接続先、 使うプロトコル などの情報が1行で表示されます。 そして、 それぞれの行を選択すると画面下 部に詳細が表示されます (図1-16)。

図1-16 Wiresharkのログ画面

1946 bytes captured and

13 https://www.wireshark.org

ログの取得

3

55

運用上の注意点と課題

ネットワークを対象としたキャプチャでは、パソコンなどとは違い、 通信内 容を記録するときに注意しなければならないことが増えます。

プライバシーとセキュリティ

キャプチャされたパケットには、 利用者の個人情報や機密情報が含まれる可 能性があります。 このため、 ログの保存や転送、 解析にあたっては暗号化やア クセス制御が必須です。

大量データの管理

ネットワーク上のすべてのパケットをキャプチャすると、 膨大なデータが記 録されます。 このため、 ディスク容量の管理やデータのローテーション、アー カイプの仕組みを整備する必要があります。

リアルタイム解析の負荷

キャプチャと同時にリアルタイムで解析を行う場合、 システムリソースへの 影響を考慮しなければなりません。 特に大規模なネットワークでは、 解析用サ ーバーの性能やデータ伝送の遅延が課題となることがあります。

法的・倫理的な配慮

ネットワーク通信をキャプチャする場合、 通信の盗聴やプライバシーの侵害 に関する法的な問題が発生する可能性があります。 このため、 取得するデータ の目的や取り扱いについて、法令遵守と倫理的配慮が求められます。

WebビーコンやWebセンサーによるログの取得方法

私たちがWeb サイトを閲覧しているとき、 Web サーバー側ではどのページ が閲覧されたのかを記録していることを解説しました。 このとき、 Web サーバ

第1章 ログの

56

ーに残るアクセスログを集計するだけでなく、マーケティング担当者が閲覧す るためのプログラムが実装されていることがあります。 これらで使われるログ の取得について紹介します。

ビーコンの特徴

アクセス解析に使うログを記録する手法として、サーバーログ型、パケット キャプチャ型、 Webビーコン型の3つの種類があります。

サーバーログ型

Apache や nginx といった Web サーバーに記録されるアクセスログを集計し、 それぞれのファイルへのアクセス数を調べる方法です。 標準で用意されている 機能で、 特別なソフトウェアの導入や設定も不要なため、 手軽に分析したい場 合には便利です。

パケットキャプチャ型

ネットワーク上にパケットをキャプチャする機器やソフトウェアを設置し、こ れを通過するパケットを記録する方法です。 Webサーバーの手前に設置するこ とで、 その通信を分析できます。

Webビーコン型

Web ページ内に JavaScript や画像ファイルを埋め込んで、 その呼び出しログ を取得する、もしくはJavaScriptから特定のプログラムを呼び出して記録する 方法です。 細かな制御ができ、 便利なサービスが提供されていることから最近 はこの方法がよく使われています。

ここでは、このWebビーコンについて考えます。

たとえば、 Google Analytics では Web ページ内に Google Analytics の JavaScriptファイルを埋め込むことで、ページが読み込まれたときにこの

57

1-3 ログの取得

JavaScriptのプログラムが実行され、 Google側に記録されます。 つまり、 Google Analyticsの JavaScript プログラムが埋め込まれたページだけが集計対 象となります(図1-17)。

図1-17 Google Analytics の動作

Webサーバー Google Analytics ④記録 JavaScript 含まれる ①Webサイトに | ②HTMLファイルを アクセス 返す ⑤ 管理画面に アクセス ⑥アクセス解析 この結果を表示 ③ JavaScriptを実行 利用者 Webブラウザで アクセス

また、Webページやメールに小さな画像などを埋め込んで、 特定の利用者が 閲覧したことを確認する手法もよく使われます (図1-18)。 これは、Web ブラ ウザやメールソフトがコンテンツを表示するときに、 画像を読み込むことを利 用したもので、特定の利用者に送信したメールにIDを付与して識別する手法で す。

図1-18 メールでのWebビーコン

④記録 ①メールを 送信 HTMLメールで <1> 画像を埋め込む ③画像を 読み込み ④画像を 表示 ⑤管理画面に アクセス ⑥ アクセス解析 の結果を表示 ②メールを 開く </>

第1章 ログの基礎知識

58

COLUMN ビーコンとは

街中で利用者の位置情報を取得するとき、 一般的にはGPSやWi-Fi、携帯 電話の基地局などを使ってスマートフォンなどの端末がある場所を取得します。 地図アプリなどで使うのであれば十分な精度が得られますが、 商業施設で店 舗に来店したことを把握することを考えたとき、建物の中ではGPSを取得 できなかったり、立体的な階層を取得できなかったりします。

そこで、 Bluetooth などの近距離無線通信の技術を使って、 一定間隔で信 号を発信する仕組みであるビーコンが使われます。 建物内に設置した機器が 信号を発信し、スマートフォンのアプリがそれを受信することでエリア内 にいることがわかります。 地下や建物の中でも、ビーコンを使うと受信者 の位置情報を高い精度で特定できるため、 移動履歴や滞在時間、特定のエ リアでの操作などを収集できます。 このため、 来店の把握やプロモーション、 室内での道案内などに使われています。

このように、ビーコンは定期的に信号を発信することで、受信可能な範囲 に端末が入ったときに店舗情報や商品情報 クーポンなどを配信したり、 ポ イントを付与したりできます。

JavaScriptを使うメリット

上記ではWebビーコンの実現方法として Google Analytics のような JavaScriptのプログラムを使う方法について解説しました。 このようなプログ ラムを使う理由として、 キャッシュ14の影響を受けにくいことがあります。

Web サーバーでアクセスログを調べるときは、Webブラウザ内でのキャッ シシュやプロキシサーバーでのキャッシュ、 CDN の使用などを考えなければなり ません。

たとえば、Webブラウザやプロキシサーバーは、同じファイルを何度もWeb サーバーから取得しないように、 1度取得したファイルをキャッシュとして保

14 1度開いたWebページのデータを保存しておいて、次回に同じWebページを開くときに素早く表示する 仕組み。

-

ログの取得

59

持しています。 これにより、ネットワークの通信量やWebサーバーの負荷を減 らす効果があります (図1-19)。

図1-19 CDNやプロキシサーバーの仕組み CDN 事前にコピー Webブラウザ CDN Webサーバー リクエスト |プロキシサーバー レスポンス Webブラウザ リクエスト プロキシ サーバー レスポンス リクエスト レスポンス Webサーバー 初回のアクセスを キャッシュとして記録 2回目以降はキャッシュを応答 m

しかし、キャッシュされているファイルをWebブラウザが表示してしまう と、Web サーバー側には通信が発生しないため、 アクセスログを記録できませ ん。 もちろん、閲覧するときにファイルが変更されていないかを Web ブラウザ が問い合わせることはありますが、そのリクエストが来ないと、 Web サーバー 側では閲覧されたことを認識できません。 このため、 実際の閲覧数よりも少な くカウントされてしまいます。

Google Analytics などの JavaScriptのプログラムを使うと、 Web ブラウザ で表示した時点でプログラムが実行されます。 ページを再読み込みしても、そ の都度プログラムは実行されます。 JavaScriptが動作したタイミングでGoogle のサーバーとの間で通信するため、 キャッシュとは無関係に記録を残せます。

また、ページを再読み込みせずに、 Ajaxのようなプログラムでページの一部 を書き換えたり、ページをどこまでスクロールしたかを計測したりすることも

第1章 ログの基礎知識

60

できます。 ページを離脱したタイミング (unloadイベントの発生)で計測す ることもできるため、細かくログを取得できることが特徴です。

さらに、ログインが必要なWebサイトの場合は、サーバーログ型では実現で きなかったような利用者の識別が可能になるため、 アクセス数だけでなくユー ザー数も測定できるメリットもあります。

ただし、JavaScriptのプログラムはWebブラウザでJavaScriptを無効にして いる、もしくはJavaScriptに対応していないWebブラウザでは動作しないこと には注意が必要です。

また、一部の Web ブラウザでは JavaScript から Cookieにアクセスすること を防いだり、 Googleなどのサーバーにデータを送信することをブロックした りします。 このため、JavaScriptのプログラムが問題なく動作するとは限らな いことには注意が必要です。

Web センサーの使用

温度や湿度、光、 振動、 圧力、 加速度など、 さまざまな量を測定するときは センサーが使われます。 工場や農地などで業務用に使われるほか、 家庭でもス マートハウスやヘルスケアなど多様な分野で使われています。

センサーはリアルタイムで計測し、 その情報をサーバーに転送する使い方が 一般的です。 そして、一定間隔に記録された情報を分析することで、リアルな 現場の状況を時系列データとして保存できます。

これにより、温度や湿度といった環境の変化を監視したり、機器の状態をモ ニタリングしたり、 異常を検知したりできます。 また、長期間にわたってデー タを取得することで、 故障を予防するための対策を実施できる可能性もありま す。

起きている変化をリアルタイムで監視しているため、 異常値の検出や急激な 変化を即座にログに反映できます。 これにより、システムに問題が発生したと きには早期に警告できます。

このようなセンサーの考え方をWebにも適用したものとしてWebセンサー

61

1

3 ログの取得

1

ます (図1-20)。 があります。 たとえば、 警視庁サイバー警察局では全国の警察施設のインター ネット接続点にセンサーを設置し、取得した通信内容の観測結果を公開してい

04/09 10:00 200 100 図1-20 警視庁サイバー警察局 「不審なアクセスの観測状況」 の 「宛先ポート別推移」 15 TZ+0900 (JST)/(c) 2003-2025, National Police Agency JAPAN 500 400 300 04/10 10:00 04/11 10:00 04/12 10:00 04/13 10:00 Oters □6379/tcp ■443/tcp 18/icmp 22/tcp 8728/tcp 80/tcp 123/tcp 04/14 10:00 04/15 10:00 04/16 10:00

このグラフの横軸は時間、 縦軸は1時間ごとに集計された1IPアドレス単位の 件数です。 1時間ごとに更新され、 インターネット上でどのようなポート番号 への攻撃が増えているのかを把握したり、 ワームなどの発生状況の変化を捉え られたりします。

上記のほかにも、 発信元国別推移 (アクセス元の国別にどの国からのアクセ スが多いか)や攻撃手法別推移 (管理者権限の取得やWebアプリの攻撃のよう に、どの手法を狙ったアクセスが多いか) などが公開されており、 Web センサ ーに対する不審なアクセスの推移を確認できます。

15 https://www.npa.go.jp/bureau/cyber/koho/observation.html

|1章 ログの基礎知識

第

62

第

2 ログの記録と収集

章

現代の企業では多くのシステムが連携して動作しています。 そし て、第1章では、さまざまなシステムでログが出力されているこ とを解説しました。 このとき、 それぞれのシステムに個別にログ インしてログを確認するのは面倒です。 しかも、システムごとに ログの形式が異なることもあります。 そこで、それぞれのシステ ムから必要なログだけを収集し、 一元管理することを考えます。

2

ログの収集方法、 ツールの紹介

役立つのはこんなとき

どうやってログを出力するのか知りたい ログを収集するツールを導入したい

それぞれのシステムからログを収集するため、 どのようにログが出力される のかを解説したあと、ログを収集・管理するツールについて解説します。

ログの出力

ログを収集する前に、まずはログを出力するところから始めます。 ログを出 力するシステムが多様であることを第1章で解説しましたが、 ここでは検証で きるように、 手作業でログを出力することから始めます。

ログを出力してみよう

まずはコマンドからログに出力すると、 どのように記録されるのか、 その動 作を確認しておきましょう。

ここではLinux環境を使って syslogに出力することにします。 最近では、サ ーバーを構築する Linux としてCentOSやその後継となるOS が多く利用され ています。 本書ではLinux として Alma Linux 1 を使いますが、 Rocky Linux な

1 Alma Linuxの9.6や10.0のminimal版を使用。

-

64

ど CentOS と同じようなディストリビューションであれば、コマンドや設定は 基本的に変わりません。

パソコンに Linuxをインストールするときは、公式サイトからISOファイル をダウンロードして、 USBメモリなどに書き込んでコンピュータを起動する だけです。 手軽に構築したいときは、 インターネット上で提供されているVPS (Virtual Private Server 仮想専用サーバー) を使う方法もあります。 VPSの 場合は、テンプレートが用意されていて、簡単にインストールできるサービス もあります。 1日数十円で利用できるVPSや検証用に無料で利用できるVPS も あるため、これを契約して使うとよいでしょう。

Alma Linux に root権限でログインし、次のコマンドを実行すると、これま でに出力されているログを確認できます。

# tail -f /var/log/messages

これは、 「/var/log/messages」というファイルの末尾10行を出力するコマン ドでした。 そして、 ファイルの変更を監視し、 ログが末尾に追記されるなどフ ァイルが更新されると、 その更新内容がリアルタイムに表示されます。 更新さ れる状態を停止するには、 「Ctrl + C」 を押します。

このファイル 「/var/log/messages」 にコマンドからログを出力するには、次 のように 「logger」 というコマンド名に続けて出力したいメッセージを指定し ます。

# logger test-message

これで、 「test-message」 というメッセージがログに追加されます。

/var/log/messages

May 01 12:34:56 xxxxxx root [1234]: test-message

ログの収集方法、ツールの紹介

1

2

65

このloggerというコマンドは、 syslog にログを出力するコマンドで、いろい ろなオプションが用意されています。 Alma Linux などの rsyslog が動作してい る環境で何もオプションを指定しないと、 「/etc/rsyslogd.conf」 という設定フ ァイルで指定されている場所に出力されます。 Alma Linux など CentOS系で は 「/var/log/messages」、 Ubuntu 系では 「/var/log/syslog」 が標準で指定さ れているため、 ここに出力されます。

ログの優先度を指定する

logger コマンドでは、 「-p」 (--priority) というオプションを使うと、ログ の優先度を指定できます。 この優先度の設定は 「ファシリティ (facility)」と 「レベル (level)」 の組み合わせで指定します。 たとえば、次のように 「mail. info」という優先度を指定してコマンドを実行すると、 「ファシリティ」が 「mail」、 レベルが 「info」 (情報) になり、 「test-message」 という内容をログ として出力します。

# logger -p mail.info test-message

そして、このファシリティによって、 「/etc/rsyslog.conf」 で指定されたフ ァイルに出力されます。 ファシリティには、 表2-1 の値を指定します。

2-1 ファシリティに指定する値の例

ファシリティ 說明 auth セキュリティや認証のメッセージ authpriv プライベート認証のメッセージ cron cron デーモン daemon システムデーモン

第2章 ログの記録と収集

66

ファシリティ ftp kern local0~local7 1pr mail news syslog 説明 FTPデーモン カーネルメッセージ ローカル環境で独自に使えるメッセージ プリンタ メール ニュース syslog によって生成されたメッセージ user uucp 一般ユーザーレベルのメッセージ UUCP

出力先のファイルを変更するには、上記の 「/etc/rsyslog.conf」 を編集しま す。たとえば、標準では次のように指定されており、 左側にある条件と、右側 にある出力先が対応しています。

/etc/rsyslog.conf

~中略~ .info;mail.none; authpriv.none;cron.none /var/log/messages ~中略~ authpriv.* ~中略~ mail.* ~中略~ cron. * /var/log/secure -/var/log/maillog /var/log/cron ~中略~

この「/var/log/messages」 に出力する条件として指定されている 「*.info」 は、すべてのファシリティのうち info レベル以上のログを対象にすることを意 味します。 また、 「mail.none」 という指定はメールに関するログは除外するこ

ログの収集方法、ツールの紹介

1

2

67

とを意味します。 「authprív.none」 と 「cron.none」も同様です。

上記のコマンドのように、「mail.info」 という優先度を指定すると、上記の 条件から「/var/log/messages」 には出力されず、「/var/log/maillog」に出力 されることがわかります。 また、レベルには表 2-2 のいずれかの値を指定し ます。

表2-2 レベルとして指定する値の例

說明 システムが使用不能 レベル 数值 emerg 0 alert 1 直ちに対応が必要 crit 2 重大な状態 err 3 エラー warning 4 警告 notice 5 通常とは異なるが重要でない info 6 情報 debug 7 デバッグ用

logger コマンドに何もオプションを指定しないと、 「user.notice」という優 先度を指定したものと同じです。

オプションを指定することで、 出力先やレベルを変えられるため、「ファシリ ティ」に「authpriv」、 レベルに 「info」 を指定すると、 上記の設定ファイルで 指定されている 「/var/log/secure」 に出力されます。

# logger -p authpriv.info test-message

2 この 「/var/log/maillog」 の先頭にある「-」はログへの書き込みが非同期であることを意味している。

第2章 ログの記録と収集

68

同様に、 「ファシリティ」 に 「cron｣ レベルに 「alert」 を指定して実行する と、 「/var/log/cron」 に出力されます。

# logger -p cron.alert test-message

なお、レベルに 「emerg」 を指定すると、 ログとして出力されるだけでなく緊 急メッセージとしてログインしている全員の端末にブロードキャストされるよ うに設定されています。

# logger -p uucp.emerg test-message

ログにタグをつける

ログを出力する際、 あとから検索するときに便利なように、 タグをつける方 法があります。 タグをつけるには、 loggerコマンドで 「-t」 (--tag)というオ プションを指定します。

どのアプリケーションやサービスから出力されたものであるかを識別できる ように、アプリケーション名をタグとして指定することが一般的です。 たとえ ば、次のように実行すると、 「myapp」 というタグがつけられたログが出力され ます。

#logger -t myapp "This is a log message"

出力されたログを見ると、 次のように 「myapp」 というタグがついていること がわかります。

/var/log/messages

May 01 12:34:56 xxxxxx myapp [1234]: This is a log message

1 ログの収集方法、ツールの紹介

2

69

プログラムからログを出力する

前節では手作業でログを出力することを解説しましたが、 実際にはプログラ ムの開発者が、そのプログラム内からログを出力します。 このとき、 多くの言 語には syslog にログを出力するライブラリが用意されています。

PHPからの syslog 出力

プログラミング言語としてPHPを使っている場合、 標準で 「openlog」 と 「closelog」、「syslog」 という関数が用意されています。 「openlog」 は syslog への接続を開始、「closelog」 は接続を終了する関数で、 「syslog」 はログを出 力する関数です。 このため、これらの関数を実行するだけで、 上記のloggerコ マンドと同様に出力できます。

log.php

<?php

openlog ("php_test", LOG_PID, LOG_USER);

syslog (LOG_WARNING, "test-message");

closelog();

ここで、 openlog 関数の引数に指定している 「LOG_USER」 が上記のファシリ ティでの 「user」 に、 syslog 関数の引数に指定している 「LOG_WARNING」が上 記のレベルでの 「warning」 に該当します。 それぞれ、 定数として宣言されてい るため、これを指定するだけで、 1ogger コマンドと同等のログを出力できるこ とがわかります。

Webアプリとして配置してアクセスするか、 コマンドから実行するなどの方 法で、 実際にログとして出力されることを確認してください。

第2章 ログの記録と収集

70

Python 50 syslog

プログラミング言語として Python を使っている場合は、 標準ライブラリ の「syslog」をインポートして使います。 このライブラリには、 「openlog」と 「closelog」、 「syslog」 という関数があります。 これらはPHPで示した内容と 同じような役割があります。

このため、これらの関数を次のように実行するだけで、上記のlogger コマン ドと同様に出力できます。

log.py

import syslog

syslog.openlog ("python_test", syslog.LOG_PID, syslog.LOG_USER); syslog.syslog (syslog.LOG_WARNING, "test-message"); syslog.close log();

COLUMN ログを出力するときに意識すること

ログをどのレベルで出力するのかを考えたとき、 すぐに思いつくのは 「な んでも出力しておくこと」 です。 何かイベントが発生したときに、それをロ グとして出力しておけば、あとからそのログを検索して調査できます。 逆に、 ログに出力していないと、 何が起きたのかを調べることは困難です。

このため、 とにかくなんでも出力しておけばよいと考える開発者がいます。 実際、 コンピュータのストレージは大容量になっており、 テキスト形式で ログを出力しても容量の面ではそれほど問題にならなくなっています。 また、 コンピュータの性能も向上し、膨大なログから検索できるようになりました。

しかし、使うときの用途を意識してログを出力しておかないと、 トラブル が起きたときに絞り込むのは簡単ではありません。 何を目的として出力する のかを意識しておかないと、 まったく役に立たない膨大なログができてしま います。

このため、「どのような目的でログを使うのか」 「どのような情報を出力し ておく必要があるのか」 を意識しなければなりません。

2-1 ログの収集方法、ツールの紹介

71

ログ収集・管理ツールの種類

ログをコマンドやプログラムから出力できることがわかったので、出力され たログを収集することを考えます。 このとき、 ログ収集管理ツールとしてい くつかの製品が提供されていますが、 「ホスト型」 「ゲートウェイ型」 「統合ログ 「管理」に分けて考えられます。

ホスト型のログ収集・管理ツール

パソコンやサーバーなどにインストールして使うツールをホスト型と呼びま す。それぞれの端末に導入が必要ですが、 その端末で発生したさまざまなログ やイベントを収集できるため、 多くの組織で使われています。

中小企業でログを収集するときは、 「IT資産管理ツール」 を導入する方法が 手軽です。 具体的なツールとして、 SKYSEA Client View や System Support Lab、 LANSCOPE、 My LogStar などがあります。 数百台程度のパソコンなど を管理する目的であれば、 数百万円程度から導入でき、 専門的なスキルを備え ている人がいない中小企業でも比較的導入しやすいものです。

COLUMN ITAM, ITSM, ITOM

IT資産管理はその頭文字を取ってITAM (IT Asset Management) と 呼ばれます。 これと似た役割を担う管理方法としてITSM (IT Service Management) や ITOM (IT Operation Management) があります。

ITAM が 図2-1 のようなハードウェアやソフトウェア、 ライセンスの 管理のような資産に注目するのに対し、 ITSMはシステム障害などのイン シデントの管理、 システムの更新などのサービスに注目しています。 また、 ITOMは運用や監視などによる安定稼働に注目しています。

こういったツールはUSBデバイスなどを接続しようとしたときの利用制限や

第2章 ログの記録と収集

72 -

利用内容の記録、 禁止アプリケーションの起動制限、 不正PC端末の接続検知、 Webサイトアクセス検知や制限といったセキュリティ機能を備えていることが 多く、ログを収集・管理する機能も備えています。

図2-1 IT資産管理ツールの機能

ハードウェア管理 パソコン ソフトウェア管理 ・OS ライセンス管理 ・プレインストール スマートフォン ネットワーク機器 ・USBメモリ アプリケーション それぞれのバージョン ・パッケージ ボリュームライセンス

ログの管理に特化しているツールではありませんが、 IT業界でない中小企業 であれば、このようなツールから始めて、 まずはログを残しておくことが重要 です。 攻撃を検知したり、 定期的なレポートを作成したりするような高度な作 業は難しいことから、最低限のログを残すために、このようなツールを使うこ とから始めます。

何か問題が発生してログを解析するときには、 残しておいたログをもとに専 門の業者に依頼することになりますが、 ログが残っていないと解析することも できませんし、 取引先に対する説明責任を果たすこともできません。

上記のようなログ収集管理ツールを導入するときにコストが問題になる場 合や、ログ管理についての専門的な知識がある場合は、 オープンソースのログ 収集・管理ツールの導入を検討することもあります。

ゲートウェイ型のログ収集・管理ツール

ネットワーク上を流れる通信データ (パケット) を取得し、それをもとにロ グを管理するツールはゲートウェイ型と呼ばれます。 通信データを取得する際 は、スイッチ等のネットワーク機器にあるミラーリングポートを使用して接続 する方式や、ネットワーク上に機器を設置して通信を中継する方式などがあり

-

2-1 ログの収集方法、ツールの紹介

73

ます。

ホスト型と異なり、それぞれの端末に専用のツールを導入する必要がなく、端 末の動作に負荷をかけないメリットがあります。 監視対象のサーバーが1箇所 のオフィスなどに集約されており、対象となる端末が明確であれば、比較的容 易に導入できます。

図2-2 ゲートウェイ型 監視対象のサーバー o ゲートウェイに設置 インターネット ログ 管理者

また、クラウドサービスを利用するような場合で、 クラウドサービス側で特 別なログ収集・管理ツールが用意されていないような場合も、 クラウドサービ スへのアクセスを中継することでログを管理できます。 たとえば、 ファイルサ ーバからパソコンへのファイルのコピーのように、 通信内容をパケット単位で 取得するため、 すべての通信を記録できるメリットがあります。

このことから、重要なデータを保存するサーバーに対する内部不正の抑止や、 インシデントが発生したときの原因の分析、 影響範囲の特定などには有効なツ ールだといえます。

-

74

ただし、それぞれの端末でUSBメモリにファイルをコピーする、といった操 作はネットワークを経由しないため、 記録できないことはデメリットです。

SIEM などの統合ログ管理ツール

大企業などを中心に、 多くの機器を管理する企業では、 複数の機器が発す るログを一元管理したいものです。 このとき、 個々に管理するのではなく、統 合して管理できるツールとしてSIEM (Security Information and Event Management) があります。

これは、サーバーが発するログだけでなく、 IDSやIPSなどネットワーク上 にある機器が発するログを統合し、リアルタイムに情報を収集して表示する仕 組みを備えたものです。

これにより、単一のログだけでは検出できない攻撃を発見したり、ログをリ アルタイムに分析したりすることで、 外部からの攻撃や内部でのポリシー違反 などを早期に検知できます。

このようなツールの特徴として、 ログの保存容量や検索速度が挙げられます。 特に、ログの検索速度についてはそれぞれのツールによって特徴があり、工夫 されています。 ただし、 初期導入のコスト、 運用コストとともにそれだけ高価 であることも多いです。

導入前における準備と運用上のポイント

ログ収集・管理ツールにはさまざまな種類があることがわかりましたが、こ れらを導入するときに注意しなければならないこととして、 次のことが挙げら れます。

大量のログをスムーズに処理できるか

システムの数が増えるとログの量も増え、 アクセスが集中するなどの理由で 一時的にログの量が増加する可能性もあります。

75

ログの収集方法、ツールの紹介

1

2

安全に転送できるか

ログには重要な情報が含まれる可能性があるため、 データの暗号化や改ざん の防止などの機能が必要です。

ログが原因でシステムのパフォーマンスが低下することがあってはなりませ ん。 また、 ログ収集・管理ツールの性能が追いつかず、 ログが失われてしまっ ては本末転倒です。 そこで、システム全体としてログを収集するときの要件を 整理しなければなりません。

取得したいログを転送して一元管理するように設定しても、その管理体制を 構築しないと、 単独でログを出力しているのと変わりません。 転送するシステ ムが増えると、それだけ管理の負担が増えるだけでメリットがないためです。

そこで、どのようなログをどのタイミングで収集するか、 どのような分析の ために収集するのか、 なんらかのログが出力されたときにアラートを設定する かといった運用ルールを策定することから始めます。

一般的には、次のような手順に沿って導入します。

1. 収集するログの要件を決める

まずはどのシステムから何のログを収集するのか、そしてどのような条件で 抽出するのか、 転送先のシステムはどこなのかを明確にします。 特に重要なの は、転送した先で誰がどのような目的で使うのかを明確にすることです。

2. 環境を構築する

検証環境に導入し、 ログの収集や転送、 変換処理が正しく動作するかを検証 します。 特に必要なデータが欠損していないか、 処理中にエラーが出ていない か、システムの負荷は問題ないか、 などを一定の期間をかけて確認します。

3. 本番環境に導入し、 運用を開始する

検証環境での動作に問題がなければ、 本番環境に移行します。 移行したあと

第2章 ログの記録と収集

76

も、 実際に運用しながら定期的に監視します。 特に、 システムの負荷やログの アーカイブ状況などを確認し、 事前に策定しておいた計画の範囲内で容量面や 性能面が維持できているか、 問題が起きたときに対応できるかを確認します。

以下、 中小企業でログ管理を導入するときによく使われるツールについて解 説します。

Fluentd Logstash

ホスト型でもゲートウェイ型でも導入できるオープンソースの代表的なログ 収集ツールとして、 Fluentd や Logstashがあります。 異なるフォーマットや プロトコルで出力されたログを一元管理し、 他のツールに転送できる役割を果 たすために、さまざまなプラグインを使えることが特徴です。

Fluentd の概要

Fluentd は軽量かつシンプルな構成で、 高速にデータを転送できるログ収集 ツールです。 TLS3による通信の暗号化やセキュリティプラグインによって、 安 全に転送できることに加え、 導入したあとの運用管理やトラブルシューティン グのしやすさも多く選ばれている理由です。

図2-3 のように、 入力 (source)、 フィルタ (filter)、 出力 (match) とい う構成で、 それぞれに導入するプラグインを組み合わせることで、柔軟に構成 できます。

3 Transport Layer Security の略で、暗号化や認証、 改ざん検出などを備えたSSL (Secure Sockets Layer) の後継プロトコル。

2

1

1

ログの収集方法、ツールの紹介

77

図2-3 Fluentd の構成

入力 出力 Fluentd フィルタ

入力のプラグインから収集されたデータは、フィルタのプラグインで整形・ 加工されたあと、 出力のプラグインで保存先に転送されます。 これにより、 さ まざまなシステムからのログが統合され、 出力先に指定された分析ツールやデ ータベースに渡されます。

Fluentd をホスト型として使う場合は、それぞれのサーバーに Fluentd をイ ンストールし、端末内でログを収集します。 一方、ゲートウェイ型として使う 場合は、ネットワーク上に Fluentd を導入した端末を設置し、 複数の端末が出 力するログを一括して収集します。

Fluentd のインストールと設定

Fluentd を Alma Linux にインストールするときは、 次のコマンドを実行し ます。 これは、 Alma Linux が CentOS 系 (Red Hat Linux系) であるためです。

# curl -fsSL https://fluentd.cdn.cncf.io/sh/install-redhat-fluent- package6-1ts.sh | sh

インストールが完了すると、 次のコマンドでデーモンを起動します。

# systemctl start fluentd.service

第2章 ログの記録と収集

78

# systemctl enable fluentd.service

さらに、次のコマンドでデーモンの起動を確認します。 ここで 「Active」 の 欄に「active(running)」 と表示されていれば、問題なく起動しています。

# systemctl status fluentd. service fluentd. service fluentd: All in one package of Fluentd Loaded: loaded (/usr/lib/systemd/system/fluentd.service; disabled; preset: disabled) Active: active (running) since Thu 2025-05-01 12:34:56 JST; 8s

ago Docs: https://docs.fluentd.org/ Process: 16620 ExecStart=/opt/fluent/bin/fluentd --log $FLUENT_ PACKAGE_LOG_FILE --daemon /var/run/fluent/fluentd.pid $FLUENT_ PACKAGE_OPTIONS (code=exited, status=0/SUCCESS) Main PID: 16626 (fluentd) Tasks: 9 (limit: 4660) Memory: 79.9M CPU: 1.242s CGroup: /system.slice/fluentd. service -16626 /opt/fluent/bin/ruby/opt/fluent/bin/fluentd --log /var/log/fluent/fluentd.log --daemon /var/run/fluent/fluentd. pid

L-16629 /opt/fluent/bin/ruby -Eascii-8bit: ascii-8bit / opt/fluent/bin/fluentd --log /var/log/fluent/fluentd.log --daemon / var/run/fluent/fluentd.pid --under-supervisor

May 01 12:34:56 xxxxxx systemd[1]: Starting fluentd: All in one package of Fluentd... May 01 12:34:56 xxxxxx systemd[1]: Started fluentd: All in one package of Fluentd.

ログの入力元や出力先、 フィルタの内容などを設定するには、 「/etc/fluent/ fluentd.conf」 という設定ファイルで指定します。 この設定ファイルは、 Fluentd の動作やログ収集、 転送の挙動を定義するもので、 主に 表2-3 の3つ のセクションで構成されています。

2-1 ログの収集方法、ツールの紹介

79

表2-3 Fluentdの設定ファイルのセクション

セクション 内容 source データの入力元 (ログファイルなど) を定義する filter 入力データの加工や変換を指定する match データの出力先 (ファイル、 データベースなど) を定義する

まず 「source」 については、次のように設定します。

/etc/fluent/fluentd.conf

<source> @type tail path/var/log/messages pos_file /var/log/fluent/messages.pos tag messages format syslog </source>

それぞれの行は、表2-4 の内容を指定しています。

表2-4 source の指定内容

項目 值 @type tail path /var/log/messages pos_file 内容 ファイルの末尾を追尾する 監視するログファイルのパス /var/log/fluent/messages. 読み込み位置を記録するファイル (再 起動時の読み飛ばし防止) pos ログに付与するタグ tag messages (続くfilterやmatchで利用する) format syslog ログのフォーマット

第 2 章 ログの記録と収集

80

この 「@type」で指定したものがプラグインです。 「tail」 という値を指定す るとファイルの末尾に追記される内容を追尾します。 また、 「path」 で監視する ログファイルを指定しています。 このログファイルへのアクセス権限がFluentd に必要です。

また、 「pos_file」 では、これまでに読み込んだ位置を指定しています。 こ れを指定しておかないと、 Fluentd が再起動したときにログを重複して読み込 んでしまいます。 そして、 「tag」 でタグを指定しています。 このタグを後続の 「filter」 や 「match」 で指定することで、複数の 「source」 に対して、それぞ れどのようなフィルタをするのか、 どこに出力するのかが決まります。

最後の 「format」 でログのフォーマットを指定しています。ここでは syslog 形式を前提としていますが、 カスタムフォーマットであれば、 そのフォーマッ トを指定します。

続いて、「filter」 を設定します。 このフィルターは、タグとして 「messages」 を指定したログの中から 「message」 フィールドに 「error」 という値が含まれ るものだけを抽出します。

/etc/fluent/fluentd.conf

<filter messages>

@type grep

<regexp>

key message

pattern error

</regexp>

</filter>

どのようなフィルタをするのかは 「@type」 で指定したプラグインによります。 「grep」 を指定すると、 「regexp」 で指定した正規表現で抽出します。 ここでは、 message の部分に 「error」 という文字列を含むログを抽出しています。

最後に、 「match」 についても同じような形式で設定します。 次のように指定

ログの収集方法、ツールの紹介

1

2

81

すると、タグとして「messages」を指定したログを 「path」 で指定したファイ ルに出力します。

/etc/fluent/fluentd.conf

<match messages> @type file path/var/log/fluent/syslog_filtered.log </match>

なお、「/var/log/messages」 に root以外のユーザーではアクセス権限がない 場合、このファイルに 「fluentd」 というユーザーからのアクセスを許可する必 要があります。たとえば、 「admin」 というグループを作成し、このグループに 読み込み権限を付与、そしてこのグループにfluentdユーザーを追加します。

# chgrp admin /var/log/messages # chmod 640 /var/log/messages # usermod -aG admin fluentd

その後、Fluentd を再起動します。

# systemctl restart fluentd.service

これで、 「/var/log/messages」 に出力されるログに 「error」 という文字が含 まれると、「/var/log/fluentd/syslog_filtered.log」というディレクトリ内に 出力されます。

他の Fluentd サーバーに転送するには、 「match」のプラグインとして 「@type」 に 「forward」 を指定します。 そして、 転送先のサーバーを指定すると、ログを 他のマシンに転送できます。

の記録と収集

82

Logstash の概要

Fluentd と似た機能を持つソフトウェアとしてLogstashがあります。 Elastic 社が開発したオープンソースのデータ収集ツールで、単体で使うというよりも、 Elastic Stack と呼ばれるツール群の1つとして使われることが多いものです。

このElastic Stack は、 「ELK Stack」とも呼ばれており、「Elasticsearch」 「Logstash」 「Kibana」 という3つのツールの頭文字からなるものです。 つまり、 Elasticsearchや Kibana と組み合わせてログ解析の基盤を構築するときに一般 的に使われています。

Logstash も Fluentd と同様に、 多様なログを収集し、必要に応じてフィルタ リングや変換をしたあとで、 さまざまな出力先に送信します。 これにより、複 雑なログの前処理や集約ができ、 分析や監視を効率化できます。 多数のプラグ インが用意されていることも Fluentd と同様です。

Logstashのインストールと設定

Logstash は Java 仮想マシン上で動作するため、 Java の実行環境のインスト ールが必要です。 Alma Linuxに導入する場合、 まずJavaがインストールされ ているか確認します。

# java --version

インストールされていなければ、 OpenJDK をインストールします。 この 「dnf upgrade」はシステムを最新にするコマンドです4。 ｢-y」 というオプションは、確 認を求められたときに自動的に 「yes」 を応答します。

# dnf upgrade -y

4 dofコマンドは以前のyum コマンドと同等だという認識で十分。

-1 ログの収集方法、ツールの紹介

2

83

# dnf install -y java-21-openjdk

続いて、Logstash をインストールする前に、まずはdfコマンドのリポジト リを Elastic社のものに設定します。 そのうえで、 パッケージを指定します。

# rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch #tee/etc/yum.repos.d/logstash.repo <<EOF [logstash-9.x] name=Elastic repository for 9.x packages baseurl=https://artifacts.elastic.co/packages/9.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md EOF

そのうえで、次のようにLogstashをインストールします。

# dnf install -y logstash

インストールが完了すれば、 サービスを起動し、 システム起動時に自動起動 するように設定します。

# systemctl start logstash. # systemctl enable logstash

続いて、 Logstashの設定ファイルを編集します。 「/etc/logstash/conf.d/」 というディレクトリに格納されたものが使われ、 次のように 「input」 「filter」 「output」の3つに分けて指定します。

第 2 章 の

84

/etc/logstash/conf.d/error_filter.conf

input {

file {

path => "/var/log/messages"

start_position => "beginning"

sincedb_path => "/var/log/logstash/sincedb_file"

}

filter {

if "error" not in [message] {

drop {}

}

}

}

output { file { path => "/var/log/logstash/syslog_filtered.log" } }

Fluentd で解説したものと同様に、 「/var/log/messages」 に root以外のユー ザーではアクセス権限がない場合、このファイルに 「logstash」というユーザ ーからのアクセスを許可する必要があります。 「admin」 というグループを作成 し、このグループに読み込み権限を付与、そしてこのグループに1ogstashユー ザーを追加するには、 次のコマンドを順に実行します。

# chgrp admin /var/log/messages # chmod 640 /var/log/messages # usermod -aG admin logstash

そして、「start_position」 に 「beginning」 を指定すると、ファイルの先頭 から読み込みますが、これは初回のみ有効です。 そして、 「sincedb_path」 には 読み込んだ位置を記録します。

ログの収集方法、 ツールの紹介

1

2

85

また、監視する 「/var/log/messages」 に 「error」 という値が含まれるログ が出力された場合、 「/var/log/fluentd/syslog_filtered.log」 に出力されます。 この設定完了後、Logstash を再起動すると、ログの転送が始まります。

# systemctl restart logstash

COLUMN Logstash [grok]

Logstash が便利な理由として、 「grok」 というフィルタがあります。 これ を使うと、 正規表現を使って高度なパターンマッチングができ、 「パターン」 という名前付きの正規表現が多く用意されています。

たとえば、 「%{COMBINEDAPACHELOG}」 という指定を使うと、 Apache のアクセスログを解析するときに便利な正規表現が用意されています。 これ を使うと、 次のようにフィルタを記述できて便利です。

/etc/logstash/conf.d/apache.conf

filter { grok { } match => { "message" => "{COMBINEDAPACHELOG}" } date { match => ["timestamp" "dd/MMM/yyyy: HH:mm:ss Z" ] } }

第 2 章 ログの記録と収集

86

2

インポートとエクスポート

2

役立つのはこんなとき

他のシステムからログをインポートしたい ログをエクスポートする方法を知りたい

リアルタイムにログを処理するときは、前節で解説したような方法が使えま す。 しかし、システムの規模や体制によってはリアルタイムに処理する必要が あるかを考えたほうがよいでしょう。 過去に出力されたログを他のシステムに 取り込みたい、もしくは他のシステムに出力したいこともあります。

このような場合には、 Fluentd や Logstashのようなログ収集ツールに手動 で取り込むとよいでしょう。

インポート

まずは既存のログデータがファイルに保存されていたとして、これをログ管 理システムに取り込むことを考えます。

インポートの必要性

外部にあるデータをシステム内に取り込む処理をインポートといいます。 一般 的には、 あるアプリケーションが専用のデータ形式で保存しているときに、 他 このシステムから出力されたデータを、 その形式を変換しながら取り込むことを 指します。

2-2 インポートとエクスポート

87

インポートすることで、 異なる記録形式で保存されているログデータを統一 でき、過去のデータを含めた一元管理が可能となります。 これにより、 分析す るときの信頼性が向上します。

一方で、リアルタイムで取り込む方法と、インポートする方法ではそれぞれ 2-5 のようなメリットとデメリットがあるため、 どちらを選ぶのかを検討し なければなりません。

表2-5 リアルタイムでの取り込みとインポートの比較

デメリット タイミング メリット リアルタイム いつでもすぐにログを検索できる インデックスの作成に負荷がかか ることがある インポート 事前にインデックスを作成してお ける 最新の内容が反映されるまでに 時間がかかる

このインデックス (索引) の作成について考えます。 ツールによっては、高 速に検索や解析ができるようにするために、ログのインデックスを作成してい ます。 この処理に時間がかかるのであれば、 ログをまとめて取り込んでからイ ンデックスを作成することで効率よく処理できる可能性があります。

また、ログを取り込む頻度を考えたとき、 1日1回だけバッチ処理でログを取 り込むようにすると、リアルタイム性は確保できませんが、インデックスの作 成の負荷を軽減できる可能性があります。 このため、 ログにリアルタイム性が 求められるのか、 一定時間ごとに検索できればよいのか、 ログに求めるものを 考えることが必要です。

データ形式の変換

インポートするときは、入力として渡されるファイルから形式を変換する作 業が必要になることが一般的です。 たとえば、 CSV や JSON XML といった汎 用的な形式で提供されたファイルであっても少し変換が必要ですし、 独自のフ

-

第 2 章 ログの記録と収集

88

ァイル形式からの変換が必要なときもあります。

変換するときには、単に形式を変換するだけでなく、 不要な空白の除去、 文 字コードの変換、 不適切な文字の排除など、 データの加工が発生します。 当然 のことながら、 変換したことによって、 必要なデータが失われてしまうことは 避けなければなりません。

また、変換できないデータがあった場合に、エラーとするのか、無視するの か、などの対応も決めておきます。 さらに、 取り込むデータにマルウェアが含 まれるなど、セキュリティ面での注意もあります。 このため、 事前にフィルタ して、不正なデータを排除することも検討します。

インポートツールの活用

ログデータをインポートするときは、 専用ツールを使う方法が一般的です。 前 節でも解説した Fluentd や Logstashなどを使うことで、取り込みを自動化で きたり、ストリーム処理やバッチ処理にも対応できたりします。

なお、Fluentd ではログを読み込む設定は前節で解説したものと同様です。 フ ァイルの先頭から読み込むには、 Fluentd の設定ファイルで 「pos_file」 を指 定しないようにします。 また、 「read_from_head」 に 「true」 を指定するとファ イルの先頭から読み込み、 「refresh_interval」 に 「0」 を指定するとファイル の監視を無効化できます。

/etc/fluent/fluentd.conf

<source>

@type tail

path/var/log/messages

tag messages

format syslog

read_from_head true refresh_interval 0

</source>

2-2 インポートとエクスポート

89

Logstashの場合も同様ですが、 設定ファイルで 「sincedb_path」 という項目 に 「/dev/null」という値を設定すると、毎回ファイルの先頭から読み込みます。

/etc/logstash/conf.d/error_filter.conf

input { } file { } path => "/var/log/messages" start_position => "beginning" sincedb_path => "/dev/null"

ただし、いずれも本来リアルタイム監視のためのツールなので、 読み込んだ あとも監視を続けます。 そこで、 読み込んだあとは終了または停止します。 大量のデータがある場合はインポートすることで、 インポート中にエラーが 発生したときに、リトライできるメリットもあります。 大量のデータをインポ ートするときは、その処理にかかる時間も検討してツールを選択します。

エクスポート

システムの内部に記録していたデータを他のシステムでも使えるように出力 する処理をエクスポートといいます。 専用のデータ形式から、 汎用的に使える データ形式に変換して出力することが多く、 他のアプリケーションでも使える ようにするために使われます。

■データ形式の変換

エクスポートするときも、 出力先のシステムや目的に応じて、 適切な形式に 変換しなければ、そのデータを活用できません。 CSV や JSON XMLといった 汎用的な形式のほか、特定のツールに特化した形式に変換することもあります。 このとき、次のようなことを考えます。

第2章 ログの記録と収集

90

● フォーマットの標準化

複数のシステム間でデータをやり取りする場合、 共通の標準フォーマット に合わせることで、 データの一貫性と互換性を保証できる。

● データの圧縮

ZIPなどでログを圧縮してデータ量を減らせれば、ストレージの使用量を 減らせるだけでなく、 ネットワークで転送するときの所要時間を削減でき る。

負荷の分散

複数のエクスポート先に分けてデータを出力することで、 1台あたりの負 荷を軽減し、 高速に転送できる。

・ 非同期処理の採用

リアルタイムにエクスポートするのではなく、 CPUの空き時間などに処理 を進めることで、本番環境の性能に影響を及ぼさずに、大量のログを転送 できる。

大規模なWebサービスやクラウド基盤では、それぞれのサーバーが生成する ログをログ管理システムに集約することで効率よく管理できます。 たとえば、 そ れぞれのサービスからリアルタイムに送信されるログを、 Fluentd や Logstash の機能によって形式を変換し、他のサービスと連携する方法が考えられます。

COLUMN バックアップと監査のためのデータ出力の自動化

法令遵守や内部監査の観点から、 ログデータの長期保存や定期的なバック アップを求められます。 このため、 エクスポートするときは、圧縮して容量 を減らすだけでなく、 保存期間を適切に管理しなければなりません。

これを手作業で実施していると負担が大きいため、スクリプトなどを作成 して自動化するか ジョブ管理ツールなどが使われることが一般的です。

2-2 インポートとエクスポート

91

エージェント型と

3

エージェントレス型

役立つのはこんなとき

エージェント型とエージェントレス型の違いを知りたい > ZabbixやNagiosなどを導入したい

ログを収集するとき、 大きく分けて2通りの考え方があります。 それは、エ ージェント型とエージェントレス型です。 それぞれの特徴を考慮して、ログを 収集する方法を選ばなければなりません。

エージェントの基本概念と役割

ログを収集するとき、 監視対象のコンピュータからログ情報を発信するのか、 監視対象のコンピュータに接続してログ情報を収集するのかによって、 必要な 準備が変わります。 それぞれにメリットとデメリットがあるため、その違いを 解説します。

エージェント型

監視対象のコンピュータからログを発信するには、 発信するためのプログラ ムが必要です。 一般的なプログラムは、 利用者の目的に応じて起動し、その目 的が終われば終了します。 単独で動作することが多く、 利用者の操作に対応し、 与えられた入力に対する応答を出力することが一般的です。

一方で、 サーバーやパソコン、 loTデバイスなどでは利用者の操作がなく動

92

作しているプログラムもあります。 このとき、どのような処理が動いているの かを調べたい場合、 常駐してバックグラウンドで動作することが求められます。 そして、 明示的に入力が与えられるわけではなく、 自動的に情報を収集して他 のサーバーに送信するような目的で動くプログラムもあります。

このような目的で、それぞれの端末で動作するプログラムとしてエージェン トがあります。 エージェントは小型のプログラムで、システムの状態の監視や ログの収集、パフォーマンスの計測など、さまざまな目的で動作しています。

このようなエージェントを、 監視対象のコンピュータにインストールして、管理 サーバーなどにデータを送信する仕組みをエージェント型といいます (2-4)。 監視対象のコンピュータでエージェントが常駐して動作するため、 監視対象に ついての状態を詳細に把握でき、 管理サーバーとの間で通信が途切れたとして も、監視対象のコンピュータとエージェントが動作していれば、 データを収集 できます。 通信が再開したときに、それまでに蓄積したデータを送信すればい いのです。

図2-4 エージェント型

管理サーバー ログ 監視対象 エージェントを インストール

ただし、 監視対象となるコンピュータのCPUやメモリといったリソースを消 費するため、負荷がかかることが難点です。 もちろん、 最近のコンピュータは CPUも高速で、 メモリも大容量になりつつありますが、 IoT 端末など性能の低 い端末ではパフォーマンスの問題が発生する可能性があります。

-

2-3 エージェント型とエージェントレス型

93

エージェントレス

監視対象のコンピュータに対してエージェントのようなアプリケーションを インストールせずに、外部から監視するような方式をエージェントレス型とい います(図2-5)。 管理サーバーからSNMP5やICMP WMI7といったプロト コルを使って、監視対象との間で通信してデータを収集します。 エージェント 型と比べると初期設定や更新などにかかる作業が少なく、 監視対象にかかる負 荷も小さいことが特徴です。

図2-5 エージェントレス型 管理サーバー ログ 監視对象 エージェントは 不要

ただし、監視対象のコンピュータとの間でネットワークがつながらないよう な状況では、その動作状況を把握できません。 通信が再開しても、 切断されて いた間の状況を取得できないこともあります。

5 Simple Network Management Protocolの略で、 ネットワーク経由で監視するときに使われる簡易的な プロトコル

Internet Control Message Protocol の略で、問題なく通信できているかを確認するときに使われるプロ トコル。

7 Windows Management Instrumentation の略で、 Windows コンピュータの管理情報にアクセスするた めに使われるプロトコル。

第2章 ログの記録と収集

94

エージェントのメリットとデメリット

それぞれの端末にエージェントを導入すると、 各システムの状態を詳細に把 握でき、中央集約的な監視と管理が可能となります。 また、 エージェントから リアルタイムにデータが送信されることで、障害や不正アクセスの兆候があっ たときに、迅速に対応できます。 たとえば、 CPU 使用率に異常な値があったり、 ネットワークの通信量が急増したりした場合、即座に通知できます。

システムの規模が拡大したときも、 エージェントを追加するだけで、 均一な 監視体制を維持できます。 このような仕組みは、 多くの端末を管理する大規模 なシステムや、クラウドで分散管理するような環境で有効です。

まずは、どの端末にエージェントを導入するのか、 そして収集するデータの 種類を検討します。 たとえば、 Web サーバー、 データベースサーバー、 各種 IoTデバイスなど、 端末ごとに収集する情報が異なる場合は、それぞれの要件 を整理します。

このとき、リアルタイム性を重視する場合と、 バッチ処理による集約を想定 する場合で、 エージェントのデータ送信頻度やデータ形式が変わってきます。

エージェントは便利な一方で、 端末を追加するときには、エージェントのイ ンストールが必要です。 これについては、 自動プロビジョニングツールが使わ れており、それほど大きな問題ではなくなってきました。 プロビジョニングと は、 必要なソフトウェアの導入や設定をする一連の作業を指します。 これを自 動化することで、人為的なミスを防ぎ、 一貫性のある設定ができます。

このことから、 最近はエージェント型

が多く使われているように感じます。

エージェントレス型のGraylog

エージェントレス型の代表的なログ収集ツールとして Graylogがありま す。 syslog などからログを収集するもので、 MongoDB や OpenSearch

2-3 エージェント型とエージェントレス型

95

(Elasticsearch) と組み合わせて動作します。

インストール手順などが複雑なこと、監視できるログの種類が少ないことも あり、本書では割愛します。 公式サイトなどを確認して試してみてください。

以下では、Zabbix と Nagiosという2つのツールをエージェント型で導入す る方法について、 具体的なインストールや設定について解説します。

Zabbix

監視対象に専用のエージェントソフトウェアをインストールして詳細な情報 を収集する 「エージェント監視」 と、 SNMPやIPMI SSH、 Telnet などのプ ロトコルを使ってエージェントを使わずに監視する 「エージェントレス監視」 の両方に対応しているソフトウェアとしてZabbixがあります。

■ Zabbixの特徴

Zabbix は、 オープンソースの統合監視ソフトウェアです。 サーバーだけでな く、 ネットワーク機器、 クラウドサービス、 アプリケーションなど幅広く一元 的に監視できることが特徴です。

単なる障害の監視だけでなく、性能の監視や容量の管理、 傾向分析などさま ざまな視点で監視できます。 具体的には、ハードウェアの稼働状態やネットワ ークのトラフィック、 Webアプリケーションの応答速度、 データベースのクエ リ時間などを収集し、 可視化できます。

Zabbix は収集したデータに複雑な条件式 (トリガー) を設定でき、 閾値を超 えた場合にアラートを発生させることができます。 そして、 メールやSMS、 チ ャットツールなどに通知できるため、 障害の発見から対応までのフローを効率 化できます。

ネットワーク内に新しい機器が接続されると自動的に検出し、これを監視対

8 Elasticsearch をよりオープンなライセンスとしてフォークしたものがOpenSearch。

第2章 ログの記録と収集

96

象として自動登録する機能も持っています。 これにより、 監視対象の増減に伴 う管理負担を軽減できます。 また、収集したデータをリアルタイムにグラフ表 示でき、 ダッシュボードで監視状況を一目で把握できます。

Zabbix は 「アイテム (Item)」 という単位で監視データを収集します。 アイ テムはCPU負荷やディスク使用率、 HTTP応答時間といった具体的な計測項目 を指します。 これらのアイテムに対して 「トリガー (Trigger)」を設定し、 定の条件を満たすと障害や警告として検知します。

たとえば、 CPU 使用率が90%を超えたら警告を出す、 ディスク容量が10% 未満になったらクリティカルとする、 といった設定が可能です。 監視データは 定期的に収集され、 データベースに保存されます。

■ Zabbixのインストール

Zabbix のインストールについては、公式サイトに詳しく掲載されています (図2-6)。導入する Linuxのディストリビューションなどを選択し、それぞれ のバージョンに合わせて用意された手順に沿って導入するだけです。

図2-6 Zabbixの公式サイト 9

1 Choose your platform ZABBIX VERSION OS DISTRIBUTION OS VERSION ZABBIX COMPONENT DATABASE WEB SERVER Server Frontend Agers 72 70 LTS BOLTS Amazon Linux Centos 9 amd64 arm64) PostgreSQL D Server Frontend Agent 2 Proxy OpenSUSE Leap Oracle Lina Raspberry ROS Red Hat Enterpri Linux Rucky Linux SUSE LI Enterprise Server Agent Agent 2 Java Gateway Web Service

9 https://www.zabbix.com/download

2-3 エージェント型とエージェントレス型

97

ここでは、 Alma Linux 9にインストールします10。 まずはAlma Linux に root ユーザーでログインし、次のコマンドで最新の内容に更新します。

# dnf update -y

さらに、Zabbix の公式リポジトリを追加します。 ここではZabbix 7.4を例 に解説します。

# rpm -Uvh https://repo.zabbix.com/zabbix/7.4/release/alma/9/noarch/ zabbix-release-latest-7.4.el9.noarch.rpm # dnf clean all

そして、 ZabbixサーバーやWeb フロントエンド、エージェント、日本語の ロケールパッケージをインストールします。

# dnf install -y zabbix-server-mysql zabbix-web-mysql zabbix-apache- conf zabbix-sql-scripts zabbix-selinux-policy zabbix-agent glibc- langpack-ja

続いて、 初期データベースを作成します。 ここでは、 MariaDB (MySQL) をインストールし、 有効化します。

# dnf install -y mariadb-server # systemctl enable --now mariadb

さらに、MariaDB (MySQL) の root ユーザーのパスワードを設定し、不要

10 執筆時点でAlma Linux 10がリリースされているが、安定性について検証できていないため。 Alma Linux 10でも同様の手順でインストールできると思われる。

98

なユーザーを削除します。

# mysql_secure_installation

そして、 Zabbix用のデータベースとユーザーを作成します。

# mysql -uroot -p CREATE DATABASE zabbix CHARACTER SET utf8mb4 COLLATE utf8mb4_bin; CREATE USER zabbix@localhost IDENTIFIED BY 'p@ssword'; GRANT ALL PRIVILEGES ON zabbix.* TO 'zabbix'@'localhost'; SET GLOBAL 10g_bin_trust_function_creators = 1; EXIT;

最後に、Zabbixデータベーススキーマをインポートします。 次のコマンド を実行するとパスワードの入力を求められるので、上記で指定したパスワード (p@ssword) を入力してください。

#zcat /usr/share/zabbix/sql-scripts/mysql/server.sql.gz | mysql --default-character-set=utf8mb4 -uzabbix -p zabbix

これで導入は完了です。

Zabbixの設定

引き続き、 Zabbixのサーバーについての設定を進めます。 Zabbixサーバー の設定ファイルは、 「/etc/zabbix/zabbix_server.conf」 にあるので、ここに上 記の MariaDB で作成した 「zabbix@localhost」 というユーザーのパスワードを 指定します。

2-3 エージェント型とエージェントレス型

99

/etc/zabbix/zabbix_server.conf

~中略~

DBPassword=p@ssword

~中略~

そして、PHPについての設定をします。 Zabbixは Web サーバー上で動作す るため、 PHPのタイムゾーンを設定します。

/etc/php-fpm.d/zabbix.conf

php_value[date.timezone] = Asia/Tokyo

設定が完了すれば、サービスを起動して有効化します。

# systemctl restart zabbix-server zabbix-agent httpd php-fpm # systemctl enable zabbix-server zabbix-agent httpd php-fpm

最後に、 ファイアウォールでHTTP (80番ポート) を開放すると、 リモート 環境のパソコンからもZabbixの Web サーバーに接続できます。

# firewall-cmd --permanent --add-service=http # firewall-cmd --reload

Zabbix を導入したサーバーのIPアドレスを指定した次のURLにアクセスす ると、 図2-7 のような画面が表示されます。

http://< サーバーIPまたはホスト名>/zabbix

100

図2-7 Zabbix の Web 設定画面

ZABBIX ようこそ 条件のチェック データベース接続設定 設定 ようこそ 設定パラメータの確認 インストール Zabbix 7.4 デフォルトの 日本語 (n_jp)

次のステップ

ライセンスはAGPLv3

ここで初期設定を進めます。 画面の指示にしたがって、 データベースのパス ワードを入力すると設定が完了します。 その後、 ログイン画面が表示されるの で、IDとして「Admin」、パスワードに 「zabbix」 を入力することで、Zabbix のスーパーユーザーとしてログインできます。

これ以降のZabbixについての設定はこのWeb画面の操作で、 日本語にも対 応しているので直感的に扱えるはずです。 主な設定項目は表2-6の通りです。

表2-6 Zabbixの設定

設定項目 ホスト登録 内容 監視対象のサーバーやネットワーク機器を登録する (IPアドレスやホスト名を指定する) 監視する具体的なデータ項目を定義する アイテム設定 トリガー設定 (CPUやメモリ使用率など) 障害判定の条件式を作成する (複数の条件を組み合わせることもできる)

エージェント型とエージェントレス型

3

2

101

設定項目 アクション設定 内容 障害発生時の通知方法や実行するスクリプトを指定する (メールの送信や自動復旧) テンプレート管理 監視設定のひな形を作成し、 一括で適用する (運用を効率化する)

■ Zabbix エージェントの導入

上記ではZabbix エージェントも導入しているため、導入した端末がWeb画 面にホストとして表示されているはずです。 他の端末も監視対象とするために は、その端末にZabbixエージェントを導入します。

たとえば、 Alma Linuxをクライアントとして監視対象にするには、次のコ マンドでZabbixエージェントを導入します。

# dnf install -y zabbix-agent

続いて、 設定ファイルでZabbixサーバーのIPアドレスを指定します。

/etc/zabbix/zabbix_agentd.conf

Server = < ZabbixサーバーのIPアドレス> ServerActive = <ZabbixサーバーのIPアドレス> Hostname= < 監視対象ホスト名>

そして、エージェントを起動し、 サービスとして登録します。

# systemctl start zabbix-agent # systemctl enable zabbix-agent

第2章 ログの記録と収集

102

最後にZabbixサーバー側のWeb画面から 「監視データ」 「ホスト」 → 「ホ ストの作成」で監視対象のホストとして登録すると、 監視が始まります。

Nagios

サーバーやネットワーク機器、 アプリケーションなどの状態を監視し、障害 や異常を早期に検知し、 管理者に通知するオープンソースソフトウェアとして Nagiosがあります。

Nagios Cl

Nagiosはエージェント型で動作し、サーバーやネットワーク機器だけでなく、 HTTP SMTP POP3 FTP、 SSH といった各種サービスの稼働状況も監視 できます。 さらに、 CPU 使用率、 メモリ使用量、 ディスク容量 プロセスの状 態など、 システムリソースの監視にも対応しています。

最大の特徴は、 プラグインによる拡張性です。 標準で多くのプラグインが用 意されているほか、独自にプラグインを作成して監視項目を追加できます。 プ ラグインは外部プログラムとして動作し、 Nagios本体はこれらのプラグインを 呼び出して監視結果を取得します。

監視対象に異常が発生した場合、 メールやSMS、 チャットツール (Slackな ど) を通じて管理者に通知します。 通知のタイミングや条件は柔軟に設定でき、 障害の検知から復旧までの状況をリアルタイムで把握できます。

Nagios は障害発生時に自動的にスクリプトやコマンドを実行して復旧作業を 試みることもできます。 たとえば、サービスの再起動やサーバーのリブートな どのアクションを設定できます。

Zabbix と同様に、 Web ブラウザから監視状態の一覧表示や履歴の確認、設 定の変更などができるため、 いつでもどこでも監視状況を把握できます。

Nagios は定期的に設定された間隔で監視を実行し、 プラグインが返す結果に 応じて正常 (OK)、 警告 (WARNING)、 異常 (CRITICAL) などのステータ

2-3 エージェント型とエージェントレス型

103

スが決定されます。

Nagiosのインストールと設定

ここでは、Alma Linux 9 に Nagios をインストールします。 まずはシステムを アップデートします。 そして、 EPEL (Extra Packages for Enterprise Linux) を使うように設定しておきます。

# dnf update -y # dnf config-manager --set-enabled crb # dnf install -y epel-release # dnf makecache

続いて、 Nagios をインストールします。 次のコマンドを実行すると、 関連す る Web サーバー (httpd ) やPHPなども導入されます。

# dnf install -y nagios nagios-common nagios-plugins nagios-plugins- all nrpe

インストールが完了すれば、 Nagiosのサービスを開始し、 有効化します。

# systemctl start nagios # systemctl enable nagios.

す。 そして、 Web サーバー (httpd) についてもサービスを開始し、 有効化しま

# systemctl start httpd # systemctl enable httpd.

第2章

104

最後に、ファイアウォールでHTTP (80番ポート)を開放します。

# firewall-cmd --permanent --add-service=http # firewall-cmd --reload

これで、リモート環境のパソコンからもNagiosのWebサーバーに接続でき ます。 動作確認のため、 Web ブラウザで次のURLにアクセスします。 http://< サーバーIPまたはホスト名> / nagios

最初に BASIC認証の画面が表示されますが、 ユーザー名とパスワードの両方 に 「nagiosadmin」 を入力するとログインできます。 これでNagiosのダッシュ ボードが表示されれば成功です (図2-8)。

図2-8 Nagiosのダッシュボード

Nagios Home Documentation Current Status Tactical Overview Hosts Services Нам Groups Summary Gnd Service Groups GM Problems Services (Unhandled) Hosts (and) Get Started Start monitoring your infrastructure Change the look and feel of Nagios Extend Nagios with hundreds of addons Get support Get training Get certified Latest News Nagios' Core™ Daemon running with PID 70571 Naglos® Core™ Version 4.4.14 August 01, 2023 Check for updates Don't Miss... Quick Links Nagics Library (utorials and doca) Nagics Labs (development blog) Nagios Exchange (plugina and Nagics Support (techsupport) Nagios.com (company) Nagios.org (project) Copyright 2010-2023 Ng Care Devant Team and Community Contr 1999-2009 En G See the Threation

Corred under the GNU General Pure and AS IS with NO WARRANTY OF ANY KIND, INCLUDING THE WARRANTY OF DESIGN A PARTICULAR PURPOSE H

Nagios

3 エージェント型とエージェントレス型

1

2

105

この左側のメニューから 「Hosts」を選択すると、監視対象のホストの一覧 が表示され、まずは今回のAlma Linuxが表示されているはずです。 同様に、 「Services」を選択すると、監視しているサービスが表示されます。

また、「Reports」の中にある 「Alerts」 を見ると、 警告メッセージが表示さ れていることがわかります。 その他、 「Event Log」 を見ると、 警告以外のログ も表示できます。

Nagios の設定は、表2-7 の設定ファイルで変更します。 これらの設定ファ イルは 「/etc/nagios/」 にあります。

表2-7 Nagiosの設定ファイル

設定ファイル

内容

nagios.cfg

Nagios の全体設定

(監視間隔やログファイルの場所など)

cgi.cfg

nrpe.cfg

CGI関連の設定

後述するNRPE (Nagios Remote Plugin Executer) の設定

passwd

objects/commands.cfg

BASIC認証のユーザー名とパスワード

コマンド名とコマンドの設定

objects/contacts.cfg

アラート通知先の情報

(メールアドレスや通知方法など)

objects/localhost.cfg

localhost (Nagios サーバー) の監視設定

(IPアドレスやホスト名など)

objects/printer.cfg

プリンターの監視設定

objects/switch.cfg

スイッチの監視設定

objects/templates.cfg 設定ファイルのテンプレート

objects/timeperiods.cfg 通知の時間帯の設定

objects/windows.cfg

Windows サーバーの監視設定

第2章 ログの記録と収集

106

監視対象のホストを追加するには、 この 「objects」 ディレクトリ内に 「hosts. cfg」などの名前でファイルを作成し、 設定を記述します。 たとえば、次のよう なファイルが考えられます。

/etc/nagios/objects/hosts.cfg

define host { use host_name alias address } linux-server myserver Example Host 192.168.1.100

このファイルでは、 「define host」 でホストを定義しています。 それぞれの 設定内容は表 2-8 の通りです。

表2-8 監視対象ホストの設定

パラメータ 内容 use host_name alias address 既存のホストテンプレート linux-server」はLinuxサーバー用の基本設定) Nagios 内でホストを識別するための名前 (今回は 「myserver」) ホストの説明や別名 (Web画面で表示される名前) 監視対象のホストのIPアドレスやホスト名

「この 「use」 で指定するテンプレートには、ホストに共通の設定 (チェック間 隔、通知方法など) をまとめて定義しておけます。

続いて、ホストを監視するサービスを定義します。 「objects」 ディレクトリ 内に 「services.cfg」 などの名前でファイルを作成し、次のような設定を記述 します。

3 エージェント型とエージェントレス型

1

2

107

/etc/nagios/objects/services.cfg

generic-service example-host PING check_ping!100.0,20%!500.0,60% } define service { use host_name service_description check_command

それぞれの設定内容は 表2-9 の通りです。

表2-9 監視対象サービスの定義

パラメータ use host_name service_description check_command 内容 既存のサービステンプレート (「generic-service」は基本的なサービス監視設定) 監視するホストの名前 (上記の監視対象ホストの識別名と一致させる) 監視するサービスの名称 (Web画面で表示される名前) 実行する監視コマンド (ここでは標準プラグインのping を実行)

なお、 「check_command」 で指定している 「100.0,20%! 500.0,60%」という パラメータは、最初の 「100.0,20%」 で警告の閾値を設定しています。 これは、 100.0ms の応答時間か、 20% のパケットロスで警告状態になることを意味し ます。 また、次の 「500.0,60%」 はクリティカル閾値と呼ばれ、 これは500.0ms の応答時間か、60% のパケットロスでクリティカル状態になることを意味しま す。

2章 ログの記録と収集

第

108

Nagios エージェントの導入

Nagiosのエージェントとして、 「NRPE (Nagios Remote Plugin Executor)」 や「Nagios Agent (NSClient++)」 などがあります。 ここではNRPEの導入 について解説します。

NRPEを導入するには、 次の手順で進めます。

1. 端末の準備

監視対象の端末に NRPE と Nagios プラグインをインストールする。 2.NRPEの設定

設定ファイル 「/etc/nagios/nrpe.cfg」 にて Nagios サーバーからの接続 を許可するIPアドレスを指定する。

3. サービスの起動

NRPE サービスを開始し、 自動起動を設定する。

4. Nagiosの設定

Nagios サーバー側で、 NRPE を使った監視コマンドを定義して割り当て る。

5. 動作の確認

Nagios サーバーからNRPE経由でコマンドを実行し、正常に監視できる かを確認する。

ここでは、 Alma Linux を導入した端末に Nagios エージェントを導入するこ とにします。

まずはNRPE サーバーとNagios プラグインを導入します。

# dnf update -y # dnf config-manager --set-enabled crb # dnf install -y epel-release # dnf makecache

-

2

1

3

エージェント型とエージェントレス型

109

# dnf install -y nrpe nagios-plugins

続いて、 NRPE の設定をするため 「/etc/nagios/nrpe.cfg」 を開き、 このフ アイルの「allowed_hosts」 の末尾に、 NagiosサーバーのIPアドレスを追加し ます。

/etc/nagios/nrpe.cfg

allowed_hosts=127.0.0.1, <NagiosサーバーのIPアドレス>

そして、 NRPE サービスを開始し、 自動起動を設定します。

# sudo systemctl restart nagios-nrpe-server

# sudo systemctl enable nagios-nrpe-server

Nagios サーバー側でNRPEコマンド設定するため、 サーバー側の 「 /etc/ nagios/objects/commands.cfg」 を編集します。 あとはNagios サーバーから正 常に監視できることを確認できればOKです。

監視できない場合は、 ファイアウォールでNRPEのポート (通常5666番) が 開いているかなどを確認してください。

COLUMN Prometheus Grafana

時系列データベースと監視ツールを備えたツールとして Prometheus や Grafana があります。 ZabbixやNagios と併用することで、より詳細な監視 を実現でき、 グラフ化やカスタマイズが得意で、監視用のダッシュボードを 作成するときは便利です。

これらのツールの導入や使い方について、詳しくは第5章で解説します。

第2章 ログの記録と収集

110

2

4

2-4 ログの保存と圧縮、 アーカイブ

ログの保存と圧縮、 アーカイブ

役立つのはこんなとき

ログの保存方法について知りたい

セキュリティを意識してログを管理したい

出力されるログをすべて収集していると、その量は膨大になります。 これら のデータはそれほど頻繁に使うことはなく、分析に使えばそのあとはどこかに 保存されていれば十分です。

ただし、一般的なデータとは異なり、 法律などで保管期間が決められていた り、改ざんされたりしないように適切に管理する必要があります。 このときの 注意点と、 圧縮やアーカイブの手法について解説します。

また、ログの改ざんなどに備えなければならないため、 ログへのアクセス制 限や、改ざんの防止、 タイプスタンプの付与について解説します。

ログを長く保存する

まずは膨大な量のログを長く、 失われないように保存することを考えなけれ ばなりません。 このときの保存方法について考えます。

ログの保存先

ログを保存するとき、 その保存先としてローカルストレージ、ネットワークス トレージ、クラウドストレージなど複数の場所が考えられます。 特にローカル

111

ストレージについては、内蔵のハードディスクやSSDのほか、CDやDVD、 外 付けハードディスクなどさまざまな媒体があります。 それぞれの特徴は表2-10 の通りです。

表2-10 ローカルストレージの種類

媒体 特徵 内蔵ハードディスク 安価に大容量のデータを保存できるが、 衝撃に弱い SSD CD + DVD 高速で衝撃に強いが、 容量あたりの単価は高い 容量はそれほど大きくないが、 1度保存したものを書き換えら れない、もしくは書き換え回数が限定的 外付けハードディスク やUSBメモリなど 使うときに接続が必要なのが面倒だが、 取り外して使用でき る

これらはいずれもネットワークに接続していない状態でもコピーできること が特徴です。 内蔵ハードディスクやSSDはもっとも手軽ですが、 誤操作やウイ ルス感染、 故障などでデータが失われる可能性があることを意識しなければな りません。

CDやDVD、 外付けハードディスク、 USBメモリは、差し替えることで多く のデータを保存できますが、 手作業が発生するため手間がかかります。

ネットワークストレージはNASとも呼ばれ、 複数のハードディスクを格納し てRAIDなどに対応したものが多く販売されています。 ファイルの共有やバッ クアップの用途で使われることが多く、 複数のデバイスから接続してファイル を同期できます。

さらに、オンラインストレージとして、 OneDrive や Google Drive、 Box、 Dropbox などがあります。 これらはインターネットなどのネットワークに接続 していないと、データを保存できませんが、 遠隔地に保存できるメリットがあ ります。 しかしサービスの事業者によって管理されるため、 アカウントを停止 されると自分のデータにアクセスできなくなりますし、 大容量を保存する場合 にはコストが増加します。

第2章 ログの記録と収集

112

データのバックアップについては 「3-2-1ルール」 が知られています。 これ はデータの「3つのコピー」 を 「2種類のメディア」 に保存し、 そのうち「1つ は離れた場所」 に保存する、 というものです。 ログについても失われると困る ため、このルールに沿って保存しておくとよいでしょう。

コログの保存期間

会社などの組織では、重要なデータの保存期間を 「情報管理規程」などの文 書で定めていることが多いものです。 この保存期間を決めるとき、 法律や業界 でのガイドラインを参考にすることが多いでしょう。たとえば、請求書や領収 書といった会計に関するデータでは、 法人税法などで7年から10年といった期 間が決められています。

これはログでも同じで、 業務内容によってさまざまなガイドラインがありま す。 たとえば、 クレジットカードを扱う事業者であれば、 PCI DSSに関連する 「データセキュリティ管理基準」 が定められています。 また、 金融機関であれば、 金融情報システムセンター (FISC) による 「金融機関等コンピュータシステム の安全対策基準」 が、 医療機関向けには厚生労働省による「医療情報システム の安全管理に関するガイドライン」 があります。

このように、 特定の業界向けに定められた管理基準がある一方で、 一般的な 企業に向けたガイドラインはありません。 そのため、 ISMS、 プライバシーマ ークなどに対応するためにログを取得していることも多いです。 これらへの対 応だけであれば1年程度の保存期間で十分でしょう。

また、犯罪に関するログを調査するときには、法律によって保存期間が決め られていることもあります。 たとえば、 外部からサイバー攻撃を受けた、 内部 の従業員がデータを持ち出していた、といった事案が発生したときにその証拠 となるログを残しておきたい場合です。 犯罪に関する法律やJ-SOX法 (内部統 制) などでは、 表2-11 のような期間が保存期間として考えられます。

I

4 ログの保存と圧縮、 アーカイブ

2

113

表2-11 保存期間の例

法律 保存期間 根搥 刑事訴訟法 1ヶ月間 第197条 「通信履歴の電磁的記録のうち必要な ものを特定し、三十日を超えない期間を定めて、 これを消去しないよう、 書面で求めることができる」 サイバー犯罪に関す 3ヶ月間 る条約 第16条2 「必要な期間 (九十日を限度とする)、 当該コンピュータ データの完全性を保全し及び 維持することを当会社に義務付けるため、 必要 な立法その他の措置をとる」 不正アクセス禁止法 3年間 時効 (刑事訴訟法第250条2項6号) 脅迫罪 3年間 時効 (刑事訴訟法第250条2項6号) 金融商品取引法 5年間 第25条 「内部統制報告書及びその添付書類並 びにこれらの訂正報告書 五年」 電子計算機使用詐 欺罪 7年間 時効 (刑事訴訟法第250条2項4号) 詐欺罪 7年間 時効 (刑事訴訟法第250条2項4号) 竊盜罪 7年間 時効 (刑事訴訟法第250条2項4号)

これを踏まえると、 一般的な企業では5年から7年程度の保存が必要であると 考えられます。 なお、 単にデータを保存しておけばよいわけではなく、 あとで 解析できるようにまとめて保存しておきます。

最低でも、 ログがある程度の時間で見つけられるように検索できる仕組みが 必要です。 そして、 書き換えられない、システム障害などでも失われない、と いった対策が必要です。

第2章 ログの記録と収集

114

■ ログ圧縮とアーカイブ

大量のログデータを保管しようとすると、 保存媒体の容量を圧迫するため、圧 縮して保存することが一般的です。 圧縮により、 ストレージの容量を節約でき るだけでなく、 ネットワーク経由で転送するときの時間も削減できます。 また、長期的に保存するためにはアーカイブも必要です。

ログの圧縮

一般に、大容量のファイルを転送するときには圧縮という手法が使われます。 圧縮には可逆圧縮と非可逆圧縮の2通りがあります。 可逆圧縮は圧縮前のデー タを完全に復元できる手法で、 ZIPやLZH RARなどがあります。 一方の非可 逆圧縮は一部のデータを削除して圧縮率を高める手法で、 JPEGやMP3、MP4 などがあり、 画像や音声、動画などに使われます。

ログを圧縮するときはデータが失われると困るため、 可逆圧縮が使われます。 このときに使われる圧縮アルゴリズムは、一般的な圧縮ツールと同じで、 ZIP やLZHなどが使われます。 Windows では標準でZIPが用意されていますし、 UNIX系のOSではtar.gz (GZIP) などが使われることが一般的です。

ログは数年単位で保管されるため、特殊な圧縮形式を使ってしまったことで 数年後に復元できないといったことが起きると大変です。 このため、 幅広く使 われている一般的な圧縮形式を使います。

ログのアーカイブ

データを内蔵ストレージに保存しておくと、それだけ容量を消費するだけで なく、誤操作で消去してしまう、ウイルスに感染する、 故障によって失われる といったリスクがあります。 過去のログを頻繁に参照することは基本的にあり ませんので、可能であれば端末から分離し、 保存しておきたいものです。

2-4 ログの保存と圧縮、 アーカイブ

115

このように、長期的に保存するために他の場所に移動し、 効率よく管理する 目的で保管することをアーカイブといいます。 アーカイブすることでメインの ストレージに保存されている容量が減り、 検索速度を維持できます。

第2章 ログの記録と収集

また、過去のログが必要になる場合でも、 月単位や年単位など時系列で決め られた範囲だけを保管しておくと、 速やかに調べられます。 監査や法的な調査 を求められた場合でも、スムーズに対応できることは重要です。

さらに、長期保存に適したストレージの種類を選択することで、コストを削 減できる可能性もあります。 最近では、 AWSをはじめとしたクラウドサービス を使ったシステムが増えていますが、 この場合もアーカイブ用のストレージを 使うと、 保存コストを下げられます。

アーカイブの手法

アーカイブするときは、どのタイミングまで手元に保管するのかを決めなけ ればなりません。 アーカイブしてしまうとすぐには検索できなくなるため、 検 素が必要な事態が生じる可能性を考慮して期間を設定します。 もちろん、 アー カイプしても、その情報をローカルストレージに戻すことで検索できるように なりますが、 その手間が発生します。

たとえば、直近3ヶ月は手元の内蔵ストレージに保管し、 それより古いもの はアーカイブするといった運用ルールが考えられます。 この期間はシステムの 規模や、ログの検索を要求される頻度、 元に戻す手間などによって変わります。 アーカイブの保存先としてよく使われるのは、階層的なストレージを使う方 法です。 階層的というのは、 新しいデータは高速にアクセスできるストレージ に保存し、 一定期間が経過すると低コストで利用できるアーカイブ用ストレー ジへ移動させる方式です。

たとえば、 AWS (Amazon Web Services) を使っている場合には、普段は Amazon S3 (Simple Storage Service) に保管しておき、 一定期間が経過す ると S3 Glacier というアーカイブ用のストレージに移行する方法が考えられま す。 これにより、性能は低下するものの保管する料金を安く抑えられます。

116

ログの閲覧や改ざんを防ぐ

ログを保存していても、 第三者によってログにアクセスされると、記録され ている内容を閲覧されてしまいます。 ログには重要な情報が含まれるため、 適 切なアクセス制限を実施しなければなりません。 これは分析に使う最新のログ だけでなく、 アーカイブなどで保存しておく過去のログでも同じです。

また、ログを改ざんされると、その記録の信頼性が揺らいでしまいます。 な ぜ改ざんされるのか、 そして改ざんを防ぐための技術について解説します。

ログのアクセス制限

ログに個人情報や機密情報を出力すべきでないことは第1章で解説しました が、それでもログが第三者に閲覧されると問題になることがあります。 たとえ ば、ログを見るだけで、使っているソフトウェアやそのバージョン、処理内容 がわかることがあります。 この情報を使って脆弱性の有無を調べて攻撃するこ とが可能になるかもしれません。 単体のログでは問題なくても、 複数のログを 組み合わせることで、システムの動作状況がわかり、 それを悪用されるかもし れません。

また、ログにはシステムの操作履歴エラーメッセージ、IPアドレスなどの情 報が含まれることもあります。 現時点では問題なくても、 将来的にログの出力 内容が変更されることで、 セキュリティ上の問題が発生する可能性もあります。 このため、ログからの情報漏えいなどを防ぐ意味でも、アクセス権限が最小 隈になるようにしなければなりません。 これを実現するために、暗号化とアク セス制御、認証についての考え方は必須です。

たとえば、ログを転送するときには、盗み見を防ぐためにも暗号化が必須で す。また、ログを保管するときには、 利用者を認証し、 必要な権限を持ってい るかを確認するアクセス制御を実施します。

Fluentdの解説をしたときに、 「/var/log/messages」 へのアクセス権限につ

4 ログの保存と圧縮、 アーカイブ

2

117

いて解説しました。 最近のLinux では、標準設定でroot以外はアクセスできな いように設定されているはずです。 この権限を増やすときも、 誰でもアクセス できるように設定するのではなく、 管理者として追加したユーザーだけに限定 しなければなりません。

Linux であれば「1s -1」というコマンドで、 アクセス権限を確認できます。 行の左端が 「-rw-------」のようになっていると、 所有者しかアクセスできな いことを意味します。 そして、 所有者が 「root」 になっていることを確認すれ ば、 root 以外は読み書きできないことを確認できます。

$ ls -1 /var/log/messages

$

-rw------- 1 root root 7320 5 1 12:34 /var/log/messages

このアクセス権限を変更するには、 「chgrp」 や 「chmod」、 「usermod」 コマンド を使います。 Fluentdの解説で使ったように、次のように実行すると、 「admin」 というグループに読み込み権限を付与でき、 この 「admin」 グループに 「fluentd」 というユーザーを追加できます。

# chgrp admin/var/log/messages # chmod 640 /var/log/messages # usermod -aG admin fluentd

ログを改ざんする目的

次にログの改ざんについて考えます。 ログを改ざんする理由として、 不正行 為を隠すことが挙げられます。

たとえば、 従業員が悪意を持ってデータを持ち出した場合です。 USBメモリ を使って機密データをコピーし、 売却するといった状況が考えられます。 この とき、 USBメモリを接続した、 データをコピーした、 といった行為の際にはロ

第2章 ログの記録と収集

118 -

グに記録が残ります 11。 このログを調べることで、いつ誰がどのデータを持ち出 したのかがわかります。

このとき、 ログを消去したり、 書き換えたりすることで、 その証拠がなくな るのです。 ログを持ち出した人にとっては、そのログを改ざんすることを考え るでしょう。

これは、外部からの攻撃者の立場でも同じです。 何度もログインを試行する と、ログインに失敗したログが残りますし、 どのサーバーにログインして、ど のデータを盗み出したのか、 どのファイルを書き換えたのか、といった内容が ログに残っていると、 自分の操作についての証拠が残っていることを意味しま す。

そこで、このログを消去したり、 書き換えたりすることを考えます。

改ざん防止の技術

ログの改ざんを防ぐためには、 さまざまな対策が考えられます。 これまでに 解説した内容では、アクセス権限の設定があります。 前節ではアクセス権限と して 「root」 だけが読み書きできる設定について解説しましたが、 この設定で あれば root ユーザーとしてログインしないと書き換えることはできません。 つ まり、 root ユーザーのパスワードを知られないように管理することが求められ ます。

Fluentd などに閲覧権限を与えましたが、 変更権限を与えなければ、ログを 閲覧できても書き換えはできなくなります。 このように、ユーザー単位できめ 細かなアクセス制限を実施することで、ログの改ざんを防ぐことができます。

また、書き換えが物理的に不可能なメディアを使う方法もあります。 たとえ ば、CD-RやDVD-Rといった媒体に保存することで、そのログについては改ざ んができなくなります。 そして、このような媒体を物理的な金庫に格納するな ど、できるだけアクセスできないようにする方法です。

11 ログに記録するには、 利用者の環境によっては事前に設定が必要です。

2-4 ログの保存と圧縮、 アーカイブ

119

さらに、リアルタイムに他のサーバーに転送する方法もあります。 これまで に解説したように、 Fluentd や Logstashなどを使って他のサーバーに転送し ておくと、元のログを改ざんされたとしても、転送先のログを書き換えるのは 難しくなります。

最後に、 ハッシュ値を計算する方法です。 ログが記録されているファイルか らハッシュ値を計算しておくと、ログが書き換えられたときにハッシュ値が一 致しなくなります。 ハッシュの計算にはMD5やSHA1 SHA256などがよく使 われます。 コマンドからこれらを実行するには、次のように実行します。

$ md5sum/var/log/messages

$ sha1sum /var/log/messages

$sha256sum/var/log/messages

そして、計算されたハッシュ値と一致するかを確認するときは、 「-c」 (--check) というオプションを使います。 たとえば、次のようにsha256sum コマンドの 結果を保存しておいたとします。

$ sha256sum /var/log/messages > log. sha256

このファイルを使って、 次のコマンドを実行します。

$ sha256sum -c log.sha256

ファイルの内容が書き換えられていなければ 「OK」 が表示されますが、 書 き換えられていると 「FAILED」 と表示されます。

ただし、ログは頻繁に追加されるため、 ファイル単位でハッシュ値を保持し

第2章 ログの記録と収集

120

ておくのは現実的ではありません。 ログが追加されるたびにファイル全体のハ ッシュ値を計算すると、 CPUの負荷が高くなりますし、 ハッシュ値を頻繁に保 存すると、 ログの容量が増加します。

そこで、 ログの1件ごとにハッシュを計算する方法が考えられます。 これに より、 CPUの負荷も低くなりますし、 署名処理の時間も短縮できます。 しかし、 この場合は1件のログそのものを削除されたり、順序を入れ替えられたりする 改ざんを防ぐことはできません。

これに対応するために、 ハッシュチェーンと呼ばれる仕組みを使う方法もあ ります。 前の記録も含めてハッシュ値を求める方法で、 ブロックチェーンの考 え方に近いものです。 これにより、 連続性を保証でき、 途中の記録を削除した り書き換えたりすると、 ハッシュ値が一致しなくなります。 ただし、 この方法 は実装が複雑ですし、 ログが欠損すると、 チェーンが切れ、それ以降のログを 検証できなくなってしまいます。

整理すると 表2-12 のようになり、それぞれにメリットとデメリットがありま す。

表2-12 メリットとデメリット

デメリット 方法 メリット ファイル単位 実装が比較的容易 ファイルが大きいと負荷が高い ログ単位 細かく検知できる チェーン方式 改ざん検知が強力 ログそのものの削除や入れ替えを防げない 実装が複雑

タイムスタンプの付与

デジタル署名に似た技術として、 タイムスタンプを付与する方法もあります。 ここでの「タイムスタンプ」という言葉は、第1章で解説した「ログに記録さ れている日時」とは異なります。 ログに記録されている日時は、イベントが発 生した日時です。 たとえば、 Web サーバーのアクセスログやエラーログには、

ログの保存と圧縮、 アーカイブ

4

-

2

121

アクセスやエラーが起きた日時が記録されています。 これは、システムの時計 (OSの時刻設定) に依存しますし、ログファイルが改ざんされると日時情報も 書き換えられる可能性があります。

一方、ここで解説している 「タイムスタンプ」は、 「信頼できる第三者機関 (タイムスタンプ認証局) によって発行される電子的な証明書」 を指します。こ れは、あるデータが特定の日時に存在していたことを証明し、 データの改ざん がないことを保証するための根拠となるものです。

基本的にはデジタル署名の技術を使い、 第三者機関が発行した証明書を使い、 改ざん防止のために暗号技術を利用したもので、 法的な証拠能力を持つ場合も あります。

COLUMN コンプライアンスを意識したログ管理

技術的な対策を実施するだけでなく、 組織全体としてログを扱う従業員の コンプライアンス意識の向上も必要です。 どれだけ厳格なアクセス制御を実 施しても、そのデータを勝手に個人で契約しているオンラインストレージに コピーする、CDやDVDにコピーして持ち出す、といった操作をされてしま うと、 無意味なものとなってしまうためです。

まず実施すべきこととして、 従業員への教育があります。 多くの組織では 年に1度程度のセキュリティ研修を実施していますが、 ここでプライバシー の保護やコンプライアンス教育を実施するようにします。これにより、日常 業務における情報管理の重要性を浸透させ、 不注意やミスによる情報漏えい のリスクを低減します。

また、定期的に内部監査を実施し、 ログの管理を含めた対策が遵守されて いるかをチェックします。 必要に応じ、 最新の法令やガイドラインに基づい また運用の改善を検討するとよいでしょう。 具体的には、プライバシーポリシ ーの見直しやデータ管理などについての規定を変更し、それぞれの部署での 遵守状況をモニタリングする仕組みを構築する方法もあります。

また、内部通報制度など、 違反が発生した場合にも早期に対応できる体制 を確立します。

第2章 ログの記録と収集

122

第

3 ログの分析手法

章

第2章ではログを収集しましたが、 これをただ保管しておくだけ では意味がありません。 せっかく保存したログを活用するために は、その分析手法について知る必要があります。 そこで、 ログの 検索や分析手法、 分析に使うツール、リアルタイムに検出する方 法などについて解説します。

ログの検索

役立つのはこんなとき

ログの検索方法について知りたい

ログの検索ツールを導入したい

システムを運用する現場では、膨大なログが記録されている中から、 欲しい ログを探し出さなければなりません。 トラブルが起きたときにはその原因調査 に使える時間は少なく、 効率よく解決することが求められます。

greやテキストエディタでの検索

ログの多くはテキスト形式で保存されているため、 メモ帳などのアプリケー ションでも閲覧できます。 当然、 このようなアプリケーションが備えている検 索機能を使う方法もあります。

しかし、複雑な条件でログを検索することを考えると、少し便利な機能を備 えたコマンドを使うか、 テキストエディタなどのアプリケーションを使うと便 利です。 ここでは、 コマンドの例としてgrep を使う方法と、 テキストエディタ を使う方法を紹介します。

grep を使う

大量のデータが格納されている中から検索するとき、 基本は特定のキーワー ドを入力して、そのキーワードと一致するものを調べる方法を使います。 ログ

124

に出力されているキーワードさえ知っていれば、そのキーワードを入力するこ とで検索できます。

たとえば、エラーに関するログであれば 「Error」 というキーワードで検索す ることで抽出できるでしょうし、画面にエラーメッセージが表示されているの であれば、そのエラーメッセージの内容を入力することで検索できることもあ るでしょう。

このとき、サーバーなどでよく使われるソフトウェアとしてgrep 1があり ます。 grep は Linuxなど多くのUNIX環境に標準でインストールされており、 macOS でも使えるコマンドラインツールです。 Windows環境であれば、 WSL (Windows Subsystem for Linux) などを導入して使う方法があります。

grep を使うと次のように検索できます。

$ grep Error sample.log

このコマンドを実行すると、 「sample.log」 というファイルから 「Error」 と いうキーワードを含む行を表示できます。 もし複数のキーワードを含む行を検 索したい場合は、 次のように実行します。

$ grep Error sample.log | grep Message

このようにパイプでつなぐと、 1つ目で 「Error」 というキーワードを含む行 が抽出され、 2つ目で 「Message」 というキーワードを含む行が抽出されるた

め、結果として 「Error」 と 「Message」 の両方を含む行を抽出できます。

逆に、一致しないものを抽出したい場合は、次のように 「-v」というオプシ ョンを指定します。

$ grep -v Error sample.log

1 https://www.gnu.org/software/grep/

-1 ログの検索

3

125

これで、 「Error」 というキーワードを含まない行だけを抽出できます。 なお、大文字と小文字を区別せず検索する場合は、次のように「-i」という オプションを指定します。

$ grep -i error sample.log | grep -i message

コ VS Code のインストール

上記のようにコマンドを使う方法もありますが、 初心者にとっては難しいも のです。 そこで、 GUIで操作することを考えます。 GUIでテキストファイルを 扱うツールとしてテキストエディタがあり、 多くの会社から提供されています。 ここでは Microsoft社が無償で提供しているVisual Studio Code (VS Code) を使って解説します。

VS Codeをインストールするには、 公式サイトにアクセスしてダウンロード する方法が手軽です。 使っているパソコンのOSに合ったファイルをダウンロ ードしてインストールが完了したら、 VS Code を起動します。 初回の起動では、 図3-1 のような画面が表示されます。

図3-1 VS Code の初回起動画面

Get Started with VS Code

第3章 ログの分析手法

126

ここで画面の指示にしたがってテーマを選ぶなどの初期設定を済ませると、 図3-2 のようになります。

図3-2 VS Code の初期設定完了画面

Edit Selection View EXPLORER NO FOLDER OPENED You have not yet opened a folder Open Falder Opening a folder will close all currently open editors To keep them open add a folder instead Welcome X Visual Studio Code Editing evolved Start Open 82000 Recent You have no recend faiders op Walkthroughs Learn the Fundamentals

上記のような英語の画面でも使えますが、 便利に使うために日本語で表示す る拡張機能をインストールします (図3-3 )。 左側のアイコンの一番下にある 「拡張機能」 のアイコンを選び、 検索欄に 「japanese」 と入力してください。 そ して一番上に表示される 「Japanese Language Pack for VS Code」 を選択し、 インストールします。

図3-3 VS Code を日本語化する拡張機能

TRACE Extension Japanese Language Pack for Visual Studio Code Japanese Language Pack for Visu Microsch and ***** Language pack extern for Japanese Japanese Language Pack for VS Code Calegues Resources

1 ログの検索

3

127

これで再起動すると画面が日本語になり、 初心者でも使い方に迷うことは少 なくなります。 「ファイル」メニューの 「開く」 や 「フォルダを開く」 などを選 択して、適当なログファイルを開いてみましょう。

第3章 ログの分析手法

■ VS Code での検索

ファイルを開くと、画面右側にファイルの内容が表示されます。 このファイ ルからキーワードで検索することを考えます。

VS Code では、開いているファイル内を検索する方法と、 開いているフォル ダ全体から検索する方法があります。 まずは開いているファイル内を検索する 方法を実行してみましょう。

このためには、「編集」メニューから「検索」 を選ぶか、キーボードのショー トカットキーで 「Ctrl + F (Command +F)」 を押します。 そして、 表示された 画面内でキーワードを入力します。 そうすると、 図3-4 のように検索欄の右側 にある矢印ボタンで順に検索結果をたどれます。

図3-4 ファイルの検索

FM masuijpeo.com.access_jlog_20250601 x Users masuipe Downicads masuipeo.com access log 20250601 13 1 weather.sasuipes.com 133.32.219.52 weather.masuipeo.com 118,186.64.2 GET weather.masuipeo.com 106.73.9.192 weather. masuipeo.com 119.729.117.61 weather.masuipeo.com 14.12.138.64 weather.nasuipeo.com 61.206.247.208 weather.masuigeo.com 126.236.41.198 weather.masuipeo.com 58.191.168.145 weather.nasulpeo.com 14.10.163.101 weather.nasuipeo.com 106.179.129.228 weather.nasuipeo.com 59.190.158.216 weather.masuipeo.com 180.16.7.139- weather.masuipee.com 180.197.242.88 1/19999+1=X [01/Jun/2@25:03:17;64 +0jbd)/yokohama,ics HTT [01/Jun/2025:03:17:04 40960」 "GET /nagoya.ics HTTP/ [81/Jun/2025 03:17:04 •8920] "GET /yokohamaics HTTP [01/Jun/2825:03:17:04 + "GET /osaka.ics HTTP/ [01/Jun/2025:03:17:04+0see] "GET /kagoshima. ics HTT [01/Jun/2025:03:17:05 090] "GET /nagoya.ics HTTP -[01/Jun/2025:03:17:05 see] "GET /osaka. ics HTTP/ (01/Jun/2025:03:17:05 see] "GET /wakayama, ics HT 101/Jun/2025:03:17:05 +0000] "GET /osaka, ics HTTP/2 (01/Jun/2025:03:17:05 09e0] "GET /shizuoka, ics P (01/Jun/2025:03:17:05:esee] "GET /osaka.ics HTTP/ [01/Jun/2025:03:17:06 +0900) "GET /fukuoka.ics HTTP/ 101/Jun/2025:03:17:06 esael "GET /gifu.ics HTTP/2 weather.masuipeo.com 106.72.43.33-(01/Jun/2025:03:17:86-8900] "GET /tokyo.ics HTTP/2. weather. masuipes.com 126.36.247.50 weather.masuipee.com 123.8.88.94 weather.masuipeo.com 119.230.253.65 weather. Masuipeo.com 180.195.162.221 weather.masuipee.com 113.150.238.65 weather, masuipe.com 219.104.139.8 (01/Jun/2025:03:17:26 +0900) "GET /yokohama.ics HTT [01/Jun/2025:03:17:06 +0900] "GET /utsunomiya.ics HTT (01/Jun/2025 03:17:07 +0900) "GET /osaka.ics HTTP/ (01/Jun/2025 03:17:07 -090] "GET /nagoya.ics HTT 101/Jun/2025103:17:27 esee) "GET /hiroshima, ics H [01/Jun/2025:03:17:07 508] GET /tokyo.ics HTTP/2

フォルダを開いているとき、開いているフォルダ内のファイルから検索した

128

い場合は、画面左端にある虫眼鏡のマークから検索できます (図3-5)。

図3-5 フォルダの検索

GET 葉のファイルに20000 エディター セットにはすべての ます。 より限定 サブセットのみが含まれてい 条件を入力して、を取り込みま す。 masuipeo.com.access_log_20250601 20000 FM Jun/2025:03:17:04 +0900) "GET /yokohama.ics HTTP/2.0 200 Jun/2025:03:17:04 +0900) "GET /nagoya is HTTP/2.0 200 21 Jun/2025:03:17:04 +0900) "GET/yokohama.ics HTTP/2.0 200 2025:03:17:04 +0900) "GET /osaka.ics HTTP/2.0 200 215. Jun/2025:03:17:04 +0900) "GET /kagoshima.ics HTTP/2.020_ Jun/2025:03:17:05 +0900) "GET /nagoya.ics HTTP/2.0 200 21. 2025:03:17:05 +0900) "GET (osakics HTTP/2.0 200 215. Jun/2025:03:17:05 +0900) GET/wakayama_Ics HTTP/2.0 200

なお、 図3-5 の検索条件の入力欄の下にある「･･･」というボタンを押すこと で、 検索対象のファイルを指定することもできます。

正規表現での検索

キーワードで検索する方法は手軽ですが、 指定する値は限定されてしまいま す。 特に、 複数の値を探さなければならないときは、キーワードを変えながら 何度も検索する作業が必要です。

grep では AND条件やNOT 条件を指定する方法を紹介しましたが、もう少し 複雑な条件で検索することを考えます。 たとえば、 IPアドレスで検索すること を考えると、 特定のIPアドレスを調べるのではなく、ある範囲内にあるIPア ドレスを探したいことがあります。 また、 IPアドレスのように特定の形式にな っているデータをすべて探したいこともあります。

IPv4 のIPアドレスは 「192.168.1.2」 のようにピリオド(ドット)で区切ら れた数字の並びで構成されています。 このとき、 すべてのIPアドレスを順に入 力したり、 OR検索で並べて検索したりするのは大変です。

このようなときに便利なのが正規表現です。 次のような正規表現を利用する

129

ログの検索

1

3

ことで、 「数字 数字 数字 数字」 という形式に合致するログをすべて検索でき ます。

[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+

grep で正規表現を使うときは、キーワードの代わりに正規表現を指定し、 「-E」 というオプションを指定します。

$ grep - '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+' sample.log

VS Code では、図3-6 の検索欄にある 「*」というボタンを押すと、 正規表 現をオンにするかオフにするかを切り替えられます。

図3-6 VS Codeでの正規表現の検索

[0-9]+.[0-9]+.[0-9]+\.[0-9]+ 1個のファイルに20000 エディターで開く セットにはすべての サブセットのみが れてい ます。 より限定的な条件を入力して検結果を絞り込みま す。 masupeo.com.access_log_20250601 20000 weather.masujipeo.com 133.32219.52 (01/Jun/2025:03:17:04 FM weather.masuipeo.com 118.106.64.82 (01/Jun/2025:03:17:06. weather.masuipeo.com 106.73.9.192-(01/Jun/2025:03 17:04+ weather.masuipeo.com 119.229.117.61 [01/Jun/2025:03:17:04 weather.masuipeo.com 14.12.130.6401/Jun/2025:03:17:04 weather.masuipeo.com 61,206,247,208 [01/Jun/2025:03:17:0.. weather.masuijpeo.com 126.236.41.198 [01/Jun/2025:03:17:05 weather.masujipeo.com 58.191.160.145[01/Jun/2025 03:17:05.

同様に、 Web サーバーのアクセスログを検索するときは、 HTTPのステータ スコードで検索することがあります。 サーバーのエラーでは、ステータスコー ドが500番台であることから、これを検索キーワードにすることを考えます。 こ のとき、 「5」 のような数字で検索してしまうと膨大なログが見つかります。 そ こで、「5」で始まる3桁の数を検索するには、次のような正規表現が考えられ ます。

第3章ログの分析手法

130

\b5\d{2}\b

この「\b」は単語境界を示し、 数字の前後に他の数字や文字が続かないこと を意味します。 また、 「\d{2}」 は任意の数字が2回続くことを指定 (合計3桁) しています。

さらに、ドメイン名などで検索するときは、次のように書くこともあります。

example\.com*

COLUMN 日時での検索

ログには必ずタイムスタンプが記録されていることを第2章で解説しました。 そして、問題が起きたときに絞り込むことを考えると、日時の範囲がある程 度わかっていることが多いものです。

そこで、日時を範囲で絞り込むことで、 調べる必要があるログの件数を 減らせます。 特に、 キーワードでの検索など他の条件と組み合わせることで、 より絞り込んだ結果だけを確認できます。

このときに注意すべきこととして「タイムゾーン」 があります。 日本だ けで仕事をしていると、 時刻を無意識に日本時間で考えてしまいがちですが、 インターネットをはじめとしてネットワークの接続先は海外であることは珍 しくありません。

このため、ログに出力されている日時のタイムゾーンを意識しなければな りません。 海外に設置されているサーバーであっても、 自分が管理している ものであれば、 日本時間に設定することはできますが、 すべてのサーバーを 自分が管理しているとは限りません。 このため、 日本時間以外で稼働してい るサーバーがあることも多いでしょう。 このとき、 2通りの方法が考えられます。

1つ目はそれぞれのログが記録されているタイムスタンプに合わせて検索 する方法です。 日本時間ではありませんが、そのサーバーの設定で使われて いるタイムゾーンで検索すれば求められます。 多くのシステムにまたがるロ グでなければ大きな問題にはならないでしょう。

3-1 ログの検索

131

もう1つは、ログの検索前にタイムゾーンを変換することです。 特に複数 のシステムから出力されているログの場合は、 NTPなどで時刻を同期する だけでなく、 タイムゾーンも揃えなければ検索が難しくなります。 このため、 ログ管理サーバーなどに転送するときに、変換する方法が考えられます。

いずれにしても難しいのが 「サマータイム」の存在です。 国によっては日 中の時間が長くなる夏を中心に、 時計を通常よりも進めています。 このため、 ログを扱うときには、 把握すべきシステムがどのようなタイムゾーンに設定 されているのか、どのような方法で分析をするのかを事前に考えておく必要 があります。

その他の条件での検索

ログの種類によっては、ログの量が膨大になって検索に時間がかかりすぎる ため、データを事前に絞り込んでおく手法が使われます。 このとき、 ログに記 録されたIPアドレスやログのレベル、 タグなどを組み合わせることで、件数を 大きく減らせます。

たとえば、Web サーバーへのアクセスであれば、 社内の端末からのアクセス は除外したいことがあります。 このとき、IPアドレスで絞り込んで除外するよ うな方法があります。 また、 海外からの攻撃が多い環境であれば、 国内の端末 か海外の端末かをIPアドレスで分類しておくこともあります。

ログのレベルとしては、 「重大」 「警告」 「エラー」 「情報」 などがあることを 第1章で解説しました。 Windowsのイベントログでは、 ントレベルに応じてフィルターする機能を備えています。 図3-7 のようにイベ

また、タグでは特定の文字で検索しますが、この単純なキーワード検索とは 異なり、ログの特定のカラムだけを調べることでよければ効率よく検索できま す。 ログがどのような構成になっているかによって、 検索の効率が変わること を理解しておくとよいでしょう。

第3章ログの分析手法

132

図3-7 Windowsのイベントログでのフィルター

現在のログをフィルター フィルター XML ログの日付(G): イベントレベル: 指定なし □ 大(L) (W)(B) □エラー(R) 情報 (1) (O) イベントログ (E): Pesication 9-22(5) イベント ソース (V): イベントIDを含める除外する(N): ID番号またはID 範囲をコンマで区切って入力します。条件を 外する場合、最初に 例 1,3,5-99,-76 号を入力します。 すべてのイベントID> タスクのカテゴリ(T): 1-2-F(K): ユーザー(U): すべてのユーザー> コンピューター(P): すべてのコンピューター> 消去(A) OK キャンセル

高速に検索するための準備

ログの量が少なければ、 単にキーワードで検索しても短時間で抽出できるこ とが多いでしょう。 しかし、 多くのログが溜まると、 検索に時間がかかるよう になります。

このようにログが膨大になっても高速に検索するためには、事前にログに対 してインデックス (索引) の作成やシャーディングの設定などの準備をするこ とが大切です。

インデックスとは

インデックスはデータベースでは一般的に使われている技術で 書籍の巻末 にある索引のようなものです。 索引を用意しておくと、その索引に登録されて いるものは、その場所がわかるため高速にアクセスできます。

1

I

ログの検索

3

133

たとえば、図3-8 のような木構造で索引を用意しておき、 ある値を探すとき に分岐の値より五十音順で前なら左、 後ろなら右に移動することを繰り返しま す。 今回であれば、 3回の判定で目的の値に到達できます。

図3-8 インデックスのイメージ

ID 值 プログラミング 分岐の値より 1 アルゴリズム 五十音順で 2 クラウド ドキュメント 前なら左、 3 コンピュータ 後ろなら右に進む 4 サーバー 5 セキュリティ サーバー マーケティング 6 スマートフォン スマート バック 7 データベース クラウド/ 8 フォン アップ ライブラリ ドキュメント 9 ネットワーク 10 バックアップ 11 プログラミング アルゴリズムセキュ ○○○○ 12 マーケティング 13 メール ネットワーク メール 14 ライブラリ コンピュータ データベース プログラミング ログ 15 ログ

ただし、このようなインデックスは一般的にはデータベースのように行と列 で区切られているデータに使われることが多いです。 ログも日付やエラーの種 類であればインデックスを設定できますが、エラーメッセージの内容は文章の ような項目として格納されています。

一般的に、文章から検索するときは全文検索となり、 1文字ずつ順に比較し ていくと処理に時間がかかります。 そこで、 少しでも高速に検索できるように、 英語や日本語のような文章でもインデックスを作成することを考えます。

たとえば、 英語のように空白 (スペース) で区切られた言語であれば、 単語 単位で抽出するのは容易です。 単に空白で分割し、 それぞれの単語を索引とし して登録しておけば、その単語で検索したときには速やかに結果を表示できます。

一方、日本語のように空白で区切られていない言語では、単語を区切るのは 英語ほど簡単ではありません。 しかし、 形態素解析という手法があり、 ChaSen やMeCab、 Janomeなどのライブラリも登場しているため、 ある程度の精度で

第3章 ログの分析手法

134

単語を区切ることはできます (図3-9)。

図3-9 形態素解析の例

吾輩は猫である。 名前はまだない。 単語 品詞など 吾輩 名詞 代名詞 一般, は 助詞, 係助詞,** 猫 名詞, 一般,** で 助動詞,*** ある 助動詞,*,** 記号,句点** 名前 名詞, 一般,** は 助詞、係助詞,** まだ 副詞 助詞類接続,** ない 形容詞、自立,** 記号,句点**

さらに、 n-gramという手法が使われることもあります。 n=2であれば、2文 字ずつ区切ってインデックスを作成するものです。 たとえば、 「Windows」 と いう単語であれば 「Wi」 「in」 「nd」 「do」 「ow」 「ws」 のように分割し、それ ぞれでインデックスを作成するのです。

このように索引を作成しておくと、 それを含むログを高速に検索できます。 こ のような索引を作るのは大変だと感じるかもしれませんが、 Elasticsearch や Splunk をはじめとしたログ管理ソフトには 「全文検索エンジン」 を搭載してい るものがあり、ログファイル全体に対して高速かつ柔軟な検索ができます。

これらのログ管理ソフトの導入については後述します。

シャーディング

多くのログが記録されていると、1つのコンピュータだけですべてを管理する

135

ログの検索

のは大変です。 そこで、ログをいくつかのグループに分けて、別々のコンピュー タで管理することを考えます。 これを「シャーディング」 といいます(図3-10)。

図3-10 シャーディング グループ1 のログ グループ2 のログ 大量のログ グループ3 のログ

ログのデータを分割して複数のコンピュータで並列処理できれば、 検索や集 計が速くなります。 また、 データが増えたときには、 シャーディング先のコン ピュータ (シャード) を増やすことで、システムを拡張できることも特徴です。 ElasticsearchやSplunkなどのログ管理や検索に使われるシステムでは、シ ャーディングを基本機能として採用しています。 実際、 大規模なログ解析シス テムでは、 シャーディングなしでは性能や拡張性の面で対応が難しいことも多 現代のログ管理では必須の技術だといえます。

ログ検索ツールの具体例

ログを検索するとき、単にテキストエディタなどで検索する方法もあります が、 全文検索エンジンなどを搭載したログ検索ツールがよく使われます。 具体 的には、 Elastic StackやGrafana Splunk などが挙げられます。

第3章 ログの分析手法

136

Elastic Stack

Elastic Stack は、 Elasticsearch Logstash、 Kibana といったツールを組み 合わせたものを指すことを第2章で解説しました。 具体的には、まずLogstash によりログデータを収集・整形し、 Elasticsearchによってインデックス化し、 Kibana により可視化されます。 これにより、 複雑な検索クエリでもマウス操 作などで視覚的に設定でき、 リアルタイムで結果を確認できます。

第2章では Logstashのインストールや設定について解説しましたので、ここで は Elasticsearch と Kibana のインストールや設定について解説します。 Kibana を使って具体的にどんなことができるのかを知りたい場合は、 デモ画面にアク セスして、試しにいろいろと操作してみるとよいでしょう。

Elasticsearchや Kibana は Windows や macOS のほか、 Linux でも動作しま す。 Windows や macOS では、公式サイトからダウンロードして画面の指示に したがうだけなのでそれほど難しくありません。 ここでは、 Linux でのインス トールについて解説します。

Elasticsearchのインストールと設定

Elasticsearch の実行にはJavaの実行環境が必要なため、 まずはJavaをイン ストールします。 第2章のLogstashと同様に、次のコマンドでインストールし てください。

# dnf upgrade -y # dnf install -y java-21-openjdk

Alma Linux 9にElasticsearchをインストールするには、 まず Elasticsearch のリポジトリを追加します。 これもLogstashと同様です。

2 https://demo.elastic.co/app/kibana

1 ログの検索

1

3

137

# rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch #tee/etc/yum.repos.d/elasticsearch.repo <<EOF [elasticsearch-9.x] name=Elasticsearch repository for 9.x packages baseurl=https://artifacts.elastic.co/packages/9.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md EOF

そのうえで、次のコマンドを実行します。

# dnf install -y elasticsearch

この実行中に出力されるメッセージ内に、次のように初期パスワードが表示 されますので、メモしておきましょう。

The generated password for the elastic built-in superuser is: XXXXXXXX

なお、パスワードを変更したい場合は、 次のコマンドを実行します。

# /usr/share/elasticsearch/bin/elasticsearch-reset-password-u elastic

インストールが完了したら、 Elasticsearch を設定します。 設定ファイルは、 「/etc/elasticsearch/elasticsearch.yml」 にあるため、 このファイルを編集し ます。 初期設定の状態ではHTTPSでのアクセスを前提としているため、検証 用としてHTTPSの証明書がない場合は、次の部分で 「enabled: false 」 に設

第3章 ログの分析手法

138

定を変更しておきましょう (本番環境として使う場合は 「enabled: true」に設 定し、必ずHTTPSの証明書を使ってください)。

/etc/elasticsearch/elasticsearch.yml

xpack.security.http.ssl: enabled: false

設定が完了すると、 Elasticsearch を起動します。

# systemctl start elasticsearch # systemctl enable elasticsearch

Kibana を同じサーバーに導入する場合はこれで問題ありませんが、 異なる サーバーからアクセスするときは、 必要に応じてファイアウォールを設定しま す。 Elasticsearchのデフォルトポート番号は9200なので、 外部からアクセス する場合はこのポート番号を開放します。

$ sudo firewall-cmd--add-port=9200/tcp --permanent $ sudo firewall-cmd --reload

ここまで設定できれば、 次のURLにWeb ブラウザでアクセスし、 BASIC認 証の画面でユーザー名に 「elastic」、 パスワード欄に上記でメモしたパスワー ドを入力し、 Elasticsearch の情報が表示される画面が表示されれば成功です。

http://< サーバーのIPアドレス>:9200

Webブラウザで表示される内容の例

(

"name": "xxx.xxx.xxx.xxx",

1

1

ログの検索

3

139

} "cluster_name": "elasticsearch", "cluster_uuid": "kvM4DaseQwaH2109smoSZg", "version": { }, "number": "9.0.1", "build_flavor": "default", "build_type": "rpm", "build_hash": "73f7594ea00db50aa7e941e151a5b3985f01e364", "build_date":"2025-04-30T10:07:41.393025990Z", "build_snapshot": false, "lucene_version": "10.1.0", "minimum_wire_compatibility_version": "8.18.0", "minimum_index_compatibility_version": "8.0.0" "tagline" "You Know, for Search"

このうえで、 Kibana からアクセスできるようにサービスアカウントトーク ンと呼ばれるトークンを取得します。 次の 「xxxxxxxx」のところに、上記でメ モしたパスワードを指定して実行します。 これは、 「elastic/kibana」 というユ ーザーに対するサービスアカウントトークンを発行するコマンドです。

# curl -u elastic:xxxxxx -X POST "http://localhost:9200/_security/ service/elastic/kibana/credential/token?pretty" -k

この実行結果を見ると、 次のように表示されており、このトークンをメモし ておきます。

{ "created": true, "token": { "name": "token_xxxxxxxxxx", "value": "xxXXXXXXXX" } }

第3章ログの分析手法

140

Kibana のインストールと設定

続いて、 Kibana をインストールします。 Kibanaも上記の Elasticsearch と 同様に、 Elasticsearchのリポジトリを追加した状態で、 次のコマンドを実行し てインストールします。

# dnf install -y kibana

続いて、 Kibana を設定します。 設定ファイルは、 「/etc/kibana/kibana.yml」 にあるため、このファイルで Elasticsearch のURLを指定します。 初期設定で は、多くがコメントアウトされていますが、たとえば次のような部分の設定を 変更します。

/etc/kibana/kibana.yml

server.port: 5601 server.host: "0.0.0.0" elasticsearch. hosts: [ "http://localhost:9200"] elasticsearch. serviceAccount Token: "xxXXXXXXXXX"

この 「server.host」 はどのIPアドレスからのアクセスを許可するかを指定 するもので、 「0.0.0.0」 を指定すると、 どのIPアドレスからのアクセスでも許 可します。

また、 「elasticsearch.hosts」 では Kibanaから接続する Elasticsearchのノ ードを指定します。 ElasticsearchのIPアドレスを指定します。 ここでは、同 じ環境に導入したため、上記のように設定しています。

そして、 「elasticsearch.serviceAccountToken」 には上記で取得したサービ スアクセストークンの値を指定します。

その後、 Kibana のサービスを起動し、 有効化します。 そのうえで、起動状 態を確認します。

ログの検索

3

141

# systemctl start kibana # systemctl enable kibana

また、必要に応じてファイアウォールを設定します。 Kibanaのデフォルト ポート番号は5601なので、 外部からアクセスする場合はこのポート番号を開放 します。

$ sudo firewall-cmd--add-port=5601/tcp --permanent $ sudo firewall-cmd --reload

ここまで設定が完了したら、 次のURLにWeb ブラウザでアクセスします。 Kibana のログイン画面が表示されれば成功です (図3-11)。

http://<サーバーのIPアドレス>:5601

図3-11 Kibana のログイン画面

1 Welcome to Elastic Log in

ここで、ログインするユーザーは Elasticsearchのスーパーユーザーです。 Usernameの欄に 「elastic」、Passwordの欄に上記でメモしたパスワードを入 力します。

第3章 ログの分析手法

142

そうすると、 図3-12 のようなダッシュボードが表示されます。

図3-12 Kibanaのダッシュボード

elastic Welcome home Elasticsearch Create search experiences Observability Carsandare your o Security Analytics Expert, and Get started by adding Integrations To start working with your data, use one of our many ingest options. Collect data from an app or service, or uploade you're not ready to use your own data, play with angle data set. Try managed t

このダッシュボードから、 さまざまな機能を設定できます (表3-1)。

表3-1 Kibana の設定項目の例

項目 内容 インデックスの作成 データ検索のために Elasticsearchのインデックスを指定できる ログやデータの検索、 フィルタリングができる Discover 機能 (リアルタイムにデータを確認できる) Visualize 機能 棒グラフ、 円グラフ タイムラインなどグラフを作成できる Dashboard 機能 複数のビジュアライゼーションをまとめて表示し、 カスタムダッ シュボードを作成できる Alerting 条件に基づいて通知を送るアラート設定できる (Slackやメールと連携できる) 視覚的に魅力的なレポートやプレゼン資料を作成できる Canvas 機能 Machine Learning 異常検知や予測分析ができる (有料プラグイン)

ログの検索

3

143

たとえば、インデックスパターンを作成するには、 左上の3本線のメニュー から 「Management Stack Management→ Index Management」 とたど ると、図3-13 のようにインデックスを作成できます。

図3-13 インデックスの作成

elastic QFind apps, content, and more Management Index Management Index Management docs Ingest Indices Ingest Pipelines Data Streams Index Templates Component Templates Enrich Policies Data Update your Elasticsearch indices individually or in bulk Learn more. Include hidden indices Include rollup indices Index Management Q Search Create index Index Lifecycle Policies Data Set Quality Snapshot and Restore Rollup Jobs Transforms Remote Clusters Migrate Alerts and Insights © Alerts Rules Lifecycle status Lifecycle phase ✓ Reload indices Premart Replicas Decu. Storag Datas No indices You don't have any indices yet. Create index

その他、図3-13 の左側のメニューから選択することで、さまざまな機能につ いて設定できます。

Kibana は Elasticsearch と密接に連携しているため、バージョンを合わせる ことが重要です。 また、本番環境で使う場合には、セキュリティ設定 (通信の 暗号化や認証の設定) も必須です。

Grafana

Grafana 3はオープンソースのデータ可視化及び監視プラットフォームです。 さまざまな環境から収集したデータをダッシュボード形式で分析できるツール で、幅広く使われています。

3 https://grafana.com/ja/

第3章 ログの分析手法

144

特に、ドラッグ&ドロップでウィジェットを配置する形でダッシュボードを 構成できるため、 グラフや表、 ヒートマップなど視覚的に把握しやすい画面を 手軽に用意できることが特徴です。

また、異常を検知したときにメールやSlackなどの通知を送るアラート機能 を備えているため、 問題の早期発見にも向いています。 さらに、 コミュニティ によるプラグインが活発に開発されており、 独自のプラグインを開発すること もできます。 どんなことができるのかを確認したい場合は、 Grafana について もデモ画面が用意されていますので、これを試してみてください。

上記と同様にAlma Linux に Grafana をインストールするには、 公式のリポ ジトリを追加します。

#tee /etc/yum.repos.d/grafana.repo <<EOF [grafana] name Grafana Repository baseurl=https://packages.grafana.com/oss/rpm repo_gpgcheck=1 enabled=1 gpgcheck=1 gpgkey=https://packages.grafana.com/gpg.key EOF

そのうえで、 次のコマンドでGrafana をインストール

します。

# dnf install -y grafana

インストールが完了すれば、 Grafana を起動します。

# systemctl start grafana-server #systemctl enable grafana-server

4 https://play.grafana.org

ログの検索

1

3

145

外部からアクセスするような場合は、 必要に応じてファイアウォールを設定 します。 Grafana のデフォルトポート番号は3000番のため、 次のコマンドで解 放します。

# firewall-cmd --add-port=3000/tcp --permanent # firewall-cmd --reload

設定が完了すれば、 Web ブラウザから次のURLにアクセスします。

http://<サーバーのIPアドレス>:3000/

初回のログインでは、ユーザー名とパスワードの両方に 「admin」 を指定し、 ログイン後にパスワードの変更が求められます。 そして、 ダッシュボードが表 示されます (図 3-14)。

図3-14 Grafana の画面

Grafana b Down New Aaring Convections Administration Home Dashboards Welcome to Grafana Need help? Documentation Tutorials Community Public Slack Basic The steps below Quickly finish seting you Grafana instalation TUTORIAL DATA SOURCE AND DASHBOA Grafana fundamentals Set up and understand Grafana if you have no This tutorial guides you rough the ene p and covers the "Data source and Dat right Dashboards Starred dashboards Recently viewed dashboards Remove DATA SOURCES Add your first data source Learn how in the docs Simple cloud billing with FOCUS

第3章ログの分析手法

146

まずはデータソースを追加します。 左のメニューにある 「Connections」 -> 「Data Sources」 を選択し、 「Add data source」 をクリックします。 そして、 利用したいデータソース (例:Elasticsearch、 MySQLなど)を選択します (図3-15)。

図3-15 データソースの追加

Grafana Hom Maned Explore Brdown ie n Home Connections Cats sources Add data source Chouse a data ce type Elasticsearch Aering Connections Add new connection sources Grafana Pyroscope Amministration Com Microsoft SQL Server Com MySQL

接続情報 (URLや認証情報など) を入力し、「Save & Test」 で接続を確 認します。 次に、 ダッシュボードを作成します。 左のメニューから 「+」 → 「Dashboard」 を選択し、 「Add new panel」 でグラフや表などのパネルを作 成できます。 ここでクエリや表示を設定し、ダッシュボードを保存します。

このように、さまざまなデータソースに対応できることがGrafana の特徴で す。 Kibana は Elasticsearch など Elastic Stackに特化していますが、 複数のデ ータソースを統合して可視化したい場合に向いていると言えるでしょう。

Splunk

Splunk は、 商用のログ管理ツールとして、 多くの企業で導入されています。 ログデータの自動解析や異常検知、 複雑な検索クエリの作成をGUIで操作でき

147

3-1 ログの検索

るため、経験がない担当者でも扱いやすいことが特徴です。

また、検索条件に対してアラートを設定することで、 特定の利用者による不 審なアクセスパターンを検出するために、 「search」 コマンドで条件付きの検索 を実施し、その結果に対してアラートとして設定するなど、 多彩な運用が可能 です。

Splunk は Windowsや macOSのほか、 Linux など幅広いOSに対応してい ます。 ここでは、 Linux でのインストールについて解説します。

まずはSplunkの公式サイト 5からインストーラーをダウンロードします。無 料で試すには 「無料トライアル」 の会員登録が必要ですが、 OS に合わせたフ アイルが 「splunk-<version> <build> -Linux-amd64.tgz」のようなファイル名 で提供されています。

会員登録後、 「Splunk Enterprise」 から環境にあったものをダウンロードし ます。 ここでは、Linux版の 「.tgz」 形式をダウンロードすることにします。そ して、次のようなコマンドでインストールします。

#tar -xvzf splunk-<version>-<build>-Linux-amd64.tgz -C /opt

これで、 「/opt/splunk」 というディレクトリにインストールされます。 Splunk のWeb管理画面に外部からアクセスするような場合は、 必要に応じてファイア ウォールを設定します。 Splunkのデフォルトポート番号は8000番のため、 次 のコマンドで解放します。

# firewall-cmd--add-port=8000/tcp --permanent #firewall-cmd --reload

そして、次のコマンドで初期設定を開始します。

5 https://www.splunk.com/ja_jp

第3章 ログの分析手法

148

# cd /opt/splunk/bin # ./splunk start

初回のアクセス時に、管理者のユーザー名とパスワードを設定します。 次回 以降に自動的に起動するように設定するには、次のコマンドを実行します。

# ./splunk enable boot-start

上記のコマンドでWeb サーバーが起動しているため、 Web ブラウザから管 理画面にアクセスします。 ポート番号として「8000」 が指定されているため、 次のURLにアクセスします。

http://<サーバーのIPアドレス>:8000

そして、上記で設定した管理者アカウントでログインすると、 図3-16 のよう に表示されます。

図3-16 Splunkの画面

splunk enterprise App こんにちは, Administrator サーチ от ブックマーク ダッシュボード サーナ 表示したデータ メッセージ アンティビティ ホームページの あなたと共 4 マイブックマーク(0) ブックマークの 自分と共有 ブックマークの 自分が共有 他の管理者が共有 Splunk (13)

まずは「設定」から 「データの追加」 を開き、 ログやファイル、 ネットワー クなどのデータソースを追加します。 そして、 ログを収集するとともに、 イン デックスを作成します。 インデックスを作成するには、 「設定」から「インデッ

1 ログの検索

3

149

クス」を開くと、 新しくインデックスを作成できます。

ユーザーやロールを管理するには、 「設定」 から 「ユーザー」 や 「ロール」を 開きます。 この画面では、 権限ごとにアクセスできるデータや機能を制御でき ます。

このSplunkはライセンス容量 (データ取り込み量) に基づいて課金される ため、「設定」から 「ライセンス」 でライセンスの状況を確認できます。 なお、 停止したい場合は次のコマンドを実行します。

# ./splunk stop

使い方がわからない場合は、次のコマンドでヘルプを表示できます。

# ./splunk help

COLUMN オープンソースか商用ツールか

Elastic StackやGrafana はオープンソースで、 ライセンス費用はかかりま せん。 また、自由にカスタマイズできることも特徴です。 一方で、 自社で運 用や保守をしなければならず、 専門的な知識を持たないとカスタマイズも難 しいものです。 独自の分析ルールを実装したい、システム全体のコストを抑 えつつ柔軟に運用したいときは、 Elastic Stackを選ぶことがあります。

Splunk は商用製品なので、 ライセンス料を支払うことで、導入後にサポ ートを受けられます。 また、 使いやすい画面でアップデートも提供され、 専 門的な知識がなくても安定して運用できる一方で、 大規模な環境や複雑な 要件に対応するには、 それだけコストが高くなる傾向があります。 それでも、 短期間で運用を開始し、専用のサポート体制を求めるのであれば、 Splunk を選ぶとよいでしょう。

第3章 ログの分析手法

150

3

主要な分析手法

役立つのはこんなとき

集計やフィルタリングなどの抽出方法について知りたい 可視化や高度な統計処理について知りたい

ログの検索に使われる技術がわかったところで、 もう一段進んで分析するこ とを考えます。 一般的には 「集計」、「フィルタリング」、 「可視化」 という3つ の手法がよく使われます。 それぞれについて解説したあと、 これらを複数組み 合わせて分析する方法について解説します。

集計による概要の把握

まずはログの件数やエラー内容、 その他の項目などによって集計することを 考えます。 どのくらいの量のログが出力されているのかを知らないと、 最適な 分析手法を選ぶこともできないためです。

集計の必要性

ログには日時やイベントの種類などさまざまな情報が含まれているため、これ をいくつかの軸で集計することで、 全体の傾向やパターンを把握できます。 た とえば、次のような軸での集計が考えられます。

3-2 主要な分析手法

151 -

時間軸の集計

1時間ごと、 1日ごと、 または1週間ごと、 曜日ごとなどでログの発生件数 を集計し、時間的な傾向を把握する。

グループでの集計

特定のエラーコードやIPアドレス、 タグなどでグループに分けてログの発 生回数を集計し、グループごとの差を把握する。

・環境別の集計

サーバーごと、 アプリケーションごとなどでログの発生回数を集計し、環 境ごとの差を把握する。

いずれにしても、単に発生件数を集計して合計を求めるだけでなく、 平均や 中央値、 最大値、最小値などの基本統計量を求めることもあります。 このよう な統計量を継続的に把握することで、 システムに異常が発生する兆候を捉えた り、システムが使われるピークタイムを把握したり、 特定のリソースへの負荷 を計測したりできます。

重要なのは、 感覚として捉えるのではなく、 数値として定量的な指標を得る ことです。 また、 一時的な値ではなく、継続的に監視を続けることで、 運用担 当者はシステムの状態を俯瞰的に確認できるようになります。

表計算ソフトでの集計

ログを集計することを考えると、もっともシンプルなのは Excel などの表計 算ソフトを使うことです。そのためには、ログを表計算ソフトで処理できる形 に整形する必要があります。

表計算ソフトに取り込むとき、 よく使われるのはCSV形式のファイルです。 コンマで区切ったテキストファイルで、一般的な表計算ソフトでは読み書きに 対応しています。

第3章ログの分析手法

152

そこで、まずはログをCSV形式に変換することを考えます。 ログをCSV形式 に変換するとき、 Windows であれば標準で搭載されている PowerShell を使う 方法があります。 PowerShell はコマンドプロンプトよりも多機能なターミナ ル環境で、 ログを分析するために便利なコマンドがいくつも用意されています。

まずは Windowsのイベントログを取得することを考えます。 このとき、 PowerShell では 「Get-EventLog」 や 「Get-WinEvent」というコマンドが用意 されています。 これらのコマンドを使うと、 次のようにシステムログからエラ レベルのイベントについて最新の100件を取得できます。

C:¥> Get-EventLog -LogName System -EntryType Error -Newest 100

また、 「where-Object」 というコマンドを使うと特定のイベントIDのログを 取得するようなこともできます。 次のコマンドでは、アプリケーションログか ら、IDが1000のログを抽出しています。

C:\> Get-WinEvent -LogName Application | Where-Object {$_.Id -eq 1000 }

また、抽出結果をCSVファイルに保存するには、 「Export-Csv」というコマ ンドを使います。 たとえば、 「Get-EventLog」 で抽出したログをCSVに変換す るには、次のようにパイプを使って結果を渡す方法が考えられます。

C:\> Get-EventLog -LogName System -EntryType Error -Newest 100 | Export-Csv -Path C:\Logs\SystemErrors.csv -NoTypeInformation

このような PowerShellを使うメリットとして、 コマンドとして実行できる

6 Get-EventLog と Get-WinEventは扱えるログの種類などに違いがあります。 7 前のコマンドから出力された内容を次のコマンドへ渡す手法。

3-2 主要な分析手法

153

ことが挙げられます。 マウスで操作して検索すると、それを何度も繰り返すの は面倒ですが、 コマンドとして作成すれば定期的に実行できます。 これにより、 ログの解析を自動化できます。

専用のツールでの集計

もっと高度な集計をしたい場合は、専用のツールを使う方法が一般的です。 前述の Elasticsearch や Splunk などのログ解析ツールは、SQLのようなクエ リ言語や専用の集計機能を提供しているため、容易に集計できます。 たとえば、 Elasticsearch であれば、 「Aggregation」という機能が用意されています。こ れは、特定のフィールドの値ごとにグループ分けし、 件数や平均値を求めるク エリを実行できるものです。

こういったツールを使うことで、システム全体のログのように膨大な件数の データが与えられたとしても、 異常値の発生頻度などを一元的に把握し、対応 策を考える判断材料とできる可能性があります。

フィルタリングによるログの抽出

集計するだけであれば、 膨大な量のログがあってもそれほど問題にはなりま せんが、必要なデータだけを眺めてトラブルの原因を調査することを考えると、 現実的な時間で処理することが難しくなります。

このため、不要なデータを除外し、 条件を満たしたログだけを抽出するフィ ルタリングが求められます。 つまり、 条件を指定して抽出しておくことで調べ るべきログの数を減らすことができれば、あとは人間の目でひたすらチェック する、 生成AIなどで分析するなどの対応が考えられます。

このときのフィルタリングに使う条件は、前節の「ログの検索」で解説した ものと基本的に同じです。 ここでは、具体的なフィルタリングの方法について 解説します。

ログの分析手法

154 -

コマンドでのフィルタリング

テキスト形式のログが保存されていれば、そのファイルから特定の文字を含 む行を抽出する方法が考えられます。 この章の冒頭では、サーバーなどで使わ れる Linux などの UNIX系のOSやmacOSではgrep というコマンドがよく使 われていることを解説しました。

grep を使うと、 ログが記録されているファイルから検索条件に一致する行だ けを抽出できるため、 この出力をファイルに保存すれば、 フィルタリングした 結果を保存できます。

また、 Windows環境であれば、 WSLを導入してgrep を実行する方法のほか、 PowerShell を使う方法があります。 次の例では、 PowerShell を使って 「GetContent」 というコマンドでファイルを読み込み、 その内容をパイプで 「SelectString」というコマンドに渡して検索しています。

C:¥> Get-Content C: Logs app.log | Select-String "Error"

そのほかにも、 複雑な解析や集計をしたい場合は 「Where-Object」 での抽出 や 「Group-Object」 でのグルーピング、 「Measure-Object」 での統計処理などの コマンドを使えます。 そして、この出力をファイルに保存しておけばフィルタ リングができます。

専用のツールでのフィルタリング

コマンドで操作するのではなく、 ログ分析に特化したツールを使うと、フィ ルタリング機能が用意されていることが多く、 マウスでの操作などで簡単に条 件を設定できます。

たとえば、 Kibana を使っていると、 画面上でフィルターを設定することで、 特定のIPアドレスからのアクセスや、特定のキーワードを含むログをリアルタ イムに監視できます (図3-17)。

3-2 主要な分析手法

155

図3-17 ツールでのフィルタリング

Deta view Logs Fier your data using Search fee Auto interval No breakdown Check out context- Try ESIOL Inspect Alerts +08 Last 15 minutes 2025447703 2025120347703-30 seconds) Documents (44,701) Patterns Field statistics

直感的な把握につながる可視化

合計や平均、 最大値、最小値など集計したデータはただの数字です。 その瞬間 の値としては明確で正確に伝わる一方で、その推移を把握しようと思うと、数 値の羅列になってしまいます。

数値は値の大小を正確に把握できる一方で、 数値の個数が増えると人間が直 感的に把握するのは難しくなります。 そこで、 グラフや図解などによって可視 化することで、 専門的なスキルや知識を持たなくても直感的に把握できるよう になります。 特に経営者など、 短時間で多くの情報を総合的に判断しなければ ならないとき、 ダッシュボードなどグラフが一覧となっている画面を用意する ことは有効です。

よく使われる可視化手法

ログを可視化するときに使われる可視化手法として、 次のようなものが挙げ られます。

第3章ログの分析手法

156

●折れ線グラフ: 時系列の把握

さまざまなイベントが発生した件数を時間軸に沿って折れ線グラフで表現 することで、トレンドやピーク、 異常値を把握できる (図3-18)。

図3-18 折れ線グラフの例

0.16 コア 0.14 コア 0.12 コア 0.10 コア 0.08 コア 0.06 コア 0.04 コア 0.02 コア m 0 コア 515. 517

棒グラフ 量の把握

ログを種類などで分類し、それぞれの件数を棒グラフとして表現すること で、分布を把握できる (図3-19)。

図3-19 棒グラフの例

ページビュー 3000000 2500000 最大值 2801420 ·平均值 2000000 2463842.7 1500000 1000000 500000 0 日 月火水木金 土曜日

●円グラフ 割合の把握

エラーコード別の割合などを円グラフで表現することで、 全体に占める割 合を一目で把握できる (図3-20)。

主要な分析手法

3

157 -

図3-20 円グラフ

linux androidpie ios_ipadwinxp win7 android10 その他のOS macosx15 win10 android ios_iphone

●ヒートマップ 異常の把握

複数の要素 (日時とサーバー別など) を組み合わせて平面に色の濃淡など で視覚化することで、局所的な異常やパフォーマンスの低下を把握できる (図3-21)。

図3-21 ヒートマップ

可視化ツールの利用

上記で紹介したような可視化をするときに、もっとも手軽なのは表計算ソフ トを使う方法です。 一般的な表計算ソフトでは、上記のようなさまざまなグラ フを手軽に作成できます。

しかし、表計算ソフトではデータを手作業で更新しなければならず、 リアル タイム性はありません。 ログは常に更新され続けているため、 手作業での更新 が必要になると最新の内容を把握するのが遅れてしまいます。

第3章ログの分析手法

158

そこで、 専用のツールでグラフを作成することが考えられます。 一般的には、 Kibana や Grafana Splunk などのツールを使うことが多いでしょう。 これら のツールは、ドラッグ&ドロップなどの直感的な操作でレイアウトを作成でき、 細かいクエリを設定できます。

たとえばGrafana では、 図3-22 のようにグラフを並べて表示することで、 ダ ッシュボードとしてよく使われています。

図3-22 Grafana のデモ画面の例

wwwwwwww wwwwwww

出典: https://play.grafana.org

複数の分析手法の組み合わせ

ここまで解説した集計やフィルタリング、 可視化はいずれも基本的な分析手 法です。 それぞれを単体で使ってもログの分析はできますが、一般的にはこれ らを複数組み合わせて分析します。

具体的に、どのような使い方がされるのか、 その事例をもとに紹介します。

障害が発生したときの対応

システム障害が発生すると、 できるだけ早く復旧しなければなりません。 そ

主要な分析手法

2

3

159

のためには、 原因の特定が重要です。

多くの利用者が使うシステムでは、システム障害が発生するとシステム管理 者に問い合わせが入って気づくことがあります。 しかし、このような問い合わ せが入ってから気づくのでは遅いため、 問い合わせが入る前には事案の発生を 把握し、調査に入っていなければなりません。

システム障害が発生する前に気づくことが理想です。 これは障害の内容にも よりますが、 たとえばアクセス集中などによる障害であれば、 事前にアクセス 数の推移を把握しておき、 ある閾値を超えれば運用担当者宛に通知するような 仕組みを作る方法もあります (図3-23)。このためには、 折れ線グラフなどで 時系列の推移を把握しておき、 閾値をうまく設定する必要があります。

図3-23 閾値の設定

使用率 閾值 超えたときにアラート M ° 時間

実際には、システム障害が発生して、エラーメッセージが表示されてから気 づくことが多いでしょう。 このときも、 普段は発生することのないエラーであ ればその発生時のメッセージで原因をすぐに特定できるかもしれません。

しかし、普段からときどき発生しているエラーであれば、 特定の時間帯にな んらかの警告が急増していることで早めに気づくこともあります。 このような 場合は、そのエラーメッセージなどでフィルタリングし、 可視化することでエ ラーの発生を防ぐこともできるかもしれません。

可能であればそのようなことが起きないような根本的な対応 (ハードウェア

第3章 ログの分析手法

160

の増強など) が求められますが、 現実的には費用面や人材不足、ベンダーによ るサポート終了などの背景があって恒久対応は難しいものです。 そのため、エ ラーが起きたときにシステムが使えなくなる時間を最小限に抑えることが現実 的でしょう。

高度な統計処理との組み合わせ

複数の分析手法を組み合わせるとき、 統計処理も考えられます。 たとえば、 表3-2のような手法が考えられます。

3-2 統計処理の例 手法 概要 主な用途 時間的な変動を捉えて、 トレンドや季節性な どを抽出する 時系列解析 異常検知 予測 例) ARIMA トラフィック分析 例)指数平滑法 似たログをグループに分ける クラスタリング 例) k-means 法 パターン抽出 異常の分類 例) 階層型クラスタリング 主成分分析 (PCA) 多くの項目があるログを少ない項目で表現し 特徴量の削減 て重要な要因を抽出する 異常検知の前処理 ベイズ推定 事前確率をもとに確率的に状態を推定する 原因分析 マルコフモデル 確率モデルを使って状態遷移をモデル化する 異常検知 ログの発生件数などの数値データから傾向を 捉える 回帰分析 予測分析 例)線形回 例) ロジスティック回帰

さらに、機械学習などのアルゴリズムと組み合わせることで、 従来の分析手 法だけでは見落としがちなパターンを自動で検出できる可能性があります。

161

主要な分析手法

2

3

3 M

-

3 M

異常検知とフォレンジック

役立つのはこんなとき

異常検知の考え方を知りたい

フォレンジックの考え方を知りたい

ログを分析する理由として異常検知があります。 ここで 「異常」とは、通常 の運用状況から逸脱した動作を指します。

異常検知の考え方

システムが問題なく動作しているときは、ログを見る必要がないことが多い です。 しかし、 システム障害が発生した、 外部から攻撃を受けた、 パフォーマ ンスが低下した、 といった事案が発生したときにはログを確認しなければなり ません。

このとき、問題が発生してから対応するのではなく、 普段とは異なる状況に 早期に気づくことを考えます。 そして、 障害が発生する前の段階で異常に気づ けば、障害の発生を防げる可能性があります。

異常検知の難しさ

通常と違う状況に気づけば速やかに対処できる可能性がありますが、このと きに難しいのは、 異常な状態が発生したとしても対応が必要かどうかわからな いことです。

-

第3章 ログの分析手法

162

たとえば、 異常の検知について考えたとき、 「偽陰性」 と 「偽陽性」という 言葉がよく使われます。 陽性とは異常であること、 陰性とは異常でないこと を指すもので、 「真陽性」 「真陰性」 「偽陽性」 「偽陰性」 の4つに分けられます (3-3)。

表3-3 判定結果と実際の結果での分類

実際 陽性 陰性 陽性 真陽性 偽陽性 判定 陰性 偽陰性 真陰性

つまり、 陽性だと判定し、 実際に陽性であれば真陽性で対応が必要です。 ま また、陰性だと判定し、 実際に陰性であれば真陰性で対応は不要です。 問題なの は、陽性だと判定されたのに実際には陰性であった偽陽性と、 陰性だと判定さ れたのに実際には陽性であった偽陰性です。

つまり偽陽性の状態は、 本来は異常ではない (開発側が対応することがない) にもかかわらず、 出力されているログのせいで異常と検知された状態です。 こ の偽陽性が増えると、「オオカミ少年」 のような状態になります。 つまり、 同じ 嘘を繰り返されると誰も信用しなくなり、 実際に事案が発生したときの対処が 遅れることになります。

一方の偽陰性の状態は、 本来は対応が必要なのに、 異常が隠されていて誰も 気づいていない状態です。 ログが出力されないなど、 異常を伝える手段が用意 されていないと、 問題が発生していることに気づきません。

異常検知のアルゴリズム

異常を検知するには、ルールベースや統計的手法、 機械学習などの技術が使 われます。

3 異常検知とフォレンジック

163

● ルールベース

事前に定義された条件 (例: 一定回数以上の連続エラー、 特定のIPアドレ スからの短時間でのアクセス) に基づいて異常を検知する方法。 シンプル で即時性があるが、 未知のパターンには対応しにくいという欠点がある。

•統計的手法

正常な動作の分布を数値化し、そこからの逸脱を測定する方法。 移動平均 や標準偏差、 ヒストグラムなどを利用して、 統計値との比較により異常を 判定できる。

●機械学習

クラスタリングによるパターン識別や、 深層学習モデルによる時系列異常 検知などの方法。 膨大なログデータから未知の異常パターンを抽出できる。

フォレンジックの考え方

外部からの侵入やウイルスへの感染などの攻撃を受けたとき、 もしくは内部 からの情報漏えいがあったとき、 その手法や侵入経路、 情報漏えいの経路など を明らかにするための調査活動をフォレンジック (コンピュータフォレンジッ ク)といいます。

ここでは、フォレンジックにおけるログの調査について解説します。

コフォレンジックの目的

一般的には法的な証拠としても利用できるレベルでの詳細な解析が求められ ます。 このため、攻撃や不正行為が発生したときには、 改ざんの防止やタイム スタンプの付与など、 ログの真正性を保証します。

また、フォレンジック調査の結果に基づき、 脆弱性や運用上の問題点を洗い 出し、システム全体のセキュリティや運用体制を見直します。

第3章 ログの分析手法

164

フォレンジックの手順

フォレンジックを実施するとき、 手当たり次第に調査をしていると、 必要な データが失われたり、 正確でない調査結果が出てしまったりします。 有効な証 拠として使うためには、調査手順を明確に整備し、その手順に沿って実施する ことが重要です。

一般的には、図3-24 のような手順で進められます。

図3-24 フォレンジックの手順

証拠の 解析 → 報告 収集 保全

まずは証拠の収集と保全です。 調査対象となるパソコンやサーバーなどに保 存されているログやデータを収集します。 このとき、 改ざんされることがない ように、収集したログはコピーを作って元のファイルは触らない、 ハッシュ値 を計算するなど、 データの完全性を保証し、 法的な証拠として使えるようにし ます。

次に、収集したログをもとに異常な動きや不正アクセスの痕跡などが残って いないかを解析します。 ログの時間順に並べて異常なIPアドレスからアクセス が増えていないか、大量のログイン失敗がないかなど、 時系列解析や異常検知 のアルゴリズムが使われます。

ここで原因が特定されれば、 いつ、 どのような異常があったのか、そして影 響範囲や問題の根本的な原因を特定し、報告します。 関係者に説明できるよう に詳細を整理するとともに、客観的な視点から報告書などを作成します。

そして再発を防止するために、不具合や脆弱性の修正、 パスワードの変更や ファイアウォールなどの設定の見直し、 システムの性能不足であればハードウ ェアの増強などのシステム改善を実施します。 さらに、 運用ルールの見直しな どを実施することもあります。

3-3 異常検知とフォレンジック

165

この作業手順の中で特に重要なのは、いつ誰が何をしたのか、 詳細な記録を 残すことです。

COLUMN 生成AIを用いたログの分析

最近ではChatGPT をはじめとした生成AIが話題になっています。 これら のツールにログを読み込ませると、 その内容を手軽に要約できます。 たとえば、 あるWebサーバーへのアクセスログを読み込ませると、次のような応答が 得られました。

> 添付のファイルの内容を要約して

このアクセスログは2025年7月10日 のものです。

記録されている主な内容は以下の通りです。

* アクセスの傾向:

*ログには合計で約200件のアクセスが記録されています。

* 2つのIPアドレス(xx.xxx.xx.xx と yy.yy.yyy.yy) から、 WordPress の脆 弱性を探すような自動スキャンと思われるアクセスが多数見られます。 *これらは存在しないPHPファイル (wp_filemanager.php, moon.php, admin.php など)へのリクエストを大量に送っています。

*これらのスキャンアクセスに対しては、 主に301リダイレクト)、 404 (見つかりません) 403 (アクセス拒否) のステータスコードが返 されています。

* ルート (/)へのアクセスでは500 (内部サーバーエラー) が複数回発生 しており、サイトのトップページでなんらかの問題が起きている可能 性を示唆しています。

*通常のクローラー (OpenAl, Facebook) や一般ユーザーと思われるア クセスも見られ、 200 (成功) のステータスコードが返されています。 (以下略)

第3章ログの分析手法

166

第

4 セキュリティ面での活用方法

章

ITエンジニアであればセキュリティに関係する問題が発生したと きにログを使う人が多いでしょう。 情報セキュリティの3要素とし てよく使われる 「CIA」 という言葉は、 機密性 (Confidentiality)、 完全性 (Integrity)、 可用性 (Availability) の頭文字を取ったも のです。 この章では、これらを意識して業務を進めるうえでログ をどのように使えるのか、 ログの活用方法について解説します。

モニタリング、アラートの発出

役立つのはこんなとき

ログをモニタリングする考え方を知りたい 複数のシステムで分析して連鎖障害を防ぎたい 原因を分析するための手法を知りたい

安定稼働が求められるシステムでは、サービスが正常に動いているかどうか を常に監視し、即時対応できる体制を整えなければなりません。 特に、 なんら かの異常や普段と違う事象が発生したときには、 すぐに担当者に連絡しなけれ ばなりません。

この監視から連絡までの仕組みの構築、 障害が発生した場合の分析方法、 再 発防止策の策定について考えます。

緊急性の高い事案のリアルタイム検知

システムで問題が起きたとき、 放置すると被害が拡大したり、 サービス停止 時間が長引いたりするため、 できる限り早い段階で問題に気づかなければなり ません。

そこで、「リアルタイムに検知する」、 「即座に通知する」 という2つに分けて 考えます。

リアルタイムに検知する

ログには正常なものも含まれるため、 その中から異常な状態をリアルタイム

第4章 セキュリティ面での活用

168

に検知することが大前提です。 このとき、 どうすれば異常な状態だと判定でき るでしょうか。

異常が発生していることを判定するときにわかりやすいのは、エラーメッセ ージの出力です。 明らかになんらかの問題が発生した場合には、エラーとして ログが出力されることが多いため、これを監視することで異常を検知できます。 たとえば、 「Web サーバーでステータスコード500が発生した」 「メールサーバ ーに接続できなかった」 などの事態が発生すると、画面にエラーメッセージが 表示されます。

このようなエラーが発生したときは、画面にエラーメッセージを表示するだ けでなく、 ログにエラーの詳細を出力しておくことが重要です。 Webサーバー やデータベースサーバー、 メールサーバーなど一般的なサーバー向けのソフト ウェアであれば、 このようなエラーを出力する機能は標準で備えています。 独 自のソフトウェアを開発する場合は、監視を意識してどのようなログを出力す るのかを意識して開発しなければなりません。

自分が管理しているサーバーなどでエラーが発生したとき、 出力されている ログの中から対応が必要かを判断します。 エラーであればすべて対応する方法 もありますし、 特定のエラー番号が出力されたときにだけアラートを発生させ る方法もあります。

難しいのは、エラーが発生していなくても問題が発生する前に検知したいと きです。 たとえば、 「CPU 使用率が90%を超えた」 「ストレージの空き容量が 10MBを下回った」 といった状態を検知することを考えます。

このようなエラーが発生する前に検知するために、 使用率や空き容量を常に 目視するのは面倒なので、 警告メッセージとしてログに出力するように設定し ておく方法があります。 たとえば、 CPUの使用率であれば、 現在の使用率を定 期的に出力する方法のほか、一定の値を超えたときに出力する方法もあります。 このような閾値については第3章でも解説しましたが、 この閾値はログとして 出力する基準とすることもありますし、出力されているログから警告として扱

1 インターネット上を 「Logging Best Practice」 などのキーワードで検索すると、ログの出力についての ガイドラインが多く見つかります。

モニタリング、アラートの発出

1

4

169

う基準とすることもあります。

つまり、 ログには70%を超えたら5% ごとに出力する、 そのログの中から90% を超えたときに担当者に通知する、といった設定が考えられます (図1)。

図4-1 設定

閾値の使用率4 超えたときに担当者に連絡 閾値2 閾値1 超えたときにログ出力 時間

一般的なシステムでは、このような閾値を超えても、 すぐに何か問題が起き るわけではありません。 しかし、そのまま放置すると大きな問題になることが 多いでしょう。

そこで、収集したログの中から重要なイベントや異常を検出するルールを設 定し、必要に応じて通知することを考えます。

即座に通知する

異常を検知しても、 そのまま放置するのでは意味がありません。 早い段階で 担当者に通知することで、被害を最小限に抑えられます。 ただし、 すべてを即 座に通知する必要があるかは検討が必要で、 緊急性が求められるものから、 翌 日の対応で問題ないものもあります。

緊急性が求められる場合、 運用担当者がいる監視ルームに警告音を発出した り、警告ランプを点灯させたりします (図4-2)。 これにより、問題が発生し たことを速やかに把握できます。 そして、 運用担当者から実務担当者に電話連 絡する方法などが考えられます。

-

第4章 セキュリティ面での活用方法

170

図4-2 緊急性が求められる場合の対応

監視ルーム ログ監視 緊急性が 高いもの 運用担当者

実務担当者

もう少し緊急性が低い場合は、チャットツールやSMSなどで通知する方法が あります。スマートフォンなどにチャットツールを導入しておき、 新着のメッ セージがあるときに通知する仕組みを用意しておけば、担当者がそれに気づき ます。 電話ほど即時性はありませんが、 1人だけでなく、 その業務に関係して いる部署のメンバー全員に通知し、 部署内でタスクとして管理すれば速やかに 対応できるでしょう。

ストレージの空き容量不足のようなログであれば、問題が起きるまで少し余 裕があることが多いものです。 このような場合は、担当者にメールを送信する 程度の通知方法で問題ありません。 担当者がメールをすぐに見るとは限りませ んが、翌日や週明けに担当者が出勤してからそのメールを確認して対応すれば 十分です。

一定時間内に同じエラーが複数回発生したら通知する、 といった方法も考え られます。 ログイン失敗などのエラーの場合、 1回だけ失敗することはよくあ ります。 しかし、 何度も連続して同じ利用者がログイン失敗していると、攻撃 を受けている可能性が考えられるためです。

通知から対応までのフローの整備

異常を検知したら、 その状況を担当者に伝えるために通知しますが、難しい

-

モニタリング、アラートの発出

171

のは通知が発生したあとの状況確認です。 たとえば、表4-1 のような課題が挙 げられます。

表4-1 通知したあとの課題 課題 背景 担当者と連絡が取れない アラート発生時に担当者が不在 放置した 複数の担当者がおり、 誰 も担当しない 担当者が連絡を受けたが 担当者がアラートを認識しているが対応を後回しにして しまう 担当者間で責任の所在が曖昧になり、 対応が遅れる 対応したが結果が報告さ 対応後の報告がなされず、 情報共有や次の対応に支障 れない が出る アラートの誤検知、 ノイズ が多い 実際には問題ないのにアラートが頻発し、 対応が煩雑 になる アラート内容が不明瞭、 情報不足 アラートの原因や影響範囲の情報が不足していて対応が 遅れる 対応履歴、 ログの管理が 過去の対応履歴が残っておらず、 同様の問題の再発防 不十分 止につながらない 担当者のスキル不足、 知 識不足 担当者が対応方法を知らず、 対応に時間がかかる 優先順位が不明確 複数のアラートが発生したとき、どれを優先すべきか明 確でない システムの連携不足 複数のシステムが連携しておらず、 情報が断片的にしか

得られない

このような状況の発生を防ぐため、 通知を受け取った担当者が対応を完了す るまでを適切に管理できるよう、 対応手順や連絡網を整備し、定期的な訓練や シミュレーションを実施します。

また、上記のような課題に対応するため、 次のような改善案が考えられます。

第4章 セキュリティ面での活用方法

172

担当者の役割分担と責任範囲の明確化、 自動エスカレーション

● 対応状況の報告義務化、 定期的なフォローアップ

・アラート内容の標準化、 閾値の適正化、 優先順位の設定

マニュアルの整備、 情報の一元管理ツールの導入

次の項では、 特にアラート内容の標準化や閾値の適正化について考えます。 ア ラートを発行する閾値を適切に設定しておかないと、 誤検知が多発して担当者 の負担が増える、もしくは重要な問題を見逃すリスクがあるためです。

運用状況に合わせて閾値を調整して誤検知を減らす

閾値として設定する値が低すぎると、 アラートが頻繁に発生します。 たとえ ば、 CPU 使用率が50%を超えたら警告を出す、といった閾値を設定すると、少 し処理が増えるだけでアラートが出てしまうことがあります。 その多くが問題 ない状況であれば、担当者は「アラート疲れ (アラート疲労)」 という状態にな ります。 第3章で解説した 「オオカミ少年」 の状態で、 担当者が重要なアラー トを見逃す原因になります。

逆に閾値として設定する値が高すぎると、 本当に問題が起きていてもアラー トが出ず、 早期発見ができません。 そこで、 運用状況に合わせて閾値を調整し なければなりません。

まずは現在のアラート発生状況を確認します。 どのくらいの頻度でアラート が出ているか、誤検知や見逃しはないかを分析します。 そして、 誤検知が多い アラートや、重要な異常を見逃している可能性があるアラートをリストアップ し、 閾値の見直し対象を決めます。

また、過去のログや監視データを分析し、 正常時の値の範囲や異常発生時の 特徴を把握します。 そのデータをもとに、 「閾値を上げて誤検知を減らす」 「閾 値を下げて見逃しを減らす」 など細かく調整します。

閾値を変更したあとは、その後のアラート発生状況を注意深く観察し、誤検 知が減ったか、 重要な問題を見逃していないかを確認します。

-

1 モニタリング、アラートの発出

173

誤検知を減らすための工夫として、 複数の条件を組み合わせることが挙げら れます。 単にCPU使用率などを見るのではなく、それまでの時間経過を考慮し て、徐々に増えているのか急激に増えたのかを確認します。 また、 メモリ使用 量やストレージの空き容量など複数の指標を組み合わせてアラートを発生させ ると精度が上がります。

当然、システムの利用状況や構成は変わるため、 閾値も定期的に見直すこと が重要です。実際にアラートを受け取る担当者からの意見を反映して調整する と、現場に合った設定ができます。

システム間の連鎖障害を防ぐログの相関分析

システムが複数のコンポーネントやサービスで構成されている場合、ある部 分で起きた問題が他の部分に影響を与え、連鎖的に障害が発生することがあり ます。 このため、 ログを時系列で並べて、 異常発生直前の操作やイベントを明 らかにしなければなりません。

そして、再発を防止するために、 原因を正確に特定し、障害の発生を減らす 対策を実施します。 このとき、 過去のログを用いて分析することを考えます。

連鎖障害と相関分析

あるシステムやサービスで起きた障害が、 他のシステムやサービスに影響を 及ぼし、次々に障害が発生する現象を連鎖障害といいます。 たとえば、「デー タベースサーバーのストレージの容量不足により書き込みができなくなる」 → 「Web サーバーの応答が遅延する」 → 「APIのタイムアウトが発生する」 → 「ス マホアプリが強制終了される」 といった具合で連鎖していきます。

このような連鎖障害が発生すると、 単一の障害よりも原因が複雑に絡み合っ ているため、調査に時間がかかることがあります。 そこで、複数のシステムや サービスから出力されたログを集めて組み合わせて、 関連する異常を理解する

第4章 セキュリティ面での活用方法

174

ことをログの相関分析 (コリレーション分析) といいます (図3)。

図4-3 相関分析のイメージ

① 3) ④ 次に①②③という ログがあれば④が起きそう

相関分析の流れ

連鎖障害が起きたとき、 その全体像を把握するために、 影響を受けた可能性 があるシステムのログを収集します。 このとき、 単独のログだけではなく、複 数のシステムのログを組み合わせて分析することで、 「連鎖的な問題の流れ」を 把握できます。

もちろん、収集できないシステム (利用者が使っている端末など)もありま すが、可能な限り多くのログを収集します。 そして、 すべてのログのタイムス タンプが正しく同期されていることを確認し、ずれているのであれば調整しま す。

一般的にはNTP2などを用いて常に時刻を同期しておきますが、 実際にはシ ステム間でずれていることは珍しくありません。

そこで、 時刻のずれを調整します。 手軽な方法はログファイルの時刻をスク リプトなどで一括置換する方法です。 たとえば、次のようなPythonのスクリ プトを作成し、 時刻を調整する方法が考えられます。

2 Network Time Protocolの略で、 コンピュータの時刻をネットワーク経由で同期するプロトコル。

モニタリング、アラートの発出

175

convert.py

from datetime import datetime, timedelta import re

time_offset = timedelta (hours=1) #ずれの時間

with open('old.txt', 'r') as infile, open('new.txt', 'w') as outfile: for line in infile: match=re.match(r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (.*)$', line) if match: rest + '\n') timestamp_str, rest = match.groups() dt = datetime.strptime(timestamp_str, '%Y-%m-%d%H:%M:%S') corrected_dt = dt + time_offset outfile.write(corrected_dt.strftime('%Y-%m-%d %H:%M:%S') + else: outfile.write(line)

ログ解析ツールが時刻オフセットを設定できる機能を備えていれば、それを 使う方法もあるでしょう。 少量であれば、 手作業で日時を調整する方法も考え られます。

そのうえで、 ログからエラーや警告、 重要なイベントを抽出し、 「エラー発 「生」 「処理遅延」 「再起動」 などのラベルをつけます。 これにより、 ログの種類 や意味を整理しやすくなります。

ここまでできれば、 抽出したイベントを時系列で並べ、 複数のシステム間で 起きたイベントの時間的なつながりを調べます。 たとえば、 次の流れを把握で きると、どのイベントが他のイベントを引き起こしたことを推測できます。

サーバーAでエラー発生

●エラーの数秒以内にサーバーBで処理遅延が発生

処理遅延後にサーバーCが再起動

第4章 セキュリティ面での活用方法

176

推測ができたら、 必要に応じて他のシステムについてもログを追加で調査し たり、 再現テストを実施したりすることで、 因果関係を検証します。

当然のことながら、 ログの相関分析を実施するときは大量のデータを扱うた め、手作業では大変です。 そこで、 一般的にはSplunkのようなSIEMツールや、 Elastic Stack のようなログ管理プラットフォームを使います。

COLUMN 相関分析を実施するための注意点

相関分析をするときに難しいのは出力されるログの 「粒度」 です。 細かす ぎるログは分析が難しくなり、 逆に粗すぎると原因が特定できません。 ログ の粒度を調整できるシステムであれば、 どのようにログを使うのかを考慮して、 必要な情報を出力しておきましょう。

また、勘違いしやすいのは、 相関があったとしてもそれが因果関係にあ るとは限らないことです。 つまり、必ず同じ時系列の順序でエラーなどの イベントが並んでいても、 必ずしも一方が他方の原因であるとは限りません。 偶然発生しただけかもしれませんので、 実際に再現テストを実施するなど、 検証作業を怠らないようにしましょう。

根本原因分析による再発防止策の検討

システムやサービスでトラブルが起きたとき、 単にその場の問題を解決する だけでなく、 同じトラブルを繰り返さないための対策 (再発防止策) を実施し ます。 この分析のステップで、 ログをどう活用するのかを考えます。

根本原因分析とは?

トラブルや障害の 「根本的な原因」 を見つけ出すための調査手法として根本 原因分析 (RCA; Root Cause Analysis) があります。 これは、表面的に目に 見える動作を確認するだけでなく、 その問題が起きた理由を深く掘り下げて探 る手法です。

-

1 モニタリング、アラートの発出

177

たとえば、あるシステムが停止したり、 ログから通知があったりしたとき、 ダ ッシュボードを確認することで 「サーバーの CPU が高負荷になっていた」 とい った状況を把握できます。 ただし、 知りたいのはCPUが高負荷になっているこ とよりも、なぜCPU が高負荷になったのかという原因です。 具体的には、 特定 の処理で無限ループが発生してCPU を使い切った、 多くの利用者がアクセスし て負荷が高まったことなどが考えられます。

そこで、ログを使って根本原因分析をすることを考えます。 ログには、トラブ ルが発生したときのシステムの状況や操作履歴が細かく記録されています。 こ そのため、問題が起きる前後の状況を正確に把握できます。

ここでも重要なこととして、 1つのログではなく複数のログを組み合わせる ことが挙げられます。 設定ミスのような単純なものであれば1つのログを見る だけで原因を特定できることもありますが、 長く運用している中で発生したト ラブルを解決するときには、 複数の原因が組み合わさって発生している可能性 があるためです。

根本原因分析に使われる手法

根本原因分析を進めるとき、 よく使われる手法として 「なぜなぜ分析」 があ ります。 「なぜ?」 と繰り返し問うことで、問題の原因を深掘りし、 根本原因に たどりつく手法です。 (図4−4 )

図4-4 なぜなぜ分析 なぜ? なぜ?_ 原因 1-1 原因 なぜ? 原因1-2 問題の発生 原因 2-1 原因2 真の原因 原因 2-2

第4章 セキュリティ面での活用方法

178

それに加えて、 視覚的に表現するために 表4-2 のような図を使い、問題の構 造や因果関係を明らかにしていきます。

表4-2 根本原因分析に使われる図や表

図や表 内容 フィッシュボーン図 (特性要因図) フローチャート 故障モード影響解析 (FMEA) 問題の原因と結果の関係を視覚的に表した図。 原因を太い骨 で表現し、 詳細な要因を小さな骨で展開する。 処理の流れや手順を順序立てて整理し、 問題が起きている場 所を特定する。 故障のモードと影響を表に整理し、 ボトムアップでリスクの高 い原因を特定する。 故障の木解析 (FTA) 問題の発生からその原因を木構造で表現し、トップダウンで展 開する。

ここで使われるフィッシュボーン図とは、図4-5 のようなものです。

図4-5 フィッシュボーン図の例 要因 要因 要因 要因 要因 要因 特性

モニタリング、アラートの発出

4.

179

2

セキュリティインシデントの 検出

役立つのはこんなとき

不審なログインをログから調べたい

マルウェア感染をログから調べたい

モニタリングするのはエラーの発生だけではありません。 外部から攻撃を受 けている、社内にマルウェアが広がっているなどのセキュリティインシデント が発生しているかどうかも把握しなければなりません。

このときに使われる手法について解説します。

不審なログイン試行やアクセスを自動検出する

インターネット上にサービスを公開すると、 外部からさまざまなアクセスが あります。 その多くは通常の通信ですが、 中には悪意を持った不審なログイン 試行や異常なアクセスが含まれます。 こうした不正なアクセスを早期に見つけ 出し、 すぐに調査や対応を実施することが重要です。

ここでは、 不審なログイン試行やアクセスを自動的に検出し、 できるだけ早 <調査を開始するために実施すべき対応について解説します。

不審なログイン試行とは

不審なログイン試行として、次のような状況が挙げられます。

第 4 章 セキュリティ面での活用方法

180

●短時間に同じユーザーIDで何度もログインに失敗する

●複数の異なるIPアドレスから同じアカウントに対してログイン試行がある

通常の利用時間帯とは異なる時間帯にアクセスされる

• 通常とは異なる場所 (例: 海外) からアクセスされる

特に前半の2つのようなログイン試行は 「IDを固定してパスワードを変える」 方法で、 「総当たり攻撃」 や 「ブルートフォース攻撃」 と呼ばれます。 この対策 としてよく使われるのは「パスワードを3回間違えたらアカウントをロックす る」という方法です。

この対策を回避するログイン試行として、 「パスワードを固定してIDを変え る」 方法があり、 「リバースブルートフォース攻撃」 と呼ばれます。 同じIDに 対するログイン失敗でなければ、 どのIDのアカウントをロックすればいいのか わからないため、 上述の対策は使えません。 もちろん、 同じIPアドレスからの ログイン試行であれば、そのIPアドレスからのログインをブロックする方法が 考えられますが、 2つ目のようにIPアドレスを変えられるとブロックするのは 大変です。

図4-6 ブルートフォース攻撃とリバースブルートフォース攻撃

ブルートフォース攻撃 リバースブルートフォース攻撃 ID パスワード ID パスワード shoeisha 0000 shoeisha 1234 shoeisha 0001 shoeishb 1234 shoeisha 0002 shoeishc 1234 shoeisha 0003 shoeishd 1234 www. shoeisha 9999 shoeishz 順に試行 1234 順に試行

そこで、 図4-6 のような4桁の数字のようなパスワードではなく、長く複雑 なパスワードを設定することが求められます。

一方で、 後半の2つのようなログイン試行は、判断が難しいものです。 偶然、 本人が普段とは違う時間帯にログインしようとしたのかもしれませんし、 海外

181

2

セキュリティインシデントの検出

4

旅行中なのかもしれません。 このようなときは、 「リスクベース認証」 と呼ばれ る方法が使われ、 普段の認証方法に加え、追加でなんらかの認証を加える方法 が用いられます。

異常なアクセスとは

ログイン試行以外にも、 不審なアクセスに分類されるものとして、 Webサー バーなどをダウンさせることを狙った攻撃が考えられます。 一般的なWebサー バーへのアクセスは人がWebブラウザを使ってマウスで操作するため、リクエ ストの間隔はそれほど短くなりません。

しかし、Webサーバーをダウンさせようとする攻撃の場合、 通常とは異なる 大量のアクセスやリクエストが発生します。 特定のページや機能に対し、プロ グラムを使ってアクセスするため、 1秒間に数百、 数千といったアクセスが殺 到します。

もちろん、テレビで放送される、 有名人がSNSで発信するなどの理由があり、 多くの人がアクセスしているなら理由はわかっており、 それに向けてサーバー のハードウェアを増強する必要があるかもしれませんが、 そうでなければなん らかの攻撃を受けているのかもしれません。

このような場合は、ハードウェアを増強してもそれを上回るペースで攻撃を 受けると意味がありません。 この場合は、 発信元のIPアドレスを特定してブロ ックしたり、攻撃のパターンを把握してブロックしたりするなどの対応が実施 されます。

また、 CDN (Content Delivery Network) などを使い、 攻撃のトラフィッ クを分散させてサーバーへの負荷を軽減する方法も考えられます。

自動で検出する仕組み

不審なログイン試行や異常な数のアクセスがあったときに、 それを自動で検 出することを考えます。 このためには、まずはログを収集しておき、 通常時の

4章 セキュリティ面での活用方法

第

182

状態を把握しておかなければなりません。

そのうえで、ログを解析して不審なアクセスを検出するルールを考えます。 一 般的には統計的に分析し、 通常時よりもネットワークへのアクセスが急激に増 加していることから判断します。 また、 CPU などの負荷が高まってWeb サイ トやサービスの応答が遅くなったり、まったくアクセスできなくなったりする ことから判断できることもあります。

そして、不審な動きを検知したら、即座に担当者に通知します。 通知を受け た担当者は、すぐにログの内容を確認し、 調査や対応を開始します。 ここで問 題になるのは、どうやって正常なアクセスだけを許可し、攻撃のアクセスだけ を拒否するのかを判断する仕組みです。

よく使われるのは、 「特定のIPアドレスや国からのアクセスをブロックする」 という方法です。 日本国内の日本人向けに提供しているサービスであれば、 海 外からのアクセスはブロックしても問題ないことが多いでしょう。 日本国内か らの攻撃をブロックすることはできませんが、 海外からの攻撃が多いことから 管理者のログイン画面などではよく使われています。

また、 「通常のアクセス時間帯外のログイン試行を検知する」 といった方法も あります。 社内システムであれば、 営業時間中のアクセスだけを許可すること で、深夜のアクセスは拒否できます。 ただし、この方法でも営業時間中に攻撃 を受けることを防ぐことはできません。

CPU負荷などのシステムリソースの監視をするときには、第2章で解説した Zabbixや Nagios といったネットワーク監視ツールが使われることが多いです。

上記でも解説したように、 「連続してログインに失敗した回数の閾値を設定し 超えた場合にアラートを出す」 という方法もあります。 同じIPアドレスから複 数のIDに対して連続してログインに失敗しているのであれば自動で検出できる でしょう。

しかし、図4-7 の 「パスワードリスト攻撃」 のように、 どこかで流出したパ スワードのリストを使って攻撃されたとき、別々のIPアドレスからログインを 試行されると、正常な利用者によるものかどうかを検出することは難しいもの です。

42 セキュリティインシデントの検出

183

図4-7 パスワードリスト攻撃

サービスA ID パスワード yamada |X9vR2qLmZp suzuki 流出 ID パスワード yamada | X9vR2qLmZp |mJ7wV8bNpA tanaka |Zf3QxLp9Yr kobayashi B6nZtXv2Rm nakamura |R8sXkLOQev suzuki mJ7wV8bNpA tanaka |Zf3QxLp9Yr サービス B kobayashi B6nZtXv2Rm ID パスワード saito pWm4JqV9K nakamura R8sXkLOQev fujimoto Ld7NcWq9Fx 順に試行 ishikawa HsP3vZ6RmY kobayashi B6nZtXv2Rm 同じパスワードを使っている人がいる arai QxVOLp7Jsb

そこで、機械学習を活用した検出が使われることがあります。 これは、過去の 正常アクセスについてのデータを学習させることで、 異常なアクセスパターン を自動で判別する仕組みです。 利用者の行動パターンを学習させることで、 通 常と異なる動きを検知する方法もあります。

不審なアクセスを検知したあとの調査

不審なログイン試行や異常なアクセスが検知されたあとの対応は、基本的に はシステム障害などと同様です。 アラートが発出されたらリアルタイムに担当 者へ通知し、担当者はログから影響範囲を確認、 対応策を実施する、 という流 れは変わりません。

もちろん、 新しい攻撃手法に対応するため、ルールやアルゴリズムを定期的 に更新し、正常なアクセスを誤って異常と判断しないよう、 閾値や条件を適切 に設定します。

なお、ログイン試行に対しては、 2要素認証や2段階認証のようにパスワード

4章 セキュリティ面での活用方法

第

184

が知られても本人以外はログインできないようにする方法のほか、パスキーの ような新しい対策を実施することが求められています。

ログによるマルウェア感染の兆候の発見

セキュリティという言葉から多くの人が思いつくものとして、マルウェア (悪意のあるソフトウェア) が挙げられます。 コンピュータウイルスとも呼ば れ、コンピュータやネットワークに侵入して情報を盗んだり、 システムを破壊 したりします。 組織内のコンピュータがマルウェアに感染したときは、他のコ ンピュータへの広がりや外部への情報漏えいなどの被害を最小限に抑えるため に、早期発見や対応が求められます。

ここでは、 マルウェア感染の兆候をいち早く見つけるためのログの活用につ いて解説します。

マルウェア感染の兆候とは?

マルウェアに感染すると、 通常とは異なる動作をしたり、 異常な通信が発生 したりします。たとえば、 見慣れないプログラムが起動したり、 サービスが実 行されていたりします。 また、 システムファイルなど重要なファイルが変更さ れることもあります。 場合によっては、管理者権限を要求するダイアログが表 示されたりします。

このとき、 マルウェアの種類によって利用者がその動作に気づくかどうかが 分かれます。 見慣れないプログラムが起動していることに気づけばマルウェア への感染に気づけそうですが、 実際のマルウェアは利用者が気づくような動作 をするのでしょうか。

利用者が気づく例として「ランサムウェア」 への感染があります。 ランサム ウェアは 「身代金要求型ウイルス」 とも呼ばれ、 コンピュータに保存されてい るデータを勝手に暗号化し、元に戻すために身代金を要求するものです。 この ため、感染したことが画面に表示され、 コンピュータが正しく動作しなくなり

セキュリティインシデントの検出

1

2

4

185

ます。 このような場合は感染したことがわかりやすく、 身代金を支払わないた めには事前に取得しておいたバックアップからの復旧が必要です。

一方で、利用者が気づかない例もあります。 攻撃者としては、利用者に気づ かれずに感染させることで、さまざまな情報を盗み出せる時間を確保できます し、感染させる台数を増やすこともできます。 このため、 できるだけ利用者が気 づかないように感染させたいものです。 そして、情報を盗み出したあとで、そ の情報を公開すると脅すことで金品を要求する場合があり、 「ノーウェアランサ ム」 と呼ばれています。

このように利用者が気づかないうちに感染することを防ぐためには、ウイル ス対策ソフトの導入が必須です。 ウイルス対策ソフトのパターンファイルを最 新にしていると、 世の中で一般的に使われているマルウェアであれば検知でき る可能性があります。

しかし、ウイルス対策ソフトでも検出できないマルウェアもあります。 その 組織に向けて特化して作成された、世の中で出回っていないマルウェアであれ ば、パターンファイルを最新にしていても検出することはできません。

このため、ウイルス対策ソフトによっては「振る舞い検知」 という機能を備 えているものもあります。 この場合、 マルウェアのパターンとは一致しなくて も、一般的なマルウェアに似たような動作をするプログラムを不審だと検知し てくれます。

■ イベントログを活用して早期に発見する

ウイルス対策ソフトが検出した内容も、 その種類によってはログに出力され ている場合があります。 しかし、ウイルス対策ソフトが検知できなくても、 マ ルウェアの感染に気づける可能性がある方法として、OSやネットワークのロ グの確認があります。

たとえば、 OS のイベントログを見ると、 アプリケーションのインストール ログが残っている可能性があります。 また、 アプリケーションが起動した、通 信したといったタイミングでのログが残っている可能性があります。

第 4 章 セキュリティ面での活用方法

186

たとえば、Windows ではアプリケーションの起動ログをイベントビューア ーで確認できます。 なお、 新しいプロセスが起動したかどうかをログに記録す るには、監査ポリシーの有効化が必要です。

Windows には 「監査機能」 が用意されており、これを使うとプロセスの起 動についてのログを取得できます。 このためには、 Windows の 「ファイル名 を指定して実行 (Windows + Rキー)」 から 「gpedit.msc」 と入力して Enter キーを押すと「ローカルグループポリシーエディター」が開きます。

そして、「コンピューターの構成」 → 「Windowsの設定」 → 「セキュリティ の設定」 → 「ローカルポリシー」 「監査ポリシー」とたどり、 「プロセス追跡 の監査」 をダブルクリックします。 ここで、「成功」 と 「失敗」 にチェックを入 れて有効化します (図4-8)。

図4-8 プロセス追跡の監査の設定

ローカルグループポリシーエディ ファイル) 操作() 表示M ヘルプも ローカルコンピューターポリシー コンピューターの構成 ポリシー アカウント ログオンイベントの ソフトウェアの設定 Windom の設定 アカウント管理 オブジェクトアクセスの リソー 2 スクリプト スタートアップ/シャットダ されたプリンター ディレクトリサーのアクセスの プロセス セキュリティの アカウントポ ポ リシーの変更の ローカルポリシー ログオンイベントの 特種用の会 ユーザー 当て セキュリティオプション セキュリティ強化されたWin ネットワーク リストマネージャー キーのポリシー ソフトウェア の アプリケーシ セキュリティポリ の詳細な構成 ギリシーベースの QoS テンプレート ユーザーの構 ソフトウェアの設定 Windows 管用テンプレート プロセスロパティ ローカルセキュリティ プロセス □ する ゴレベルを上 れている場合は、されます。 キャングル

この設定をすると、 イベントビューアーの 「Windowsログ」 → 「セキュリ ティ」に「イベントID 4688」 で出力されます。 ここに、新しいプロセス名と

2

セキュリティインシデントの検出

4

187

して、 実行したアプリの名前が表示されます (図4-9)。

図4-9 監査結果の表示

コンピューター ファイル) ヘルプ ムー コンローターカル タスクスケジューラ イベントローアー Y7-07 Security 9-2: 147 操作 セキュリティ キーワード カスタムビュー 成功の監査 LROOKE 5 Windowsログ Application Setup 成功 成功の監査 QR 日付と時刻 2025/06/06 11:14.25 2025/06/06 11:1426 2005/06/06 11:1424 2125/06/06 11:1424 2025/05/29 11:19:59 2025/05/29 11:1959 ソース Microsoft Windows Me Microsoft Windows Secu Mosoft Windows sen Microsoft Windows secu Microsoft Windows secu イベントID タスクのカテゴリ 4688 Process Creation 4608 Procams Creation 4688 Process Creation 4588 Procm Creation Microsoft Windows secu 4688 Process Creation システム Forwarded Event サービ イベント Mrosoft Window rty auditive サブスクリプション 共有フォルダー ローカル ユーザーとグループ パフォーマンス 4 Probes Creation ▼ カスタム ビューの作成 カスタムビューのインポート 0703. ▼ 現在のログをフィルター フィルターのクリア プロテイ Q フィルターされたログファイル。 このログにタスクを設定 デバイスマネージャー 記憶 ディスクの管理 プロセス フ ィルターをカスタム サービスとアプリケーション Mandatory LabelMedium Mandatory Level プロセスのコマンドライン L70116 CProgram First OfficeO1EWWORD EXE 必須ラベル 作プロセス 作成元プロセス名 表示 1 ヘルプ イベント rosoft Wind 日 イベントのプロバ 0508500 セキュリティ ソース Morow Windows soury 050811) 2025/06/06 11:14:24 イベント DD LUKU ユーザー NA タスクのカテラバウ Process Createn キーワードCO コンピューター(0 成功の監査 このイベントにタスクを設定 コピー 選択したイベントの保存 オペコード) 情報 ヘルプ

当然、マルウェアの種類は日々変わるため、最新の情報をキャッチアップし、 新しいマルウェアの動作を把握しておかなければなりません。

ネットワークのログを活用して発見する

パソコンの内部での動作を確認するだけでなく、 ネットワークのログを使っ て不審な動作を確認することもできます。 普段は使われていないはずの外部の 不審なIPアドレスに対する大量の通信が記録されていたり、 一般的な使用では アクセスしないはずのポート番号へのアクセスが記録されていたりすると、そ の発信元を特定することが考えられます。

一般的には、ファイアウォールやIDS/IPSといったネットワーク機器の通信

第4章 セキュリティ面での活用方法

188

ログを見ると、外部への情報の送信などのログが残っている可能性があります。 よく使われるファイアウォールとして iptablesがあります。 iptables を使うと、 どのIPアドレスからどのポート番号へのアクセスについてログを取得する、と いった具合で、 出力するログを細かく制御できます。

たとえば、 Alma Linux に root権限を持つユーザーでログインした状態で、次 のコマンドを実行すると、 現在設定されているポリシーと、ルールのリストが 表示されます。

# iptables -L Chain INPUT (policy ACCEPT) target prot opt source Chain FORWARD (policy ACCEPT) destination target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination

ここには「INPUT」 「FORWARD」 「OUTPUT」 という3つの経路が書かれ ています。 この経路を Chain (チェイン)といい、図4-10 のような手順で処理 されます。

図4-10 iptables の処理手順

ネットワーク インターフェイス ルーティング INPUT ネットワーク インターフェイス FORWARD OUTPUT ローカル 处理

2 セキュリティインシデントの検出

611 189

そして、それぞれに対してどのような通信を許可するのか、 拒否するのかを 設定できます。 たとえば、 「192.168.1.1」 という IPアドレスへの通信を拒否す るには、次のようなコマンドを実行します。

# iptables -A OUTPUT -d 192.168.1.1 -j DROP

これは、 「OUTPUT」 というチェインに対して、 「192.168.1.1」 への通信は DROP、 つまり拒否 (破棄) するというものです。 この 「-d」 は 「destination」、 つまり宛先を意味します。 同様に、 「192.168.1.1」 からの通信を拒否するには、 次のコマンドを実行します。 この 「-s」 は 「source」、 つまり発信元を意味しま す。

# iptables -A INPUT -s 192.168.1.1 - DROP

ただし、単に拒否するだけではログに記録されません。 そこで、次のように ログに記録するように設定しておきます。 これは、 「192.168.1.1」 からの通信 があれば、それを1行目でログに記録し、2行目で拒否しています。

# iptables -A INPUT -s 192.168.1.1 - LOG --log-prefix "IPTables- Dropped: "--log-level4 ればなりません。 # iptables -A INPUT -s 192.168.1.1 - DROP

これを使って、 通常とは異なるログを検知するルールを設定することが考え られます。 たとえば、 不審なアプリケーションの起動やファイルの操作、 管理 者権限の要求のほか、 通常の通信パターンとは異なる通信を検知するルールが 挙げられます。

4章 セキュリティ面での活用方法

第

190

3

内部不正対策

役立つのはこんなとき

内部不正に気づくために監査したい

ログの改ざんを検知したい

企業の情報漏えいの原因として、 内部不正もあります。 権限のある従業員が、 業務で必要なデータにアクセスするのは問題ありませんが、そのデータを意図 的に外部に持ち出し、 外部に流出させてしまうと問題になります。

社内システムの操作ログから異常な操作を検知する

仕事で扱うデータは一般的に社内システムに保存されています。 そして、 そ のサーバーにはアクセス権限が付与されており、 必要な権限を持っている人だ けがアクセスできるように設定されています。

問題なのは、この権限を悪用する人がいることです。 これを検知するために できることを考えます。 たとえば、 企業や組織の重要な情報が外部に持ち出さ れたり、不正にコピーされたりすると、 情報漏えいや損害につながるリスクが 高まります。 こうした不正行為を早期に発見し防止するために、データの持ち 出しやコピー操作に関するログを活用することが効果的です。

データ持ち出し・コピー操作の監視

内部関係者による意図的または過失によるデータの持ち出しやコピーを防ぐ

- 3 内部不正対策

4

191

ことを考えると監視を強化し、 異常な操作を早期に検知することが挙げられま す。 このときに監視対象となる操作ログの種類として、次のようなものがあり ます。

・ファイルのコピーや移動操作のログ

●USBメモリや外部ストレージへの書き込みログ

オンラインストレージへのアップロードログ

印刷のログ

・ ファイルアクセスのログ (いつ誰がどのファイルにアクセスしたか)

そして、これらに対してログを監視していると、 次のような操作が実施され たことに気づきます。

・通常はあまり操作されない大量のコピーや短時間での大量ファイル移動

●業務時間外や休日の持ち出し操作

●許可されていない外部デバイスへの書き込み

特定の重要ファイルへのアクセスやコピー

このような操作を 「異常」 として定義し、 検知ルールを設定します。 そして、 異常が発生したときにアラートを担当者に通知し、 可能な限り早く調査できる ようにします。

Windows でのファイル操作のログ出力

Windows でこのようなログを取得することを考えます。 Windows 11では、 「ファイルのコピーや移動操作のログ」 は、 標準では詳細なログとして記録され ていません。 ここでも、 Windows の監査機能を使うことで、 ファイルのアク セスや操作のログを取得できます。

まずは、ファイルのコピーや移動操作を監査ログとして記録するために、 「監

第4章 セキュリティ面での活用方法

192

「査ポリシー」 を有効にします。 このためには、 Windows の 「ファイル名を指 定して実行(Windows + Rキー) 」 から 「gpedit.msc」 と入力して Enter キー を押すと「ローカルグループポリシーエディター」が開きます。

そして、「コンピューターの構成」 -> 「Windows の設定」→「セキュリティ の設定」 -> 「ローカルポリシー」→「監査ポリシー」とたどり、 「オブジェクト アクセスの監査」 をダブルクリックします。 ここで、 「成功」 と 「失敗」にチェ ックを入れて有効化します (図4-11)。

図4-11 オブジェクトアクセスの監査を設定

ローカルグループポ ファイル操作) ヘルプ & m 「ローカルコンピューターボ オブジェクトアクセス カルセキュ オブジェクトアクセス アカウント イベント ア カウント Window オブジェクトアクセス イベントの スクリプトスタートアップサットダウン) プリンター ディレクトリサービスのアクセスの プロセス セキュリティの設定 アカウントポリシー ロ ーカルポリシー ユーザーの割り当て セキュリティオプション セキュリティが強化された Windows C ネットワーク リストマネージャーポリシー 公 開キーのポリシー ソフトウェアの アプリケーション ポリシー セキュリティカル ログオンイベントの ポリシーの ポリシーベースの QoS テンプレート ユーザー ソフトウェア Window アンプレート 大 する れている場合は、撮れな してください。 CS キャンセル

そして、ログを記録したいファイルやフォルダに監査エントリを設定します。 具体的には、ログを記録したいファイルやフォルダを右クリックし、「プロパテ ィ」→「セキュリティ」 タブを開き、 「詳細設定」のボタンを押します。 すると、 「監査」 というタブがあるので、この中から 「追加」ボタンを押します。

そして「プリンシパルの選択」 から監査対象のユーザー 「Everyone」 などを 指定し、 「種類」 の欄で 「成功」 や 「失敗」 などを選択します。 さらに、 「基本 のアクセス許可」 から 「高度なアクセス許可」 を表示すると、 さまざまな操作

・3 内部不正対策

4

193

についてアクセス許可を選択できます (図4-12)。

図4-12 ファイルやフォルダへの監査エントリの設定

| のセキュリティの詳細設定 名前: CUsers 所有者: 增井敏克(] アクセス許可 監查 有効なアクセス 変更(C) リソースプロパティ

変更するには、 エントリを選択し、[編集] 使用できる場合) をクリックします。 監査エントリ プリンシパル Everyone アクセス 龍承元 成功 フルコントロール なし

詳細については、監査エントリをダブルクリックしてください。 監査エントリを

編集(E)

追加(D)

継承の無効化

キャンセル

注意:

適用(A)

このような設定を保存 (適用) して閉じます。 そのうえで、 次はファイルに 対するなんらかのアクセスがあったときのログを確認します。

Windows のイベントビューアーを開き、 「Windows ログ」 から 「セキュリ ティ」 ログを確認すると、 指定したファイルやフォルダに対する操作の詳細が ログとして記録されます。

監査ログは大量に生成されるため、必要なフォルダやファイルに絞って監査設定するこ とを推奨します。

(R)

OK

第4章 セキュリティ面での活用方法

194

Windows でのUSBメモリの接続についてのログ

もう1つの例として、 Windows 11で 「USBメモリを接続したログ」 を確認 することを考えます。 USBデバイスの接続や切断は、Windowsのイベントビ ューアーを開き、 「Windowsログ」 → 「システム」 に記録されます。

標準では、詳細なUSBメモリの接続情報が出るとは限りません。 そこで、監 査ポリシーでUSBメモリの接続を監査することを考えます。 このときも、 上記 と同様に 「ローカルグループポリシーエディター」 を開きます。

そして、「コンピューターの構成」 → 「Windowsの設定」 → 「セキュリティ の設定」 → 「監査ポリシーの詳細な構成」 → 「システム監査ポリシー」 → 「オ ブジェクトアクセス」 -> 「リムーバブル記憶域の監査」 をダブルクリックしま す(図4-13)。

そして、「次の監査イベントを構成する」 にチェックし、 成功と失敗を選択し ます。 これにより、USBメモリを接続すると、 「セキュリティ」 ログに記録さ れます。

図4-13 リムーバブル記憶域の監査

ローカルグループポリ ファイル操作(A) 表示 ヘルプ リムーバブル カルコンピューター サブカテゴリ コンピューター リケーション ソフトウェア サービスの Windows 詳細なファイル共有 ファ スクリプト スタートアップ プリンター ファイルシステムの転 会 フィルタリング ローカル リ ハンドル セキュリティ Window カーネルオブジェクトの ネットワーク リストマネージャーポリ そのオブジェクトアクセス イベント シー レジストリの査 ア リムーバブル アプリケーション セキュリティポリ 構 アカウント ログオン アカウント アクセス オ オアフェクトアクセス ポリシーの変更 使用 5 システム ポリシーベースのGos テンプレート ユーザーの構 キャンセル

43 内部不正対策

195

ログの改ざんを検出し、 証拠を保全する

システムのログはトラブル調査やセキュリティ監視に欠かせない重要な記録で すが、悪意ある攻撃者や不正な内部者がログを改ざんすることがあります。ロ グが改ざんされると、真実が隠されてしまい、 原因究明や証拠保全が困難にな ります。 そこで、ログ改ざんの痕跡を検出し、 信頼性の高い証拠として活用す る方法が重要です。

ログの改ざんを検出する必要性

ログはシステムの動作履歴や操作記録を示す証拠なので、 ログが改ざんされ ると「不正行為の証拠がなくなる」 「調査の信頼性が失われる」 「法的な証拠と して使えなくなる」 などの問題があります。 このため、 改ざんを防ぐとともに、 早期に検出することが求められます。

ログが改ざんされたことをどのようにして検出すればよいでしょうか? たとえば、ログが改ざんされると、 次のような痕跡が残ることがあります。

●ログファイルのタイムスタンプの不自然な変更

●ログの連続性や一貫性の欠如 (例: ログの順序が乱れている)

●ハッシュ値やデジタル署名の不一致

ログファイルのサイズや内容の異常な変化

改ざん検出の手法

改ざんを検知するには、まずは正規のログを適切に保管しておく必要があり ます。 ログは日々更新されるため、 そのままでは更新されるたびに改ざんと判 定されてしまいます。

そこで、1つの方法としてログローテーションされたログを対象にする方法

4章 セキュリティ面での活用方法

第

196

があります。 ログローテーションされた古いログファイル (例: 「/var/log/ messages.1」 など) は基本的に更新されないため、これらを監視対象にする方 法です。

たとえば、改ざんを検出できるツールとして、 「Tripwire」 があります。 Tripwire はファイルの整合性を監視し、改ざんや不正変更を検出するツールで す。 具体的には、ファイルのハッシュ値を記録し、定期的に比較して変更を検 出します。

Alma Linux でTripwire をインストールするには、EPEL リポジトリを有効 化し、そこからインストールします。

# dnf install epel-release-y # dnf install tripwire -y

次に、 Tripwire の初期設定として、2つの鍵を生成します。 Tripwire では、管 理者が保持する鍵である 「サイトキー」 と、 監視対象が保持する鍵である「ロ カルキー」 を使うためです。

これらの鍵を生成するには、次のコマンドを実行し、パスフレーズを入力し ます。 パスフレーズは忘れないように注意してください。

#tripwire-setup-keyfiles

このコマンドにより、サイトキーとローカルキーの2つの鍵が 「/etc/tripwire/ site.key」 と 「/etc/tripwire/ <ホスト名>-local.key」 に生成されます。

また、合わせて 「設定ファイル」と「ポリシーファイル」 も作成されます。 設定ファイルでは、 報告するレベルや、 報告するときに使うメールプログラム (sendmailなど) を指定できます。 ポリシーファイルでは監視する対象を決め ており、 テキストファイルで監視ルールを定義しています。 この設定ファイル は 「/etc/tripwire/twcfg.txt」、 ポリシーファイルは 「/etc/tripwire/twpol. txt」という名前で作成されます。

-

43 内部不正対策

197

そして、コマンドへの応答で上記のサイトキーを入力することで署名され、そ れぞれ「/etc/tripwire/tw.cfg」 と 「/etc/tripwire/tw.pol」 というファイル に格納されます。

必要に応じて、この署名前の 「/etc/tripwire/twpol.txt」 をテキストエディ タで開いて、 監視対象や除外対象をカスタマイズしてください。 ポリシーを変 更した場合は、再度署名が必要なので、次のコマンドでポリシーファイルに署 名して、Tripwireが読み込める形式に変換します。

#twadmin --create-polfile-S /etc/tripwire/site.key /etc/tripwire/ twpol.txt

ここまで準備ができれば、 監視対象のファイルの状態を記録するためにデー タベースを作成します。

#tripwire --init

改ざんを検知するには、 次のコマンドを実行します。 もし内容が変更されて いれば、レポートが 「/var/lib/tripwire/report/」 に保存されます。

#tripwire --check

運用を開始したあとは、 定期的に 「tripwire --check」 を実行し、改ざんさ れていないことを検知します。 基本的にはcron などで自動化するとよいでしょ う。

なお、 syslog サーバーなどの他のサーバーや、 SIEM にログを転送し、転送 元と一致することを確認する方法もあります。 もっと高度なチェックをしたい 場合は、 デジタル署名を用いて、 ログの改ざん検出と真正性の保証をする方法 もあります。 また、 ログを時系列に並べて連続性をチェックすることで、不自 然なずれや欠落を検出する方法もあります。

第 4 章 セキュリティ面での活用方法

198

第

5 ビジネス面での活用方法

章

ログはシステム管理者などがセキュリティ面で使うだけでなく、他 の担当者がビジネス面で使いたいこともあります。 どのような使 い方ができるのか、 具体的な方法について紹介します。

5

アクセスログの解析

役立つのはこんなとき

アクセスログの解析に使われる指標を知りたい アクセスログを使った高度な分析手法を知りたい

ビジネスの視点からログを扱うとき、マーケティング部門などでよく使われ るのがアクセスログです。 Web サイトやWebアプリを運用している企業など では広報担当者が使うことが多く、 アクセスしてきた利用者がどのようにペー ジを遷移したのか、 どのような属性の利用者がいるのか、 Web サイトをどう改 善すればいいのかを分析しています。

アクセス解析に使われる指標

アクセスログを解析する目的として、 利用者のWebサイト内での行動を把握 することが挙げられます。 利用者がどのページから流入し、 どのような経路を たどってWeb サイト内を遷移しているかを分析することで、 利用者がWeb サ イトに求めているものがわかります。 これは、Webサイトの構成を見直して最 適化することにもつながります。

まずは、記録されている内容から集計できる指標について考えます。

■ アクセスログに記録されている内容

Webサーバーのアクセスログに記録されている項目は、使っている Web サ

第5章 ビジネス面での活用方法

200

ーバーの種類や設定によります。 たとえばApacheを使っている場合、標準設 定では次のような内容が出力されています。

203.0.113.1 [01/Jul/2025:12:34:56 +0900] "GET /test.html HTTP/1.1" 200 219 "http://example.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version / 18.5 Safari/605.1.15"

ここに出力されている内容を項目別に分けると、裏5-1 のような情報である ことがわかります。

表5-1 アクセスログの情報

項目 IPアドレス 内容 203.0.113.1 ユーザー リクエスト時刻 (空欄) 01/Jul/2025:12:34:56 +0900 リクエストライン GET /test.html HTTP/1.1 HTTPステータスコード 200 送信バイト数 219 リファラー ユーザーエージェント http://example.com Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.5 Safari/605.1.15

このIPアドレスやリクエスト時刻、 リファラーなどの情報からある程度の分 析はできそうですが、 アクセスログでは得られない情報を使って分析したいこ ともあります。 このようなときは、Google Analytics などのツールを Web サ イト上に導入し、 このツールによって出力される情報を使って集計することを 考えます。

1 アクセスログの解析

5

201

以下の項では、これらの情報を使って分析できる指標について解説します。

ページビュー

Webサイトを利用者が閲覧した回数を表す指標としてページビュー (PV: Page View) があります。 一般的なWebサイトの中には複数のページがあり、 そのWebサイト内で1人の利用者が3つのページを閲覧するとページビューは 「3」 とカウントできます (図5-1 )。

単純にアクセスログに記録された内容をリクエストのURL (上記のログであ ればリクエストラインに書かれている 「 /test.html」 のようなパス名)で集計 するだけなので手軽に使えます。

図51 ページビュー

PV=4 PV=3 PV=2 <1> <1> <1>

閲覧数を把握するときによく使われる指標ではあるものの、 同じページを再 読み込みしたときもカウントされます。 つまり、同じページを同じ人が3回閲 覧したときもページビューは「3」 です。 このため、 ページビューとしてカウ ントされた値は実際の閲覧者数とは異なります。

それでも、ページビューが多いWebサイトはコンテンツの人気が高いことを 表すことが多いため、マーケティングの指標としてよく使われます。 特に、 Web

第5章 ビジネス面での活用方法

202

サイトの中に複数のページがあるとき、 どのページが多く閲覧されているかを 分析することで、 利用者の関心が高いページを特定できます。 このため、 Web サイトの構成の見直しやプロモーションの改善にも使われています。

なお、ページビューが増えることは広告収益の増加にも直結するため、 ブロ グサイトなどに広告を掲載するときに企業側が判断する評価指標としてもよく 使われます。

ユニークユーザー

Webサイトに訪れた利用者のことをユーザーといいます。 同じ人が1日に複数 回同じサイトを訪れたときは、管理者としては1人のユーザーとしてカウントし たいでしょう。 そこで、 アクセスしたユーザーの数をユニークユーザー (UU: Unique User) といいます。 ただし、 上記で解説したアクセスログに記録され ている情報だけでは、 同じユーザーを識別することはできません。

IPアドレスやブラウザ情報が同じであれば同じ利用者だと考えることもでき るかもしれませんが、 ある会社のオフィスから複数の利用者がアクセスしてい ると、同じIPアドレスになってしまいます。 同じ会社であれば、 同じOSで同 じWebブラウザを使っていることも多いでしょう。

このため、単にアクセスログを使うだけではユーザーを識別することはでき ません。 そこで、 Web サーバーから発行したデータをWebブラウザに保存す るCookieや、利用者がIDとパスワードを入力してログインしたときの情報を 使って識別する方法がよく使われます。 なお、 Google Analytics などのツール であれば、 同じユーザーによるアクセスであることを集計できる機能を備えて います。

このとき、 Cookie を使うだけであれば、 同じユーザーがアクセスしていても、 異なる端末や Web ブラウザを使った場合は、別のユーザーとしてカウントされ ることには注意が必要です。 たとえば、 1台のパソコンでChromeとSafariと いう2つのWebブラウザを使ってアクセスするとCookie の値は異なり、 同じ ユーザーとは識別されません。

5-1 アクセスログの解析

203

ログインしたときの情報を使うと、 Web ブラウザが変わっても同じユーザー であることを識別できます。

なお、Webサイトがどの程度の範囲に届いているのかを把握するためにユー ザー数を指標として使うことがあります。 たとえば、 新規ユーザー数が増えて いれば、新しい顧客を獲得できていることがわかりますし、 リピーター数が多 ければ既存の顧客が定着していることを意味します。

セッション

あるユーザーがWebサイトを訪問したときは、 1つのページを閲覧して離脱 することもありますし、 そのWebサイト内のページを順にたどることもありま す。 このとき、ユーザーが最初のページを表示してから他の Web サイトに離れ ていくまでの一連の行動をセッション (Session) といいます (図5-2)。

図5-2 セッション

流入 セッション <1> </> </> </> 離脫

このセッションの数を集計したセッション数はWebサイトに対する訪問回数 を意味するため、 ユーザー数に対して一定の期間内におけるセッション数が多 いと、リピーターが多いことを意味します。

また、1回の訪問でどのような行動をしたのかを把握できるため、 ページビ ューよりも信頼できる値として効果の測定に使われることが増えています。 た とえば、新しい広告を配信したときにセッション数が増えれば、この広告によ る集客効果があったと判断できます。

また、利用者がWebサイトを閲覧している平均的な時間を平均セッション時

第5章 ビジネス面での活用方法

204

間 (Average Session Duration) といいます。 セッションの継続時間が長くな ると、サイト内の多くのページを閲覧していることを意味し、 訪問者がWeb サ イト内のコンテンツに対して興味を持っていると想像できます。 ただし、長け ればよいわけではなく、 Web サイトの目的に応じて適切な時間は異なります。

たとえば、情報提供サイトでは長い時間滞在してもらうことが望ましい一方 で、ECサイトでは短時間に効率よく購入してもらえると嬉しいでしょう。 この ように平均セッション時間を分析することで、 ユーザーが何に興味を持って閲 覧しているのか、 Web サイト内にどのようなコンテンツを増やせばよいのかを 分析できます。

直帰率

Web サイトを閲覧した利用者が、 最初にアクセスしたページだけを閲覧して、 他のページを開かずに離脱したことを直帰といいます (図5-3 )。 そして、こ の直帰の割合を表す指標として直帰率 (Bounce Rate) があります。

図5-3 直帰

ページA </> Webサイト 離脫 ページB ページAしか 閲覧しない <1> ページ C </>

たとえば、 検索サイトで記事を検索して上位に表示されたページにアクセス して訪問したとき、 最初の記事だけを閲覧してWebサイトから離脱すると、直

205

1

アクセスログの解析

5

帰としてカウントされます。 直帰率が高い Webサイトは、 利用者がそのページ 以外に興味がない、利用者が求めている情報が得られない、ページの読み込み 速度が遅いなどの原因が考えられます。

類似する Web サイトなどと比較して、 自社の直帰率が高いと判断された場合 には、コンテンツの質を向上させたり、 関連するページへのリンクを増やした りして、他のページも閲覧してもらうことを考えます。 特になんらかの商品に ついてのランディングページを作成して公開しているとき、 そのページの直帰 率が高い場合には、広告の費用対効果に影響がありそうです。 このため、自分 が管理している Web サイトについては、それぞれのページの直帰率を測定する とともに、そのコンテンツなどを継続的に改善しなければなりません。

□ 離脱率

あるページを見たあとに、そのページを最後にしてWebサイトを離れた利用 者の割合を表す指標として離脱率 (Exit Rate) があります。 Web サイト内の ページごとに離脱率を計算し、どのページの離脱率が高いのかを計測すること で、 どのページが最後になっているのかを把握します。

特定のページの離脱率が著しく高いのであれば、そのページの内容に問題が ある可能性があります。 たとえば、ECサイトにおいて、 購入手続きの途中で離 脱する人が多いのであれば、操作が難しい、 入力項目が多い、 セキュリティに 不安を感じているなどの問題が考えられます。 このため、ページのデザインを 改善したり、 導線を見直したりすることで、離脱率を下げられる可能性があり ます。

もちろん、 コラムなどの連載を掲載している Web サイトであれば、 最終回の ページの離脱率は高くなることが想定されるため、 離脱率が高くてもそれだけ で悪いと決めつけることはできません。

1 ユーザーが最初に訪れるページで、一般的には商品の購入や問い合わせなどのアクションを期待して設置 しているページを指す。

第5章 ビジネス面での活用方法

206

コンバージョンとコンバージョン率

Web サイトを運営するときは、その組織がWebサイトを公開する目的があ るでしょう。 たとえば、 ECサイトであれば商品の購入、SNSであれば会員登 録、 企業のWebサイトであればメルマガの登録などの目的が挙げられます。

このとき、その目的に対してどのくらいの効果があったのかを測定するた めには、それぞれの目的に応じた目標を設定することが多いものです。 この ように設定した目標が達成されたことを表す数値としてコンバージョン (CV: Conversion) があります。

ECサイトであれば、 10人が商品を購入すると、 CVは10といった値になりま す。また、Web サイトを訪れた利用者のうち、 実際にコンバージョンに至った 割合をコンバージョン率 (CVR: Conversion Rate) といいます。 具体的には、 「コンバージョン数÷訪問者数×100」 で計算でき、1,000人の訪問者のうち10 人が商品を購入すると、 コンバージョン率は1% です。

もし同業他社などと比較してコンバージョン率が低いのであれば、その原因を 探らなければなりません。 もちろん、取り扱っている商品に魅力がない、 登録 するメリットを感じない、などの課題があれば、それは仕方ありませんが、そ れ以外にWeb サイト内のページ構成など、 導線や商品説明に問題があると改善 できそうです。

具体的には、 ECサイトであれば購入までの動線が見つけにくい、 企業のWeb サイトであればメルマガへの登録方法がわかりにくい、といった問題が考えら れます。

また、特定の流入元やデバイスでコンバージョン率が低いのであれば、 広告 を出すときの対象者の見直しや、スマートフォン向けサイトなどのデザインの 改善が必要かもしれません。 このため、複数の流入元、 複数のデバイスなどで A/Bテストを実施し、 コンバージョン率を比べるなど、複数の視点から分析す ることが求められます。

2 メールマガジンの略。 企業などがメールで情報を定期的に配信するために使われる。

1 アクセスログの解析

1

5

207

■ページ/セッション

Webサイトに複数のページが用意されており、 そのページを順に利用者が閲 覧しているとき、1つのセッションで閲覧されるページ数の平均をページ/セッ ション (Pages per Session) といいます。 利用者がサイト内のページに興味 を持っていて、順に閲覧していると、この数値は高くなります。

一方で、この数値が小さいということは直帰率が高く、 1回の訪問であまり 多くのページを閲覧していないことを意味します。 このようなWebサイトの場 合は、関連するコンテンツに誘導するリンクをページ内に増やしたり、 メニュ ーやサイト構成をわかりやすくしたりすることが求められます。

アクセスログを使った高度な分析

前節では、 アクセスログから見えてくる情報について紹介しました。 しかし、 これらの数値を見ているだけでなく、 実際にWebサイトを改善しなければなり ません。 このためには、 もう一歩進んだ分析が必要です。

■クロスセルとマーケットバスケット分析

顧客が買おうとしている商品だけでなく、関連する別の商品をおすすめして 一緒に買ってもらうことをクロスセル (Cross-sell) といいます。 たとえば、ス マートフォンを買おうとしている顧客に、ケースやイヤホンなどを勧めること や、ハンバーガーを注文したときにポテトやドリンクを注文してもらうことが 挙げられます。

これにより顧客単価が上がり、顧客にとってもニーズにあった商品を短時間 で手に入れられることで満足感があります。 このとき、 重要なのは購入しよう とした商品や、手に入れたいと思っていた情報に関連するものを提案すること です。 まったく無関係な商品や情報を提示されると、不信感を持たれてしまう 可能性もあります。

-

第5章 ビジネス面での活用方法

208

これはWebサイトでも同じで、 情報サイトであれば、 同じカテゴリでの記事 の人気ランキングや、 同じジャンルの記事などの一覧を表示し、これらの記事 に誘導することが考えられます。 閲覧しているページに関係する広告が表示さ れると興味を持って開くかもしれませんが、 無関係なものでは無視するだけで なく2度とアクセスしない可能性があるのです。

よく使われるのはECサイトで、 「この商品を買っている人はこんな商品も買 っています」といった表示によって、追加で購入できる商品を提案する方法が 考えられます。担当者の経験や感覚によって、追加で購入して欲しい商品を選 ぶこともできますが、 具体的な根拠を持って選びたいものです。

このときは、マーケットバスケット分析という手法が使われ、 具体的にはア ソシエーション分析という方法が有名です。 これは、複数の商品についての売 上などから関係性を調べる手法です。

複数の商品間の関係性を理論的な根拠として示すため、 数値化することを考え ます。 たとえば、 ある商品Bを手に取った人に対して、 他の商品を勧めることを 考えます。 このとき、 商品Aと商品Bについての関係性を調べるには、5-2 に挙げる4つの指標が使われることが多いです。

5-2 アソシエーション分析の指標

指標 概要 支持度 顧客全体のうち、 商品Aと商品Bを同時に購入する顧客の割合 信頼度 商品Aの購入者のうち、 商品Bも同時に購入する顧客の割合 期待信賴度 顧客全体のうち、 商品Bを購入する顧客の割合 (商品Bを単独で購入 する顧客の割合) リフト値 期待信頼度のうち、信頼度の割合 (商品Bを単体ではなく商品Aと合 わせて買われやすいことを表す)

わかりやすくするために、ハンバーガー店に100人が来店し、 5-3 のよう な商品を購入したとします。

-1 アクセスログの解析

5

209

表5-3 購入した商品の例

購入した商品 人数 ハンバーガー 70人 ポテト 50人 ドリンク 60人 ハンバーガーとポテト 40人 ハンバーガーとドリンク 30人 ポテトとドリンク 30人 ハンバーガー、 ポテト、 ドリンク 20人

このとき、ポテトについてアソシエーション分析にて他の商品との関係性を 調べると、表5-4 のような値が計算できます。

表5-4 アソシエーション分析の例

商品A 商品B 支持度 信頼度 リフト値 |比較1 ハンバーガー ポテト 40% 57% 1.14 比較2 ドリンク ポテト 30% 50% 1.00

支持度を見ると、 比較1は比較2よりも高いことから、ハンバーガーを買う人 はドリンクよりもポテトを買う人が多いことがわかります。 また、 信頼度を見 ると、ハンバーガー購入者のほうがポテトを一緒に買う割合が少し高いことが わかります。

また、リフト値を見ると、 ハンバーガーを買う人はポテトを単独で買う人よ りもセットで買う人が多いことがわかります。 つまり、ハンバーガーとポテト は一緒に購入される傾向が強く、 ドリンクとポテトの間には特別な関連性は見 られないことがわかります。

つまり、マーケティングの面では、ハンバーガーとポテトのセット販売や割

-

第5章 ビジネス面での活用方法

210

引キャンペーンの実施が効果的だと考えられます。

このような方法をログにも適用することを考えます。 たとえば、 A、B、Cと いうページがあったときに、利用者がどのページにアクセスしているのかを集 計するのです。

たとえば、 あるWebサイトに100名が訪問し、表5-5 のページにアクセス した人数が集計されていたとします。

5-5 Web サイトでのデータの例

アクセスしたページ 会社情報 人数 70人 製品情報 50人 ニュース 60人 会社情報と製品情報 40人 会社情報とニュース 30人 製品情報とニュース 30人 会社情報 製品情報、 ニュース 20人

このとき、 製品情報のページについて他のページとの関係性を調べると、 5-6 のような値が計算できます。

5-6 アソシエーション分析の例

ページ A ページB 支持度 信頼度 リフト値 比較1 会社情報 製品情報 40% 57% 1.14 比較2 ニュース 製品情報 30% 50% 1.00

使っている数値は上記のハンバーガー店と同じなので、それぞれの数値につ いての解説は省略しますが、 あるページがどのページと関係してアクセスされ ているのかを把握できます。

211

- 1 アクセスログの解析

5

その他、 協調フィルタリング (ユーザーベース、 アイテムベース) やコンテ ンツベースフィルタリング3といった手法もクロスセルの分析に使われます。

■ アップセルとRFM分析

顧客が買おうとしている商品よりも少し高価であっても性能や価値が高いも のを勧めて、 グレードアップしてもらうことをアップセル (Upsell) といいま す。 たとえば、スマートフォンを買おうとしている顧客に、より性能のよいモ デルや大容量のストレージがついたモデルを提案することや、 飲食店で通常サ イズのドリンクを注文しようとしている顧客に、大容量サイズを勧めることな どが挙げられます。

このときも顧客単価が上がりますが、 それだけ価格が上がっても納得しても らえる、満足できる価値を感じられるような提案が求められます。

Webサイトであれば、 無料記事を読んだ顧客に対し、追加で有料のコンテン ツを提供する、ECサイトであればオプションの商品を追加で購入してもらう、 といったことが挙げられます。 これは同時のタイミングでなくても、あとから 追加で購入してもらうことでも問題ありません。

そこで、このようなアップセルを考えるとき、 RFM分析という手法がよく使 われます。 これは、 Recency (直近購入日からの経過日数)、 Frequency (購 入頻度)、 Monetary (購入金額合計) という3つの指標の頭文字を取った分析 手法で、 よく購入する人や金額の大きい人に対して再アクセスを促すキャンペ ーンを実施するなどの施策に使われます。

Recency は、顧客が最後に商品を購入した日からの経過日数を意味します。 最近購入した顧客は、その商品についての関心が高いことが想像できます。 逆 に、最後に購入してから時間が経っている顧客は、 再購入する可能性が下がっ ていると考えられます。 つまり、 購入されてから期間をあけずにDMなどを送

3 似た表記の言葉として「コンテンツフィルタリング」 があるが、これは青少年を有害な情報から守ること などを指し、まったく異なる意味になるため注意。

第5章 ビジネス面での活用方法

212 -

付することにより、 顧客にその商品を思い出させることを考えます。

Frequencyは、顧客がどれだけ頻繁に購入しているかを示す値です。 該当の 商品やサービスを何度も利用する顧客は、 商品やブランドに対してよい印象を 持っていることが想像できます。 こういった顧客に有効なサービスを提供でき れば、顧客満足度の向上や企業の収益に大きく貢献する可能性があります。 ま また、頻繁に購入する顧客は、同じブランドで提供される新製品や関連するサー ビスに興味を持っている可能性が高いものです。 積極的にDMなどを送付して ブランドの認知を高め、 再訪を促すことが有効だと考えられます。

Monetary は、購入金額が高い顧客のことで、高級な商品や大量購入する顧 客だといえます。 このような顧客は 「プレミアム顧客」として特別扱いしたり、 特別なオファーを提供したりすることが有効だといえます。

こういったRFM分析を実施するには、 顧客を特定し、その顧客がいつ、 何を、 どのくらい購入したのかを記録することが大切です。 そして、それぞれの顧客 に適したマーケティングが求められ、 次の手順で進められます。

1.売上データを収集する

2. Recency、Frequency、 Monetary を計算する

3. Recency、Frequency、 Monetary の各指標をもとに、顧客をセグメント 化する

4. セグメントごとに顧客の特徴を分析する

たとえば、Recency、 Frequency、 Monetary という3つの指標でデータを 収集し、それぞれの指標を 「1」から 「5」の5段階で表現することを考えま す。 つまり、 Recency の値が大きい (最近購入していない) 顧客は 「1点｣、 最 近購入した顧客は 「5点」のように、 点数で表現します。 これはFrequency や Monetary でも同じで、それぞれを5段階評価することで、セグメント化できま す(表5-7)。

5-1 アクセスログの解析

213

表5-7 RFM分析の分類例

ランク Recency Frequency Monetary 5 7日未満 20回以上 20万円以上 4 2週間未満 10回以上 10万円以上 3 1ヶ月未満 5回以上 5万円以上 2 半年未満 3回以上 3万円以上 1 半年以上 3回未満 3万円未満

そして、セグメントごとに分析し、 分析した結果に合わせてクーポンを送付 するなどのキャンペーンを実施します。

この方法をWebサイトのアクセスログに対しても同様に解析することを考え ます。 たとえば、 同じユーザーが最後にアクセスしてからどれだけ時間が経っ たか、アクセスの頻度はどれくらいか、アクセスした回数はどれくらいか、と いったことを集計します。 そして、登録されているメールアドレスなどにDM を配信する方法が考えられます。

このときは単純なアクセスログというよりも、 ログイン履歴や商品の購入履 歴、メールのクリック率などの指標を使うこともあります。

チャネル (流入元) やデバイスによるクラスタリング

あるWebサイトを利用者が閲覧するときに、 どのような経緯でたどりついた のかを表す言葉としてチャネル (流入元) があります。 わかりやすいチャネル としてGoogle 検索などの検索エンジンやSNSが挙げられます。 また、 他サイ トからのリンク、 メールマガジンや広告、チラシに記載したQRコードなども あるでしょう。

このようなチャネルごとのアクセス数やコンバージョンといった数値を分析 することで、 Web サイトに誘導する最適な集客方法を考えます。 さらに、 それ ぞれのチャネルから流入した利用者がどのように行動するかを把握することを

第 5 章 ビジネス面での活用方法

214

考えます。

流入元ごとのユーザー行動やコンバージョン率を比較するためには、どこか ら流入したのか、リンク元の情報を取得することから始めます。

検索エンジンからの流入が多ければ、SEOの効果が表れていると判断できる でしょう。 また、SNSのフォロワーが多いと、そのSNSからの流入が多くなり ます。

このとき、それぞれのチャネルごとにコンバージョン率などを比較し、 想定 した値より低いのであれば、コンテンツの質やターゲティングの見直しが必要 だと判断できます。

このようなチャネルによる行動の違いを解析するには、 UTM パラメータ解 析という方法が使われます。 このUTMはUrchin Tracking Module の略で、 Urchin software という会社が開発した分析プログラムです。 現在はGoogle に買収され、 Google Analytics として使われていますが、 Urchin の名前はパ ラメータとして現在も残っています。

ここでのパラメータは、URLに付加するタグのことで、流入元や媒体、 キャ ンペーン名などを識別するために使われます。 利用者がアクセスすると、その パラメータがURLとともにログに出力されるため、このパラメータを取得する ことにより、どのチャネルからの流入が多いのかを確認できます。 これにより、 単にアクセスログを分析するよりも効率よく精度の高い結果が得られます。 よく使われる UTM パラメータとして、表5-8 のようなものがあります。

表5-8 UTMパラメータの例

パラメータ名 設定する内容 utm_source 流入元の媒体やWebサイトの名前 utm_medium 流入元の種類(チャネル) utm_campaign キャンペーン名やプロモーション名 utm_term 広告に設定するキーワード utm_content 広告の種類、リンクの区別

1 アクセスログの解析

5

215 -

これをマーケティングに使うときは、 広告の掲載やメールの発信、 SNSの投 稿などに使う URLの末尾にUTMパラメータを付与することを忘れないように しなければなりません。 たとえば、 夏のセールキャンペーンであれば、 URLの 末尾に 「utm_campaign=summer_sale」 のような値を付与します。

https://www.example.com/index.html?utm_campaign=summer_sale

このようなUTMパラメータを設定したURLに利用者がアクセスしたとき、 Webサーバーからの応答はパラメータの値に関わらず 「index.html」 というペ ージの同じコンテンツを返します。 しかし、 Web サーバーのアクセスログには パラメータが記録され、 Web ブラウザ内での表示をこのパラメータに応じて制 御できます。

そして、Google Analytics などの解析ツールを使ってUTMパラメータの情 報を取得します。 そのうえで、 それぞれのパラメータごとに訪問数やコンバー ジョン数、 コンバージョン率、 滞在時間などを集計できます。

また、利用者がWebサイトを閲覧するときに使っている端末のことをデバ イス (Device) といいます。 パソコンやスマートフォン、タブレット端末など があり、それぞれにさまざまなOSがあります。 たとえば、 パソコンであれば Windows や macOS、 スマートフォンであればiOSやAndroid、 タブレット端 末であればiPadOS や ChromeOSなどが挙げられます。

Webサイトを閲覧している人が使用しているデバイスを分析することで、 Web サイトの構成を見直す必要性について考えられます。 たとえば、 スマートフォ ンを使ったアクセスが多いのであれば、 小さな画面でも見やすいような画像サ イズなどの設定が必要です。 また、 パソコンを使ったアクセスが多いのであれ ば、より細かい情報を表示することも考えられます。

利用者が使っているデバイスを分析するとき、 アクセスログに記録されるユ ーザーエージェントの文字列を解析する方法のほか、 画面サイズ (解像度) を 取得する方法があります。

このように、 アクセスログに記録されている情報を使って、 利用者をセグメ

-

第5章 ビジネス面での活用方法

216

ントで分けて分析すると、 どのような利用者がアクセスしているのか、 Web サ イト全体での傾向が見えてきます。

たとえば、 IPアドレスから利用者の地域をある程度把握できます。 これは、 IPアドレスがわかると接続しているプロバイダの情報を把握できるためです。 GPSのようにピンポイントで特定できるわけではありませんが、国や都道府県 といったレベルでは比較的高い精度で特定できます。

また、海外からアクセスしていても利用者のパソコンの設定が日本語になっ ている、といったこともわかるため、 翻訳機能の必要性を判断できることもあ ります。

このようにして得られた多くのデータを使って、 ページごとのアクセス数や コンバージョン、直帰率や離脱率などの情報をもとにページをグループに分け ることを考えます。

このようなグループ化の手法としてクラスタリングがあります。 データがど のくらい似ているか (類似度) を考え、似ているデータを集めることでグルー プ(クラスタ)を作る方法です。 このとき、「似ている」という判断としてさま ざまな方法がありますが、 わかりやすいのはデータを点として表現し、その間 の距離を使う方法でしょう (図5-4)。

図5-4 クラスタリング

クラスタリングの具体的な手法としてk-means法や階層型クラスタリングな どがあります。

51 アクセスログの解析

217

k-means法は最初にグループの数を決めておき、 それぞれのデータをランダ ムなグループに割り当てます。 そして、それぞれのグループに含まれるデータ から重心を計算し、 その重心との距離が小さいデータを集め、 新たなグループ を作ります。 これを繰り返すと、 少しずつ近くにあるデータでグループが構成 されていき、最終的にグループの割り当てが変化しなくなれば終了です。

たとえば、 アクセスログを集計して、 ページごとのページビューと直帰率が 表 5-9 のようにまとめられたとします。

表5-9 k-means 法で使う集計データ

ページ ページビュー 直率 ページA 800 50% ページB 1,200 30% ページ C 1,500 60% ページD 600 60% ページE 700 20% ページ F 1,800 50% ページG 1,100 40% ページH 400 40% ページ 1,300 50% ページ 1,700 70%

これを3つのグループに分けるように設定して処理することを考えます。 調べ る軸が1つであれば、単純にその大きさで比較すればいいのですが、 ここでは2 つの軸があります。 また、 軸によって値に大きな差があるため、 数値を標準化し てから処理することにします。 たとえば次のプログラムを実行すると、図5-5 のようなグループに色分けして表示されます。

第5章 ビジネス面での活用方法

218

import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans

data pd.DataFrame([ ]) [800, 50], [1200, 30], [1500, 60], [600, 60], [700, 20], [1800, 50], [1100, 40], [400, 40], [1300, 50], [1700, 70]

labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']

# データの標準化

scaler StandardScaler () data_std scaler.fit_transform(data)

#k-meansクラスタリング (クラスタ数は例として3に設定) kmeans KMeans (n_clusters-3, random_state=42) clusters kmeans. fit predict(data_std)

#結果をデータフレームに追加 data['Cluster'] = clusters

# 散布図でクラスタを色分けして表示 plt.figure(dpi=300) plt.scatter (data[0], data[1], c=clusters, cmap='viridis')

for i, label in enumerate(labels): plt.text(data.loc[i, 0] +30, data. loc[i, 1], label)

plt.xlabel('PV') plt.ylabel('Bounce Rate') plt.title('K-means Clustering') plt.show()

5-1 アクセスログの解析

5

219

図5-5 k-means 法

K-means Clustering 70 60- 50 40- 30- C 201 400 600 800 1000 1200 1400 1600 1800 PV

階層型クラスタリングはデータ間の距離を使って、 データとデータをつなぐ 樹形図のような図を作成する方法です。 できあがった図を、 任意の高さで切る ことで、グループに分けられます。

たとえば、 上記のk-means法と同じデータを階層型クラスタリングで分類す ると、 図5-6 のような図が描けました。

図5-6 階層型クラスタリング

4.0 3.5 3.0 2.5 2.0- 1.5- 1.0- 0.5- 0.0 J FCIH ADE B G

第5章 ビジネス面での活用方法

220

この高さが 「3.5」 くらいの位置で横に切ると、 「J, F, C, I」 「H, A, D, E. BG」 という2つのグループに分けられることがわかります。 また、 高さ 「2.0」 くら いの位置で横に切ると、 右側を 「H, A, D」 と 「E,B,G」 に分けられることが わかります。

このように、あとからグループの数を決められることが階層型クラスタリン グの特徴です。 なお、距離の設定として 「最短距離法」 や 「ウォード法」など さまざまな方法があり、この距離の設定によって階層が変わるため、 できあが るグループも変わります。

クロス集計とコレスポンデンス分析 (対応分析)

どういったユーザーがどのページを多く閲覧しているのかを確認したいこと もあります。 しかし、 アクセスログに記録されているのは、誰がどのページに アクセスしたのか、 という情報だけです。

そこで、 このアクセスログからページビューやユニークユーザーのような数 値として見るだけでなく、 複数の軸で分析したいことがあります。

このようなときには、まずは複数の軸で集計することが必要です。 アクセス ログは1レコードに1つのデータが記録されているため、このアクセスログを少 し加工して、表5-10 のようなデータに変換できたとします。

表5-10 アクセスログの加工データ

アクセス日時 年齡層 性別 訪問ページ 2025-04-01 10:00:00 10ft 男性 ページA 2025-04-01 11:00:00 20ft 女性 ページB 2025-04-01 12:00:00 30ft 男性 ページC 2025-04-01 13:00:00 20ft 男性 ページB 2025-04-01 14:00:00 10代 女性 ページC

-

5-1 アクセスログの解析

221

ここから、アクセスしたユーザーの年齢層と訪問ページの関係を調べようと 考えたとき、まずはこの2つの軸でクロス集計をします。 たとえば、 表5-11 の ように集計することが考えられます。

表5-11 クロス集計表

年齡層 ページA ページB ページ C ページD 10ft 70 35 57 81 20ft 65 45 42 67 30ft 58 54 31 55 40ft 47 35 28 40 50ft 68 40 17 35

このように集計したとき、 それぞれの軸はいずれもカテゴリデータです。 こ のようなカテゴリデータの関係性を視覚的に把握するための多変量解析の手法 としてコレスポンデンス分析 (対応分析) があります。

コレスポンデンス分析は項目間の関係を視覚的に表現する手法で、 散布図の ように2次元でマッピングします。 似ているものを近くに配置することで、関 係の強さを把握できます。

上記のような表に対して、 コレスポンデンス分析をするには、 Python や R と いったプログラミング言語が便利です。 ここでは、 Python で計算します。

import pandas as pd import numpy as np from scipy.linalg import svd import matplotlib.pyplot as plt

data= pd.DataFrame({

'PageA': [70, 65, 58, 47, 68], 'PageB': [35,45,54,35,40], 'PageC': [57,42,31, 28, 17],

第5章 ビジネス面での活用方法

222

'PageD' [81, 67, 55, 40, 35], }, index ['10-19', '20-29', '30-39', '40-49', '50-59'])

total data. values. sum() P r data total P. sum(axis=1).values.reshape(-1, 1) c = P. sum(axis=0).values.reshape(1, -1) E=r@c S (P E) np.sqrt(E) -

U, S, VT = svd (S)

Dr_inv_sqrt = np. diag(1 Dc inv_sqrt = np. diag(1 np.sqrt(r. flatten())) np.sqrt(c. flatten()))

# 第1,2主成分の行列座標 F = Dr_ inv_sqrt @ U[:, :2] s[:2] G = Dc inv_sqrt @ VT.T[:, :2] * s[:2]

plt.scatter (F[:, 0], F[:, 1], label='Rows', marker='0') for i, txt in enumerate(data.index): plt.annotate(txt, (F[i, 0], F[1, 1]))

plt.scatter (G[:, 0], G[:, 1], label='Columns', marker='for i, txt in enumerate(data.columns): plt. annotate (txt, (G[i, 0], G[i, 1]))

')

plt.title('Correspondence Analysis')

plt.xlabel('Dimension 1')

plt.ylabel('Dimension 2')

plt. legend()

plt.grid(True)

plt.show()

これを実行すると、 図5-7 のような結果が得られました。

5-1 アクセスログの解析

223

図5-7 コレスポンデンス分析の結果

Correspondence Analysis Dimension 2 Rows 0.10 Columns 0.05 10-19 0.00 PageC PageD 20-29 40-49 -0.05 -0.10 PageA 50-59 30-39 PageB -0.2 -0.1 0.0 0.1 0.2 0.3 Dimension 1

これを見ると、 10代や20代はページCやページDが近く、 30代はページBが 近いことがわかります。 また、 50代にはページAが近いことがわかります。 こ のように、 年代によってアクセスしているページの傾向を直感的に図から判断 できます。

その他、第3章の表3-2 で紹介した手法の中にも、 アクセスログの解析に使 えるものがありますので、ぜひ試してみてください。

第5章 ビジネス面での活用方法

224

5

パフォーマンスの最適化

役立つのはこんなとき

> Webサイトを閲覧するときの応答時間を知りたい

応答時間を改善するための手法を知りたい

Webサイトを運営していて、その応答にかかる時間が長くなると利用者が離 脱する可能性があります。 SEOの面でも、 応答時間が長いページは Google な どの検索エンジンの検索結果で上位に表示されないことがあります。

アクセス数が多かったり、 無駄な処理が多かったりするWebサーバーではそ れだけ負荷がかかり、 高性能なハードウェアが必要になるなど、コスト面でも 問題になるため、 改善することを考えます。

必要なデータを計測する

Web サイトの応答時間などのパフォーマンスを改善するためには、現状でど のくらいの時間がかかっているのかを把握することから始めます。 まずは、 必 要なデータの計測方法について解説します。

応答時間を取得する

Apache や Nginx といった一般的なWebサーバーが出力する標準的なアクセ スログには、アクセスした日時が出力されるだけで、 応答時間の項目は含まれ ていません。 そこで、 応答時間を取得することを考えます。

225

-

-2 パフォーマンスの最適化

手軽なのは、Webサーバーのログをカスタマイズする方法です。 Apache で あれば、ログフォーマットに 「%D」 という値を指定することで、マイクロ秒単 位のリクエスト処理時間を出力できます。 また、 Nginx であれば、 ログフォー マットに「$request_time」 と指定することで、 秒単位 (小数点を含む)のリク エスト処理時間を出力できます。

ここでは、 Nginx で応答時間をログの右端に含めるように設定してみます。 次 の設定ファイルでは 「main」 という名前を指定していますが、 この名前には好 きな値を設定できます。

/etc/nginx/nginx.conf

(略) http { log_format main '$remote_addr - [$time_local] "$request" '$status $body_bytes_sent "$http_referer " "$http_user_agent" $request_time'; access_log /var/log/nginx/access.log main; (略) } リクエスト処理時間を指定

このようにして記録した応答時間を使って分析するには、それぞれのアクセ スにおける応答時間をログから抽出し、 平均や中央値、 最大値などを算出しま す。そのうえで、 応答時間が長いリクエストを特定します。

場合によっては、特定の時間帯だけ応答速度が低下する可能性があるため、時 間帯別の応答速度の変動を調べる方法もあります。 また、 ページ単位に応答時 間を調べることで、特定のURLやリクエストタイプで問題が発生しているかを 把握できます。

その後、長時間かかっている処理がわかれば、その内訳を詳しく調査し、ポ トルネックとなっている部分を推定します。 たとえば、 Web サーバー アプリ ケーション、データベースなどのどこに問題があるのかがわかると、調査しや すくなります。

第5章 ビジネス面での活用方法

226

もしリバースプロキシや負荷分散装置 (ロードバランサー) を使っている場 合は、これらが出力するログに応答時間を含めることで、ネットワーク遅延も 含めた応答時間を記録できます。

Webアプリで計測する

上記の方法は静的なページや動的なページの両方に対応できますが、 静的な ページでは基本的に応答時間はほぼゼロに近いものです。 実際には動的なペー ジでの応答時間を知りたいことが多いでしょう。

そして、静的なページでなければ、 Webアプリケーション側で応答時間を計 測して出力する方法もあります。 リクエストがあった時点で開始時刻を記録し、 レスポンスを生成した時点で終了時刻を記録することで、その差分を計測でき ます。

たとえば、 Webアプリとしてプログラミング言語にPHPを採用したプログ ラムであれば、 次のようなソースコードが考えられます。

<?php

// 実行開始時刻を取得 $start_time = microtime(true); echo "実行開始時刻: date("Y-m-

// ここに処理を記述 (例:1秒待機) sleep(1);

d H:i:s", (int) $start_time). "\n";

// 実行終了時刻を取得

$end_time = microtime(true);

echo "実行終了時刻: ". date ("Y-m-d H:i:s", (int) send_time)."\n";

// 実行時間を計算 selapsed_time = $end_time - $start_time; - echo "実行時間:"number_format($elapsed_time, 6)"秒\n"; ?>

2 パフォーマンスの最適化

1

5

227

このページにアクセスすると、 開始時刻と終了時刻、 実行時間が表示されま す。 実際には、この実行時間をデータベースなどにログとして出力しておくと よいでしょう。 ただし、 これはあくまでもアプリケーション内での処理時間で あり、 サーバーレベルでの遅延は含まれていません。

APMツールで計測する

応答にかかった時間をより細かく計測したい場合は、 Prometheus、 Grafana、 New Relic Datadog、 Elastic APMなどのAPM ツール (アプリケーション パフォーマンス管理ツール) を導入する方法もあります。

ここでは、 Prometheus と Grafana を使った方法について解説します。 Prometheus はデータの収集・保存に特化しているため、見やすいグラフやダ ッシュボードの作成にはGrafana が使われます。 たとえば、 Prometheus で収 集したCPU使用率やメモリ使用量を Grafana でリアルタイムに可視化し、監視 やアラートの設定を行うのが一般的な運用スタイルです。

Prometheus と Grafana を Alma Linuxに導入するには、 まず Prometheus のユーザーを作成します。 このユーザーでログインすることはないため、 ここ ではホームディレクトリを作らず、ログインシェルを 「/bin/false」 にするこ とでログインを禁止しています。

# useradd --no-create-home --shell/bin/false prometheus

次に、 最新のPrometheus をダウンロードします。 ダウンロードページ4から 最新バージョンを確認し、 curlコマンドでダウンロードします。 ここでは、 バ ージョン3.4.2を使います。

4 https://prometheus.io/download/

-

第5章 ビジネス面での活用方法

228

# curl -OL https://github.com/prometheus/prometheus/releases/ download/v3.4.2/prometheus-3.4.2.1inux-amd64.tar.gz

そして、ダウンロードしたファイルを展開し、 「/usr/local/prometheus」 に 配置します。 また、 バイナリファイル (実行ファイル) を 「 /usr/local/bin」 に配置します。 具体的には、 次のコマンドを順に実行します。

# tar xvf prometheus-3.4.2.1inux-amd64.tar.gz #mvprometheus-3.4.2.1inux-amd64 /usr/local/prometheus #mv/usr/local/prometheus/prometheus /usr/local/bin/ #mv/usr/local/prometheus/promtool /usr/local/bin/

続いて、必要なディレクトリを作成し、上記で作成しておいたユーザーに権 限を設定します。

# mkdir /etc/prometheus

# mkdir /var/lib/prometheus #mv/usr/local/prometheus/prometheus.yml/etc/prometheus/ # chown -R prometheus: prometheus /etc/prometheus # chown -R prometheus: prometheus /var/lib/prometheus # chown prometheus: prometheus /usr/local/bin/prometheus # chown prometheus: prometheus /usr/local/bin/promtool

導入が完了したので、サービスとして起動します。 一般的には、systemdサ ービスに登録するため、 「/etc/systemd/system/prometheus.service」というフ ァイルを作成し、次の内容を記述します。

/etc/systemd/system/prometheus.service

[Unit]

Description Prometheus

wants=network-online.target

パフォーマンスの最適化

1

2

5

229

After=network-online.target

[Service] User prometheus Group-prometheus Type=simple ExecStart=/usr/local/bin/prometheus \ --config.file=/etc/prometheus/prometheus.yml \ --storage.tsdb.path=/var/lib/prometheus/ [Install] WantedBy=multi-user.target

そして、systemdのデーモンをリロードして設定ファイルを再読み込みした あと、 Prometheusを起動します。

# systemctl daemon-reload # systemctl start prometheus # systemctl enable prometheus

次のコマンドを実行し、 「Active」 欄に 「active (running)」 という表示が出 ればOKです。 「failed」 などと表示されている場合は、 設定を見直してくださ い。 SELinux が有効な環境では必要に応じて設定の変更が必要です。

#systemctl status prometheus

また、外部からアクセスする必要がある場合は、 ファイアウォールで9090番 ポートを開放します。

# firewall-cmd --permanent --add-port=9090/tcp # firewall-cmd --reload

第5章 ビジネス面での活用方法

230

これでWebブラウザから次のURLにアクセスし、 Prometheus の Web UI を確認します(図5-8)。

http://<サーバーのIPアドレス>9090

図5-8 Prometheusの画面

Prometheus Q Query Alerts 8 Status v Enter expression (press Shift+Enter for newlines Table Graph Explain No data queried yet Add query Execute

次に、 Alma Linux に Grafana を導入し、 Prometheus と連携することを考 えます。 Grafana を導入するには、ダウンロードページから最新バージョンを 確認し、 curl コマンドでダウンロードします。 ここでは、 バージョン12.0.2を 使います。

# curl -OL https://dl.grafana.com/oss/release/grafana-12.0.2-1. x86_64.rpm

そして、ダウンロードしたファイルを使って次のようにインストールし

ます。

# dnf install grafana - 12.0.2-1.x86_64.rpm-y

5 https://grafana.com/grafana/download?edition=oss

パフォーマンスの最適化

2

5

231

Grafana をサービスとして起動し、 自動起動の設定をしておきます。

# systemctl start grafana-server # systemctl enable grafana-server

必要に応じてファイアウォールを設定します。 Grafana のデフォルトポート は3000です。 外部からアクセスする場合はポートを開放します。

# firewall-cmd --permanent --add-port=3000/tcp # firewall-cmd --reload

そのうえで、 Webブラウザから Grafana の画面にアクセスします。

http://<サーバーのIPアドレス>:3000

ここで、ログイン画面が表示され、ユーザー名とパスワードの両方に 「admin」 を入力してログインします。 初回ログインにてパスワードを変更しておきまし ょう。ログインすると、図5-9 のような画面が表示されます。

図5-9 Grafana のトップ画面

Gratina Welcome to Grafana Need help? Basic The ship your Grafana Instalation TUTORIAL DATA SOURCE AND DASHBOARDE Grafana fundamentals and covers the usuce H DATA SOURCES Add your first data source

第 5 章 ビジネス面での活用方法

232

ログイン後、 Prometheus と接続するには、 データソースとして追加しま す。 Grafana の左メニューから 「Connections」 → 「Data Sources」 にある 「Prometheus」 を選択します。 そして、 「Add new data source」 を選択する と、 接続先のURL欄が表示されるので、ここにPrometheusのアドレスを入力 し、 「Save & Test」 をクリックして接続を確認できます。

そして、 Grafana の 「+」 メニューからダッシュボードを作成することで、 Prometheus の項目を使ったグラフを作成できます。 Grafana の公式サイトや コミュニティから既存のPrometheus用ダッシュボードをインポートすること もできます。

ここでCPUの使用率やメモリ使用量、 アプリケーションの応答時間などを追 加しておくと、 過去の推移なども把握でき、 必要なデータを手軽に計測できる でしょう。

パフォーマンスの改善策を実施する

パフォーマンスについてのデータを計測したあとは、単に記録しておくだけ でなく、 具体的な改善策を講じなければなりません。

キャッシュを導入する

Webサーバーやデータベースに多くのアクセスがあったときに、 その負荷を 軽減することで、レスポンス速度を向上させる手法としてキャッシュがありま す。 Web サイトを閲覧するときに使われるキャッシュには、次のように複数の 技術が考えられます。

• Web ブラウザのキャッシュ

・プロキシサーバーのキャッシュ

• CDN (Content Delivery Network)

etc.

-

52 パフォーマンスの最適化

233

利用者の環境やサーバーの設定によって、 これらのどれが使われるかは変わ りますが、 キャッシュの技術を使うことでアクセス頻度の多いページ、サーバ ーまでの距離が遠いページなどへのアクセスを高速化できます。

わかりやすいのがWebブラウザのキャッシュです。 Web サイトを閲覧した とき、Webブラウザはそのページのコンテンツを一時的に保存しています。 こ れにより、 同じページに再びアクセスしたときは再度取得する必要がなく、 高 速に表示できます。 たとえば、 同じWebサイト内の複数のページを閲覧すると き、その Web サイトのロゴなどの画像はどのページでも同じであるため、 キャ ッシュとして保存しておくと、 高速に表示できるだけでなく通信量を減らすこ とにもつながります。

実際にどのようなデータが保存されているのかは、 Web ブラウザの設定画面 から確認できます。 Chromeを使っている場合、 設定画面から 「プライバシー とセキュリティ」 を開いて、 図5-10 のように 「閲覧履歴データを削除」という メニューから表示できます。 ここから削除することで、キャッシュを削除する こともできます。

図5-10 Chromeのキャッシュ

設定 G Google の設定 自動入力とパスワード 8 プライバシーとセキュリティ O パフォーマンス デザイン Q 検索エンジン C 限定のブラウザ [起動時 Q 安全チェック 安全に関するごください。 パスワード プライバシーとセキュリティ データ Cookie. キャッシュなどします プライバシー ガイド プライバシーとセキを確認します

Webサイトの運営者としては、キャッシュとして保存される期間を指定で きます。たとえば、ブラウザのキャッシュを制御するためには、 「Expires」や 「Cache-Control」、 「Last-Modified」、 「ETag」 といったHTTPヘッダを指定 します。

第5章 ビジネス面での活用方法

234

COLUMN Webブラウザ側での改善

Webブラウザ側でのページの表示速度を改善するときには Web ブラウ ザでの表示時間を測定することから始めることが多いでしょう。 このときに よく使われるものとして Googleが提供するサービスやツール、 指標があり ます。

まずは 「PageSpeed Insights (PSI)」 で、 URL を入力することで、その Webページのパフォーマンスを解析した結果を表示できます。

また、 PSI でも使われる自動監査ツールとして 「Lighthouse」 があり、 Webページのスコアと改善策などを表示できます。

さらに、Webページの 「読み込み体験」 「インタラクティブ性」 「視覚の 「安定性」 を判断する指標として 「Core Web Vitals」 があります。

これらを使って表示にかかる時間をできるだけ短縮できるように、 HTML の記述を改善することも求められます。

企業や学校のような組織では、プロキシサーバーを使うように設定している ことが多いものです。 プロキシサーバーを経由してアクセスすることで、セキ ュリティ面のメリットもありますが、 組織内の利用者がアクセスしたページを キャッシュとして保存できます (図5-11)。 これにより、2回目以降のアクセ スはページの表示速度が速くなることが期待できます。

図5-11 プロキシサーバーのキャッシュ

組織 利用者 初回はWebサーバーから取得 Web サーバー プロキシ サーバー 2回目以降はキャッシュを使用

1

2 パフォーマンスの最適化

5

235

また、 CDN を使う Webサイトも増えています。 これは、 Web サーバーで配 信するファイルのコピーを利用者に近いところにあるサーバーに配置する方法 です。 たとえば、 海外のWebサーバーで配信しているデータに日本からアクセ スすると、海底ケーブルを経由して転送しなければなりません。

このとき、 日本側にあるCDNのサーバーにファイルを事前にコピーしてお けば、日本国内からのアクセスは高速になります。 また、 Web サーバー側にと っても、アクセス数が増えたときにWebサーバーにかかる負荷も少なくできま す(図5-12)。

図5-12 CDN

利用者

遠いと時間がかかる

Web サーバー

アクセスが集中する

第5章 ビジネス面での活用方法

利用者 CDN Web サーバー 事前にコピー 近いと時間がかからない 負荷が下がる

負荷分散とスケーラビリティ

急激なアクセス数の増加に対応するためには、 1台のサーバーではなく複数 台のサーバーで処理することが求められます。 このためには、負荷分散の技術 が不可欠です。

一般的には、負荷分散装置 (ロードバランサー)を用いて、 サーバー間のア クセスを分散させることで、 個々のサーバーへの負荷を均一にします。 1つの サーバーにアクセスが集中してダウンしてしまったり応答速度が低下してしま

-

236

ったりすることを防ぐために使われます ( 5-13)。

図5-13 負荷分散装置 利用者には1台の サーバーに見える 複数のWebサーバーで 負荷を分散 負荷分散装置

クラウド環境であれば、 「オートスケーリング」という技術により、分散させ るリソースを動的に拡張、 縮小できるため、 アクセス数に応じて必要な性能を 確保できます。

データベースの最適化

Webアプリケーションでは、データの保存先としてデータベースがよく使わ れます。 このとき、 データベースへの保存や読み込みなどの性能が低下すると、 Webアプリケーション全体の性能にも大きく影響します。

データベースの性能が低下する理由として、 データを格納するときに効率よ い構造として設計されていない、 インデックス (索引) が最適化されていない、 取得や更新のクエリが適切でない、 キャッシュが使われていない、といったこ とが挙げられます。

たとえば、データベースにおけるテーブルの設計においては、キーが正しく 設定されていない、 正規化のレベルが適切でないといったことがあります。 ま また、インデックスについても、必要なインデックスが設定されていない、設定

パフォーマンスの最適化

2

237

されているインデックスが再構成されていない、といったことが挙げられます。 また、RDBMS (リレーショナルデータベース管理システム) では信頼性は 高いものの、大量のデータを短時間に処理するときに限界があり、 NoSQLデ ータベースなどへの移行が必要なこともあります。

一般的には、 データ構造の見直しやインデックスの設定によって応答時間を 短くできる可能性があるため、 これらの工夫によってパフォーマンスを最適化 することが考えられます。

■ アプリケーションの最適化

Webアプリケーションの性能に問題がある場合、 データベース以外にそもそ ものプログラムに使われているアルゴリズムが遅いことが考えられます。 開発 した当初はデータ量が少ないために問題なくても、 運用を続ける中でデータ量 が増えると、そのデータを処理するのに適切なアルゴリズムが選択されていな いことが原因です。

このような場合は、アルゴリズムの改善によって大幅に処理時間を短縮できる 可能性があります。 また、 不要な処理を削除する、 非同期処理を導入する、マ ルチスレッドを活用する、 といった方法により、 待ち時間を短縮できる可能性 があります。

これはサーバー側での処理だけでなく、フロント側の表示においても、 画像 の最適化、 JavaScript やCSSの圧縮、 遅延読み込みなどによって、 利用者の体 感速度を変えられる可能性があります。

第5章 ビジネス面での活用方法

238 -

顧客の行動分析

5

3

役立つのはこんなとき

ログを使って顧客の行動を把握したい 情報を収集するときの課題を知りたい

リアルな店舗では、顧客が店内をどのように動くのか、そしてどのような視 点で商品を見ているのかがわかると、 商品の配置や見せ方などが大きく変わっ てきます。 これを実際の店舗やWebの世界でも、ログを使って分析することを 考えます。

顧客行動分析とは何か?

私たちがスーパーやデパート、コンビニなどの店舗に入るときのことを考え ると、どのように店内を歩き回り、どの商品を見て、どのくらいの時間その場 所に滞在するかは人それぞれです。 店舗側にとっては、 「顧客の動き」 や 「注目 している商品」を知りたいものです。 このような情報を集められると、店舗の 売上を増やせるだけでなく、 顧客の満足度を高めることにつながります。 商品 をどこに置くか、 どんな陳列方法が効果的かを考える資料として役立つためで す。

このような店内での顧客の行動をデータとして集めて解析し、その結果をも とに店舗の運営や商品配置を改善していくことを顧客行動分析といいます。た とえば、多くの人が集まっている通路に人気商品を置くことで、売上を伸ばす ことができるかもしれません。

3 顧客の行動分析

5

239

■ 顧客行動分析に必要なログ

顧客の行動を分析するためには、その行動を記録する必要があります。 たと えば、「このお客様は入口から入って、 右の通路を通り、5分間その棚の前にい 「た」といった情報をデータとして記録します。 このようなデータを集めると、 多 くの顧客の動きを統計的に分析できるのです。 一般に 「行動ログ」 や 「行動デ ータ」、 「行動ログデータ」といい、 マーケティングだけでなく危険防止対策な どにも用いられます。

このときに記録しておくべき内容として、 どの場所を通ったのか (動線)、 ど のくらいの時間その場所にとどまったか (滞留時間)、 どの商品を手に取ったか、 どの商品が売れたかなどが考えられます。 これらの情報を組み合わせると、「ど こを通ったか」 だけでなく、 「なぜその場所に長くいたのか」 「その商品はなぜ 売れたのか」といった検証ができます。

コログデータを集めるための技術

難しいのが、このようなデータを集めるための技術をどうやって選択するか です。 物理的な場所の制約だけでなく、 費用対効果も考慮し、 店舗に合った技 術を選ばなければなりません。

一般的によく使われるのが 「監視カメラ」 です。 店舗の天井や壁に監視カメ ラを設置し、 店内の様子を映像として撮影します。 この映像を管理者が見るだ けでなく、 AIによって映像に記録されている人の動きを自動的に認識し、追跡 できます。 これを 「映像解析」 といい、 どの通路を通ったか、 どの棚の前で立 ち止まったかなどをデータ化できます。

また、センサーもよく使われます。 赤外線センサーを設置することで、人が 通ったときに通過人数をカウントしたり、 特定の場所に人がいるかどうかを検 知したりできます。 最近はスマートフォンのBluetooth や Wi-Fiの信号を利用 して、顧客の位置を推定する方法もあります。 これらは第1章でも紹介した「ピ ーコン」 や 「Wi-Fi トラッキング」 と呼ばれ、 スマートフォンを持っている顧

第5章 ビジネス面での活用方法

240

客の動きを把握できます。

さらに、商品棚に取り付ける 「圧力センサー」 もあります。 棚の商品の下に センサーを設置し、 商品が手に取られたときに重さの変化を感知して記録する 方法です。 これにより、 商品が手に取られた回数や時間を把握でき、商品の注 目度を測定できます。

より詳しく 「どこを見ているのか」 を把握したい場合は 「視線追跡 (アイ・ トラッキング)」 を使う方法があります。 これは、専用のカメラやウェアラブル デバイスを使って、 利用者の目の動きを測定し、 どの商品や棚に視線が向けら れているかを分析する方法です。

なお、 視線追跡についてはリアルな店舗だけでなく、パソコンの画面を見つ める利用者の視線を追跡するために使われることもあります。 これにより、 商 商品への興味や注目度を調べます。

ログデータを使った分析の具体的な例

集めたデータをログとして活用するときには、それをどのように使うのかを 考慮して選択しなければなりません。 たとえば、 監視カメラとセンサーを使っ て顧客の動線を分析したところ、 特定の通路が非常に混雑していることがわか ったとします。 その通路のレイアウトを通りやすく変えることで、滞留時間が 増え、売上も伸びる可能性があるでしょう。

また、視線追跡のデータを使って、 ある商品の陳列位置を変えたら、 より多 くのお客さんがその商品を見て手に取るようになったというケースもあります。 これは、視線の動きを分析することで、 どの位置に商品を置くのがもっとも効 果的かを科学的に判断できるからです。

さらに、圧力センサーのデータを使うと、 商品が手に取られる回数と実際の 購入数の差を把握でき、 商品選びの改善やプロモーションの効果検証に役立て られます。

このように、顧客行動分析を使うと、 店舗を運営するときに便利な情報を統 計的に処理できます。 商品の配置を工夫したり、 店内の動線を改善したりする

241

3 顧客の行動分析

1

5

ことで、売上を増やしたり、お客さんの満足度を向上したりできる可能性があ るのです。 このとき、 感覚や経験だけに頼らずにデータを用いて判断すること で、根拠を持って説明できます。

顧客の行動分析における課題

行動分析によって有用な情報が得られる一方で、 プライバシーの確保につい ての課題があります。 情報の収集面での課題と分析面での課題について解説し ます。

情報の収集における課題

監視カメラや位置情報を使う場合は、 利用者のプライバシーに配慮しなければ なりません。 日本では個人情報保護法があり、 EUでもGDPRがあります。 こ のように、顧客に関する情報の扱いには厳しいルールが定められています。 映 像や位置情報を収集するときは、 必要に応じて利用者に同意を得る必要がある でしょう。

また、データを分析するときにも、匿名化や仮名化といった技術により、個 人を特定できないように工夫しなければなりません。

これは Web サイトでの情報収集でも同じです。 アクセスログにはIPアドレ スやWeb ブラウザの情報、 閲覧元のページの情報などが含まれることがありま す。 IPアドレスから取得できる情報はある程度の地域までは絞り込むことはで きますが、 精度が高いとはいえません。 また、 Web ブラウザの情報がわかって も個人を特定することはできません。 このため、 Web サーバーのアクセスログ に含まれている情報を分析する程度であれば、 利用者の同意を得る必要はない でしょう。

しかし、 Google Analyticsのようなツールを導入するときには、 Cookie を

6 General Data Protection Regulationの略で、 EUにおける個人情報保護のルールを定めた法規制。

第5章 ビジネス面での活用方法

242

使ってユーザーの行動を追跡するため、 同意を得る画面を表示している Web サ イトが多いものです。 また、 広告の表示にもCookieが使われることが多く、複 数のWebサイトをまたがって関連した広告を表示されることで、利用者として は追跡されていると感じることも多いものです。

そこで、最近ではサードパーティ Cookie を拒否するような仕組みがWeb ブ ラウザに搭載されるようになっています。 つまり、 Web サーバー側で情報を収 集しようと思っても、 プライバシーに関連する項目は少しずつ取得できなくな ってきています。

そのほかにも問題になるのがリファラーで、 閲覧元のページの情報です。 た とえばGoogle検索などの検索エンジンで検索して表示された一覧から訪問する と、その検索エンジンに入力した検索キーワードを取得できることもあり、便 利に使われてきました (図5-14)。

図5-14 閲覧元で入力したキーワードがわかる

検索サイト キーワード |検索結果 1 キーワードにヒットした内容 検索結果 2 キーワードにヒットした内容 管理しているサイト アクセス元の情報が 記録される (キーワードを含む)

このような情報が収集されることについてプライバシーの面などで問題があ ります。 このリファラーを送信しているのはWebブラウザであり、 Chrome な どのWebブラウザでは標準の設定でドメインが異なるサイト(クロスオリジ ン) の場合にはリファラーの情報を送信しなくなりました。 これにより、 現在 の多くのWebアクセスにおいて、 閲覧先のページでは閲覧元のページの情報を 収集できなくなっています。

5-3 顧客の行動分析

243

情報の分析における課題

アクセスログを分析するとき、 Web サーバーにて保存されるアクセスログを 使うことが基本でした。

しかし、 第1章でも解説したように、Webサイトに埋め込んだJavaScript の プログラムで記録された情報を Google Analytics などのサーバーに送信して分 析することが一般的になっています。

こうなると、 特定のサービスに依存してしまいます。 つまり、 Google Analytics を使わないとデータを分析できず、 Googleのサービス変更や停止、 仕様変更により影響を受けるリスクがあります。 また、 カスタマイズに限界が あり柔軟な分析ができないといったデメリットもあります。

さらに、分析するときに難しいのは、ログとして記録されているのはあくまで も 「行動履歴」 の一部だということです。 すべての行動が記録されているわけ ではありませんので、 その記録と記録の間の関係は得られません。 そして、ロ グとして残っているものはあくまでも表に出ているもので、 その行動をしてい る人が持つ意図や感情はわかりません。

COLUMN ボットによるアクセスを除外する

Webサイトへのアクセスを分析したい場合、 アクセスしてくるのは人間 だけではありません。 たとえば、 検索エンジンがページを収集するときに使 われる「ボット」 と呼ばれるプログラムがアクセスしている可能性があるの です。

このため、 アクセスログに含まれる内容から不要なログを除外するなど、 分析したい内容に応じてデータの質を確保するための工夫が必要です。

第5章 ビジネス面での活用方法

244

第

AIによるログ分析

章

ここまで、 ログの解析は人間が手動で実施してきました。 よく知 られた攻撃パターンであれば、 人間が特徴を理解し、 閾値を設定 するなどの方法で検知できます。 しかし、 新しい攻撃手法が次か ら次へと開発されており、これまでのようなパターンとは一致し ないことがあります。 そこで、この章では、 AIを使ってログを分 析する手法について解説します。

機械学習にて未知の

攻撃パターンを検出する

役立つのはこんなとき

機械学習をログ解析に使うための基礎知識を知りたい 具体的な機械学習の手法を知りたい

サイバー攻撃の手法は常に進化しているため、 従来の知識だけでは検知する ことすら難しくなっています。 そこで、 未知の手法で攻撃されたときにも検知、 対処するためにAIの手法である機械学習を活用した異常検知が注目されていま す。

機械学習の基礎知識

未知の手法で攻撃されたとき、 人間がルールを設定したり、ログを眺めたり して検知することは困難です。 そこで、 機械学習を使って攻撃の発生を検出す ることが考えられます。 このとき、大量のログや通信データから 「正常な状態 の特徴」 を学習し、 それと異なる 「異常なパターン」 を自動的に検出する方法 が考えられます。

機械学習の種類

機械学習は人間がルールを教えなくても、 データを与えることでコンピュー タが自動的にルールを学習する手法です。 このとき、 「教師あり学習」 「教師な し学習」 「強化学習」 という3つの手法に分類できます。

第6章AIによるログ分析

246

教師あり学習

入力されたデータに対する正解の出力が与えられる学習手法です。 正解が与 えられるので、 この正解データに近い出力が得られるように学習を進めます。

ログの場合は、 過去のデータをもとに 「正常」 「異常」といったラベルがつい たデータを与え、それぞれについての特徴をAIが学習することで、新しいデー タが与えられたときにどちらかに分類されるかを判定することが考えられます。

教師なし学習

入力データしか与えられず、このデータのみを使って学習する手法です。 正 解は与えられませんが、 似たデータを集めることでグループに分けることはで きます。

ログであれば大量のログを入力すると、それを似たもので集めていくつかの グループを作ります。 そして、 新たなデータが与えられたときに、そのいずれ のグループに該当するのかを判断する手法です。 第5章で解説したクラスタリ ングもこの教師なし学習の1つです。

強化学習

エージェントと呼ばれるプログラムを用意し、 エージェントがなんらかの行 動をしてよい結果が得られた場合に報酬を与えることで、その報酬を最大化す るような行動をするように学習させる方法です。

上記の3つの手法を、 ログでの異常検知に使うことを考えると、わかりやす いのは教師あり学習でしょう。 ログにエラーコードなどが出力されていれば異 常データだと判断できるため、 教師あり学習で学習させる方法が使われること もあります。

しかし、ログの内容によっては異常データが圧倒的に少なく、正常か異常かの 判断が難しいことも多いでしょう。 また、 未知の手法による攻撃であれば、ど れが異常なのかを判断することはできません。 このため、 教師なし学習が使わ

247

未知の攻撃パターンを検出する

れることもあります。

ログは次から次へと発生するため、 その内容を時系列で与え続けることもで きます。 このような特徴を考えると、 強化学習も向いているように感じます。 そこで、この章ではそれぞれの手法について、 具体的な内容を解説していき ます。

機械学習のステップ

機械学習でログを処理するときには、図6-1 のような手順で進めることが考 えられます。

図6-1 機械学習の進め方

課題の明確化 運用 ・実際のログへの適用 ・モデルの再学習 モデルの評価 ・精度の計算 パラメータの調整 ・必要なデータの収集 モデルの選択 適切なモデルの選定 モデルの学習 分析手法の選定 データの加工 項目の抽出 ・集計、 整形 特徴量の抽出 ・単語の出現頻度 TF-IDF データの分割 頻度、時間的な特徴 学習データとテストデータ 交差検証の準備

まずは、どのような課題を解決するのかを明確にします。 たとえば、 異常検 知なのか、ログの分類なのかによって必要なデータや分析方法は変わってくる ためです。 そして、やりたいことや目的が決まれば、その課題を解決するため に必要なデータを集めるのですが、 ログの分析では基本的にはテキスト形式の

第6章 AIによるログ分析

248

データを使います。

問題なのは、ログは構造化されていない形式で記録されていることが多いため、 データの加工が必要なことです。 特に、必要な項目を抽出するとともに、 正規 表現やログパーサーを使って、日時やログのレベル、ステータスコードなどの 項目を抽出するとともに、 分析できるように集計することが考えられます。

ある程度の加工ができれば、 分析するための特徴量を抽出します。 たとえば、 単語の出現頻度やTF-IDF 、 ログが出力されている頻度や時間的特徴などを整 理します。

= MEMO TF-IDF

文書内に登場する単語の頻度に注目し、 重要な単語とそうでない単語を 判断する手法です。 TFはTerm Frequency の略で、 ある単語が文書内にど れくらい頻繁に出現するかを表す値です。 一方のIDF は Inverse Document Frequencyの略で、 その単語が文書全体でどれくらい珍しいかを表す値です。

このTFとDFの値を使って、 特定の文書に多く出現し、他の文書にはあ まり出ない単語を評価できます。

ここまでできれば、 機械学習として処理するために、 データを学習用やテス ト用に分割します。 場合によっては、 学習用と検証用、 テスト用という3つに 分けて、 パラメータの調整に使うこともあります。 どのくらいの割合に分ける のかは、交差検証と呼ばれる検証方法などによっても変わります。

MEMO 交差検証

与えられたデータを複数のグループに分けて、 学習用とテスト用のグル ープを組み替えながら学習させる手法です。 特定のデータに特化したモデ ルができあがる過学習を防ぐ目的で使われます (図2)。

機械学習にて未知の攻撃パターンを検出する

6

249

図6-2 交差検証

1回目 学習データ 2回目 学習データ 3回目 学習データ 4回目 テストデータ 学習データ 学習データ テストデータ 学習データ 学習データ テストデータ 学習データ 学習データ テストデータ 学習データ 学習データ 学習データ 評価 評価 評価 評価

これでデータが整えられれば、 実際にモデルを選択し、 学習を始めます。 モ デルとして、 ランダムフォレストやSVM、 ニューラルネットワークなどがあり ます。

モデルの学習がある程度進めば、 できたモデルを評価します。 このとき、 求 める精度が出ているか、混同行列などを活用し、 各種の指標を計算します。 さ らに、ハイパーパラメータなどを調整してチューニングします。

MEMO 混同行列

作成したモデルを評価するために、 モデルから得られた結果を表形式で 集計したものです。 たとえば、 スパムメールの分類(迷惑メールかどうか) のように、 いずれかのカテゴリに分類する問題を考えると、 実際の分類と 予測した分類を行と列で集計します。 この表を見ることで、それぞれに分 類されたメールの数から、 どのくらいの精度で分類できているかを確認で きます (表61 )。

表6-1 混同行列

予測した分類 迷惑メール 通常のメール 迷惑メール 20 5 実際の分類 通常のメール 15 60

第6章 AIによるログ分析

250

モデルが完成したら、 実際のログに適用して結果を監視します。 また、 運用 を始めると想定した精度が得られない可能性があるため、 新しいデータでモデ ルを再学習させます。

実際には1度のモデル作成でよい結果が得られることは少ないため、 新たな データの収集を含め、上記のステップを繰り返して改善を続けることになりま す。

ログの前処理

上記で解説したステップのうち、 手作業での工程が多いのがデータの加工や 特徴量の抽出です。 AIというと華やかな世界のイメージがあるかもしれません が、実際に分析するときにはデータを地道に加工し、 パラメータを調整するこ とが重要です。

つまり、 ログを分析するときには、ログをそのままAIに入力することはでき ず、 効率よく処理できるように手作業での加工が必要なのです。 ログには日時、 数値、メッセージなどさまざまな形式の情報が混在しており、 そのままだとAI が中身を理解しにくいからです。

まず、データを分析しやすいように加工したり、AIのモデルが処理するとき の精度を上げたりするための準備作業を前処理といいます。 たとえば、表6-2 のような作業が挙げられます。

表6-2 前処理で使われる作業の例

作業內容 概要 欠損値の処理 一部が欠けているときに補完する、 またはデータを削除する ノイズの除去 不要なデータや不正確なデータを除去する 形式の統一 日時のデータ形式の統一、 大文字や小文字などの統一 カテゴリ の統一などデータを変換する

1

機械学習にて未知の攻撃パターンを検出する

1

251 -

第5章でも紹介したように、 Apacheのログは標準では次のような値が出力さ れていました。

203.0.113.1 [01/Jul/2025:12:34:56 +0900] "GET /test.html HTTP/1.1" 200 219 "http://example.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.5 Safari/605.1.15"

このように、ログはテキストデータとして作成、 保存され、 1行に1つのイベ ントが記録されています。 これをデータ分析やAIに使うときは、 集計などの処 理がしやすいように数値データに変換する方法があります。 このような数値デ ータを特徴量といいます。

また、ログからAIが学習しやすい特徴量を作り出すことは特徴量エンジニア リングとも呼ばれます。 一般に、 数値化したデータの種類として、6-3の4 種類があります。

表6-3 数値データの種類

種類 概要 分類するために数字を割り当てたもの 名義尺度 例) 性別 (1: 男性、 2: 女性) 順序尺度 尺度 血液型 (1:A型、 2:B型、 3:O型、 4:AB型) など 順序に意味はあるが、間隔に意味はないもの 例) 評価 (5: とても良い、 4: 良い、 3: 普通、 2: 悪い、 1: とても悪い) 順位 震度など 目盛が等間隔なもの 例) 気温 (摂氏) カレンダーの日付など 間隔にも比率にも意味があるもの 比例尺度 例) 身長、体重、 年齢、 金額など

名義尺度はあくまでも分類のために数値を割り当てたものなので、このデー タに対して平均を計算するような処理はできません。 たとえば、 血液型を数値

第6章AIによるログ分析

252

化して平均を計算し、 「2.3」 という値が得られたとしても使い道がありません。 順序尺度であれば、 平均などを計算できると考えるかもしれません。 実際、 書 籍や飲食店などの評価を星の数で表現するときには平均のような値を表記して いるWebサイトはたくさんあります。 しかし、「5」 と 「4」の間隔と、「4」と 「3」の間隔は等しいとはいえず、単純な平均は適切とはいえません。

間隔尺度や比例尺度であれば平均を計算しても問題ありません。 もちろん、名 義尺度や順序尺度でも単に個数を集計して出現頻度を調べるくらいであれば問 題ありません。

このように、データの種類に合わせて、 ログに含まれる情報をうまく数値化 することで、 AIの性能が大きく変わります。

そこで、まずはログのデータを数値に変換する処理を実施します。 たとえば、 日時のデータであれば、そのまま使うのではなく、 時間帯や曜日など集計しやす い形式に変換します。 また、 ログに登場する単語の出現頻度を計算したり、ロ グのレベルに数字を割り当てたりします。

ここでは、 Python のライブラリである pandasを使ってログファイルを読み 込み、日時やメッセージに分けることを考えます。このとき、日時情報を「時間 「帯」や「曜日」に変え、ログレベルをカテゴリ変数として扱います。また、単 語の出現頻度や意味を数値に変換するプログラムを作成します。

import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn. feature_extraction.text import CountVectorizer from sklearn. feature_extraction. text import TfidfVectorizer

#ログの読み込み (実際にはファイルから読み込むが、ここではlogs = [ ] "2025-06-15 12:34:56 INFO User 123 logged in", "2025-06-18 09:01:45 ERROR Failed to load resource" "2025-06-20 13:50:21 INFO User 456 logged out", "2025-06-22 18:42:33 WARN Disk space low", "2025-06-25 01:51:18 ERROR Failed to log in", "2025-07-01 08:18:09 INFO User 789 logged in"

リストで解説

253

機械学習にて未知の攻撃パターンを検出する

6

#ログをDataFrameに整形する

data = [] for log in logs: parts log.split('', 4) #日時、 レベル、メッセージに分割 ' timestamp parts[0] + + parts[1] level = parts[2] message parts[4] data.append([timestamp, level, message])

df = pd.DataFrame(data, columns=[ ]) 'timestamp', 'level', 'message'

#日時の形式を統一し、 日時情報から特徴量を作る df['timestamp'] = pd.to_datetime (df['timestamp']) df['hour] =df['timestamp'].dt.hour df['minute'] = df['timestamp'].dt.minute df ['second'] = df['timestamp'].dt.second df['weekday'] = df['timestamp'].dt.weekday

#ログレベルを数値に変換 le LabelEncoder() df['level_encoded'] = le.fit_transform (df[ 'level'])

#メッセージの部分を数値化する vectorizer Count Vectorizer() X_text vectorizer.fit_transform(df['message'])

#ログレベルを数値に変換 df['level_code'] =df['level'].map({ }) 'INFO': 0, 'ERROR': 1, 'WARN': 2 print("L-F:\n", df['level_code

'])

#ログメッセージのテキスト処理と特徴量化 (TF-IDF) vectorizer TfidfVectorizer (stop_words='english') X_text vectorizer.fit_transform(df['message']) print("\n特徴量 (メッセージのベクトル) 形状: \n", X_text)

# 数値特徴量とテキスト特徴量を結合

第6章 AIによるログ分析

254

X_numeric df[[ 'hour', 'minute', 'second', 'weekday', 'level encoded' ]].values print("\n\n", X_numeric)

これを実行すると、 次のような内容が出力されます。

ログレベルコード:

00

1

0

2

3

1

0

Name: level_code, dtype: int64

特徴量 (メッセージのベクトル) 形状:

<Compressed Sparse Row sparse matrix of dtype 'float64'

with 11 stored elements and Coords (0, 0) Values 0.8221903715494888 (0, 5) 0.5692126078464125 (1, 3) 0.7071067811865476 (1,7) 0.7071067811865476 (2,5) 0.5692126078464125 (2, 1) 0.8221903715494888 (3, 8) 0.7071067811865476 (3, 6) 0.7071067811865476 (4, 4) 1.0 (5, 5) 0.5692126078464125 (5, 2) 0.8221903715494888

shape (6, 9)>

特徴量の

数値部分: [[12 34 56 6 1] [ 9 1 45 20]

1

2

4

5

6

1 機械学習にて未知の攻撃パターンを検出する

255

[13 50 21

[18 42 33

4 1]

6 2]

[1 51 18

[8 18 9 1

נני

ここでは、ログレベルを数値化し、 メッセージをTF-IDFでベクトル形式に 変換しています。 さらに、 数値として扱える部分 (時、分、秒、曜日など)を まとめて扱っています。 このように変換することで、 AIは与えられたログを数 値データとして扱えることがわかります。

時系列データとして集計し、 異常を検知する

ログにはタイムスタンプのような時刻に関する情報が記録されているため、こ れを使って、 時系列解析をすることを考えます。 たとえば、 異常を検知すると きは、直近のログに出力されている内容が普段のログとどのように違うかを調 べる方法が考えられます。

たとえば、 単純な方法として、 ある時間帯に急激にログの件数が増えている と、異常が発生していると考えることもできます。 そこで、 時間帯別にログの 件数を集計することを考えます。

す。 ここでも、 Python のライブラリである pandasを使うと、 手軽に集計できま

df ['timestamp'] = pd.to_datetime (df['timestamp']) df ['hour'] = df ['timestamp ']. dt. hour

#時間帯ごとのログ件数を集計

log_counts = df.groupby('hour').size()

print(log_counts)

今回はデータの件数が少ないので、出力されているログの時間帯はいずれも

第6章AIによるログ分析

20]

256

異なりますが、 大量のデータを与えると、どの時間帯のログが多いのかを容易 に把握できます。

代表的なモデルとアルゴリズム

AI でログを分析するときは、その目的に応じてモデルやアルゴリズムを選ば なければなりません。 ログの分析でよく使われる機械学習のモデルとその特徴、 簡単な実装例を紹介します。

代表的な機械学習のモデル

単に件数を集計するだけであれば、前節のような方法で特徴量を調べて集計 するだけで十分ですが、もう少し高度な処理をすることを考えます。 たとえば、 前処理によってメッセージのテキストをベクトルとして処理しておいたものを 扱って、そのメッセージの内容やログレベルから異常の発生などを判定するこ とを考えます。

機械学習を使ってログを解析するときには、 表6-4 のようなモデルが使われ ることが多いものです。

表6-4 ログに使われることが多いモデルの例

機械学習モデル ランダムフォレスト 主な用途 分類、回帰 Isolation Forest 異常檢知 サポートベクターマシン (SVM) 分類 k-近傍法 (k-NN) クラスタリング、 分類 長短期記憶ネットワーク (LSTM) 時系列解析、異常検知 BERT など テキスト解析

-

機械学習にて未知の攻撃パターンを検出する

257

以下では、分類にランダムフォレスト、 異常検知に Isolation Forest、 時系 列データとしてLSTN、 テキスト解析としてBERT を使った方法について解説 します。

ランダムフォレストで分類する

伝統的なAIの手法として決定木があります (図6-3 )。 デシジョンツリーと も呼ばれ、 木構造の分岐に条件を設定し、その条件を満たすかどうかを判定し て問題を解く手法で、 分類や回帰 (予測) に使われます。

図6-3 決定木の例

会員/非会員 会員 20人 非会員: 30人 性別=男性 性別 女性 会員 15人 会員 5人 非会員 15人 非会員 15人 年齢 <30歳 年齢30歳 年齢 <30歳 年齢 230歳 会員: 15人 非会員: 5人 会員:0人 非会員: 10人 会員:0人 非会員: 15人 会員 5人 非会員:0人

決定木はできた木構造の分岐を見るだけで、 その条件や結果を理解、 解釈し やすいことがメリットの1つです。 また、 間隔尺度や比例尺度のような量的デ ータだけでなく、 名義尺度や順序尺度のような質的データも扱えることが特徴 です。

そして、この決定木を単独で使うのではなく、 複数の決定木を使った手法と してランダムフォレストがあります (図6-4 )。 与えられたデータをいくつか に分割し、それぞれで学習させて決定木を作成したあと、 その決定木の結果を 多数決などで統合することで、 全体の予測精度を向上させます。

-

第6章AIによるログ分析

258

図6-4 ランダムフォレスト

データ 多数決 予測結果

ここでは、このランダムフォレストを使って、 異常なログが出力されたかど うかを、メッセージの内容から自動で判定するモデルを作成します。 前節の最 後で作成したデータをもとに、 「ERROR」 というログレベルに該当するログかど うかを分類するプログラムは次のように実装できます。

from sklearn.ensemble import RandomForestClassifier import numpy as np

# 特徴量としてメッセージベクトルとログレベルコードを結合 X = np.hstack([X_text.toarray() df [ ["level_code']].values) y = (df ['level'] == 'ERROR').astype (int) #1=異常,0=正常

model = Random ForestClassifier() model.fit(X,y)

#新しいログの異常判定例

new_log = ["Failed to connect to database ] new_vec = vectorizer.transform (new_log).toarray ( ) new_level_code = np.array([[1]]) # ERROR レベルと仮定 new_X = np.hstack ([new_vec, new_level_code])

pred= model.predict (new_X) print("異常判定結果:", "異常" if pred[0] == else "正常")

機械学習にて未知の攻撃パターンを検出する

1

259

今回は 「Failed to connect to database」 というメッセージが与えられたと きに、これが異常かどうかを判定しています。 実際に学習用のデータを変えな がら、 どのような結果が出力されるのかを試してみてください。

また、ラベルつきのログを使った例として、 次のようなプログラムも考えら れます。 この方法は高い精度が得られるものの、 正常か異常かのラベルづけが 必要です。

from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report

#データを訓練用とテスト用に分割 X_train, X_test, y_train, y_test = train_test_split(x,y, test_ size=0.3, random_state=42)

#モデル作成と学習 model = RandomForestClassifier(n_estimators=100, random_state=42) model.fit(X_train, y_train)

#予測と評価 y_pred= model.predict(X_test) print(classification_report(y_test, y_pred))

この例では、ログの特徴量を使って異常かどうかを分類しています。 実際に は、大量のデータに対する複雑な特徴量を扱いますが、 基本的な流れは同じで す。

教師なしデータに対する Isolation Forest

ラベルがない場合は、 教師なし学習を使います。 代表的なアルゴリズムとし て、 Isolation Forestがあります。 これもランダムフォレストと同様に決定木 を使った手法ですが、 異常なデータは他より孤立しやすい、 という考え方に基 づいています。

第6章 AIによるログ分析

260

たとえば、 異常な値の割合として0.1を想定したとき、 つまりデータの10% 程度が異常であると想定したとき、 異常か正常かを予測するには次のようなプ ログラムが考えられます。

from sklearn. ensemble import IsolationForest

model = IsolationForest (contamination=0.1, random_state=42)

model.fit(X)

#異常か正常かを予測 (-1が異常、1が正常)

pred= model.predict (X)

print(pred)

上記の6件のデータであれば、 次のように出力されました。 つまり、4件目の 「WARN」 というログレベルのデータだけが他のデータとは違った特徴を持つこと がわかります。

[11 1-1 1 1]

この方法は正解データのラベルづけが不要なため、大規模なデータでも人の 作業が少なくて済み、 ログの異常検知などで使うには便利です。

LSTMで時系列データの異常を検知する

ログは時間の流れに沿って記録されるため、 時系列解析が得意なモデルを 使う方法もあります。 よく使われる方法として、 RNN (Recurrent Neural Network) があります。 日本語では再帰型ニューラルネットワークとも呼ばれ、 記録しておいた過去の情報を使って、 次に起こりそうな出来事を予測する方法 です。

また、このRNNの改良版としてLSTM (Long Short-Term Memory) があ ります。 これは、 長期記憶と短期記憶を組み合わせて、 情報を取捨選択してお

機械学習にて未知の攻撃パターンを検出する

261

くことで、長期的な依存関係を学習できる方法です。

このように過去のデータを活用できることから、 時系列解析に使われること が多く、ログでの異常パターンの検出にも使えます。

ここでは、サンプルとして10ステップの時系列データに対し、 異常 (1) か 正常 (0) かを表すラベルを使って、 LSTMモデルを構築しています。 実際の ログでは、どのような特徴量を使うのかを考え、 データを準備することが必要 です。

import numpy as np

import keras

#仮の時系列データを1000件用意 X = np.random.rand(1000, 10,5) #10タイムステップ 5特徴量 y = np.random.randint(0, 2, 10 #0か1のラベル

# LSTMモデルの構築 model = keras.Sequential() model.add(keras.layers.Input(shape=(10,5))) model.add(keras.layers.LSTM(32)) model.add(keras.layers.Dense(1, activation='sigmoid

model.compile(optimizer='adam', loss='binary_crossentropy",

metrics = ['accuracy'])

'))

model.fit(X,y, epochs=5, batch_size=32)

そして、このモデルを使って、 次のように新しいデータが与えられたときに、 そのラベルを予測できます。

#新しい時系列データ (例:1件分) X_new = np.random.rand(1, 10,5)

# 予測実行 y_pred_prob = model.predict(X_new)

第6章 AIによるログ分析

262

# 2値分類なので、しきい値 0.5でクラスを決定

y_pred_class = (y_pred_prob > 0.5).astype (int)

print("予測確率:", y_pred_prob)

print("予測クラス:", y_pred_class)

このようにラベルを使う方法だけでなく、時間の流れを考慮して、これまで と違う傾向が出たときに異常と判断する方法もあります。 一般に、入力と同じ データを復元するようなモデルとしてオートエンコーダがあり、正常なデータ であれば入力したデータとほぼ同じ結果が得られます (6-5)。

図6-5 オートエンコーダ

入力よりも少ない 情報で表現できる 入力 出力

入力と出力を同じものにする

一方、このオートエンコーダに異常なデータを入れると、正常なデータとは 異なり、 誤差が大きなデータが出力されます。 この誤差を 「再構成誤差」 とい います。

このオートエンコーダの手法をLSTMに導入したものがLSTMオートエンコ ーダで、正常な時系列データのパターンを学習して、 再構成誤差が大きいパタ ーンが出力されたときに異常と判断します。

機械学習にて未知の攻撃パターンを検出する

263

この方法であれば、 事前にラベルを付与する必要はありません。 これを実際 に実装すると、次のようなプログラムが考えられます。

import numpy as np import keras

#仮の時系列データ (サンプル数, タイムステップ, 特徴量数) X_train = np.random.rand(1000, 10,5) X_test = np.random.rand(100,10,5)

timesteps = X_train.shape[1] features = X_train.shape[2]

# モデル構築

inputs = keras.layers.Input(shape=(timesteps, features)) encoded = keras. layers.LSTM(32)(inputs) decoded = keras.layers.Repeat Vector (timesteps) (encoded) decoded = keras.layers.LSTM (features, return_sequences=True) (decoded)

autoencoder = keras.models. Model (inputs, decoded) autoencoder.compile (optimizer='adam', loss='mse') autoencoder.fit(X_train, X_train, epochs=10, batch_size=32)

#再構成誤差を計算し異常検知

X_pred = autoencoder.predict(X_test)

mse = np.mean(np.power (X_test

X_pred, 2), axis=(1,2))

threshold = np.percentile (mse, 95) #上位5%を異常とする閾値

anomalies=mse > threshold

print("異常検知結果:", anomalies)

この方法であれば、 時系列の複雑なパターンも捉えられるため、ログの異常 を検知するときに便利です。 いずれにしても、どのような状態を異常だと判定 するのか、 閾値を調整する作業は必須です。

-

第6章AIによるログ分析

264

BERTを使ったログの意味解析

ログに含まれる日付やステータスコードであれば決まった値なので名義尺度 や順序尺度などと考えて容易に数値化できますが、難しいのはメッセージ部分 です。 エラーメッセージなどが含まれることが多く、 日本語や英語などの自然 言語が出力されることもあります。

こういったメッセージを処理するには、BERT 1 などの言語モデルを使うこと が一般的です。 BERTは 「トランスフォーマー」というモデルに分類され、文 章を一方向に処理するのではなく、 双方向に処理して学習することが特徴です。 Python では、transformers というライブラリを使ってモデルを作成できます。

たとえば、次のようなプログラムでは、 「Failed to load resource」 という メッセージからベクトル形式の特徴量を出力できます。

from transformers import BertTokenizer, BertModel import torch

tokenizer = BertTokenizer.from_pretrained('bert-basemodel = BertModel.from_pretrained('bert-base-uncased

-uncased')

log_message = "Failed to load resource" inputs = tokenizer (log_message, return_tensors='pt') outputs model (**inputs)

')

cls_vector outputs.last_hidden_state[:, 0, :] print(cls_vector.shape) # (1,768)

つまり、この 「cls_vector」 というベクトル形式の特徴量を使って、 ログの 分類やクラスタリングができます。

1 Bidirectional Encoder Representations from Transformers の略。

265

機械学習にて未知の攻撃パターンを検出する

6

2

障害予測と根本原因分析への 応用

役立つのはこんなとき

機械学習を使って障害を予測する方法を知りたい 原因を分析するときに何が必要なのかを知りたい

ログを分析する目的として異常検知以外にも、 「障害予測」 や 「根本原因分 析」があります。 AIを活用することで、 これらの作業を効率よく進められ、高 い精度の結果が得られる可能性があります。

障害予測とは?

システムを運用していると、 さまざまな原因で障害が発生することがありま す。 もちろん、停電や災害など、 予期できないシステム障害もありますが、 あ る程度は予測できる場合もあります。

障害予測の簡単な例 (LSTMによる二値分類)

過去のログなどの情報をもとに、 将来の障害発生を予測することを障害予測 といいます。 システムが故障する前にその兆候を検出できれば、 事前にメンテ ナンスを実施する、 予備の部品を用意するなどの対応ができ、 システムのダウ ンタイムを減らせます。

よくある障害として、ストレージの容量不足、 メモリの空き容量不足、 CPU の性能不足、 アクセス集中などがあります。 そして、これらはログを監視して

第6章 AIによるログ分析

266

いると、それまでの時間的な経過からある程度予測できる可能性があります。 難しいのは、測定すべき項目が多すぎて、どのような条件を満たしたときに 障害が発生するのか、 人間が把握することが難しいことです。 そこで、AIを活 用して分析することを考えます。

ここでは、ログから抽出した特徴量を使い、 一定期間内に障害が起きるかど うかを予測することを考えます。 基本的な考え方は、前節で解説したLSTMに よる異常検知と同様です。

import numpy as np import keras from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report

# 仮の時系列データを1000件用意 X = np.random.rand(1000, 2010 # 20タイムステップ、 10特徴量 y = np.random.randint(0, 2, 1000) #0=障害なし、 1=障害あり

# データ分割 X_train, X_test, y_train, y_test = train_test_split(X,y, test_ size=0.3, random_state=42)

#モデル構築 model = keras. Sequential() model.add(keras. layers.Input(shape=(2010))) model.add(keras.layers.LSTM (64)) model.add(keras.layers.Dense(1, activation='sigmoid

'))

model.compile(optimizer = 'adam', loss='binary_crossentropy metrics = ['accuracy']) model.fit(X_train, y_train, epochs=10, batch_size=32)

#予測と評価

y_pred= (model.predict (X_test) 0.5).astype (int)

print(classification_report (y_test, y_pred))

この例では、20ステップ分のログのデータをランダムに用意し、 30%をテス

267

2障害予測と根本原因分析への応用

6

トデータとして分けて学習させ、 作成したモデルから障害発生の有無を予測し ています。

■ 障害が起きたかどうかのラベルづけ

上記の方法で手軽に障害予測ができるように思えるかもしれませんが、 実際 には 「障害が起きたかどうか」 のラベルを付与しなければなりません。 障害が 起きたとき、エラーメッセージは出ていたとしても、それが障害なのかどうか は人間による判断が必要で、 ログの記録に対して運用上の記録を連携させる必 要があるのです。

この部分はどうしても手作業になってしまうことが多く、 運用上の負担にな ることは珍しくありません。 エラーメッセージやログだけでは障害かどうかの 判定が難しく、 運用担当者の判断が必要になるためです。 また、 ラベルづけの 品質がモデルの性能に直結するため、正確かつ効率的な方法が求められます。

ログと運用上の記録を連携するために、 Jira や Service Now、 Redmine など のITSM2ツールを使うことがあります。 また、 エラーメッセージを正規表現で 抽出したり、 Fluentd や Elastic Stackなどのログ解析ツールで検索したりして、 半自動的にラベルづけすることもあります。

完全な自動化は難しいものの、 便利なツールを使うことで半自動化するなど、 運用の負担を軽減する取り組みは今後も続きそうです。 生成AIの普及もあり、 今後はこういった部分の自動化も注目されています。

根本原因分析とは?

障害の発生を予測できたとしても、完全に障害を防ぐことはできません。 そし て、実際に障害が発生したときには、 同じ原因で障害が発生しないように、 再

2 IT Service Managementの略で、ITに関するサービスを利用者に安定して提供するために、継続して管 理すること

第6章 AIによるログ分析

268

発防止策を実施しなければなりません。

特徴量の重要度分析

障害が発生したあとにその原因を特定することを根本原因分析といいます。 た とえば、ストレージの容量不足であれば、 何か大量にデータを出力している処 理が原因かもしれません。 また、 メモリの空き容量不足であれば、 なんらかの プログラムでメモリリークが発生しているのかもしれません。

そこで、このような障害が発生したときに、 その原因を細かく調べることを 考えます。 このときも障害予測と同様に測定すべき項目が多く、 人間が原因を 把握するのは大変です。

そこで、ログとして出力された内容から異常の発生を判断し、 原因を推定す るときにAIを使うことを考えます。

障害を予測するときには、 どの特徴量が障害に強く影響しているかを調べる ことから始めます。 ここではランダムフォレストを使って、 特徴量の重要度を 推測します。

from sklearn. ensemble import RandomForestClassifier import matplotlib.pyplot as plt.

#20種類の特徴量を持つデータをサンプルとして作成 X = np.random.rand(1000, 20) y = np.random.randint(0, 5, 1000) model = RandomForestClassifier(n_estimators=100, random_statemodel.fit(x, y)

=42)

importances = model.feature_importances_ indices = np.argsort (importances) [::-1]

#上位10特徴量をプロット plt.figure (figsize= (10,6)) plt.title("Feature Importances") plt.bar(range(10), importances [indices][:10], align='center')

障害予測と根本原因分析への応用

269

plt.xticks(range(10), indices[:10])

plt.show()

実行すると、図6-6 ようなグラフが出力されました。 ここではランダムなデ ータを使っているため、 それぞれにあまり差はありませんが、 実際には大きな 差が出ることがほとんどでしょう。

図6-6 ランダムフォレストによる分類結果

Feature Importances

0.05 0.04 0.03 0.02 0.01 0.00 7 11 9 3 14 5 1 18 17 0

このように、 重要度の高い特徴量から順に表示できると、 障害の原因となり やすいログのパターンやイベントを特定できます。

特徴量の設計

上記のように、 特徴量に応じた重要度が求められたとして、 難しいのは障害 に関連する可能性のあるイベントなどをどうやって抽出するのか、という問題 です。

第6章 AIによるログ分析

270

ここまでに解説してきたように、 データの前処理や特徴量エンジニアリング によって、 TF-IDF や Word2Vecなどの手法を使う、 平均や最大値、最小値、 変 動などを調べる方法もあります。

機械学習によってある程度のよい結果が得られたとしても、 その理由がわか らないと、 実務では使えません。 そこで、 そのモデルを説明できるように、 モ デルが予測に使った根拠を明示する必要があります。

その中で知っておきたいのがSHAPやLIMEを使う方法です。 SHAPは 「SHapley Additive exPlanations」 の略で、 モデルの予測結果に対して、それ ぞれの特徴量がどれくらい影響を与えたかを数値化する方法です。

たとえば、障害が発生することを予測したいとき、 「CPUの温度が高い」 「フ ァンの音が大きい」 「エラーメッセージが出ている」 など、 さまざまな情報が与 えられます。 この中で、 「壊れそう」 という予測に対し、 それぞれの情報がどの くらいの影響を与えているのかを調べる方法です。

これにより、 ある予測で 「CPU使用率」 が高いという情報が障害発生の予測 に大きく影響していれば、その特徴に対してSHAPとして大きな正の値が割り 当てられます。 逆に、 ある特徴量が予測にほとんど影響していなければ、 小さ い値になります。

もう1つのLIME は 「Local Interpretable Model-agnostic Explanations」 の略で、 モデルの 「ある1つの予測だけ」を簡単なモデルで近似し、その予測 に影響する特徴量を説明する方法です。

たとえば、あるログを使って障害が予測されたとき、 その予測に似たデータ を作り、単純なモデルで 「CPU 使用率が高いから障害の可能性が高い」という ように、わかりやすい内容で説明します。

このように、 複雑な原因が絡み合っている中でも、 どれか1つに注目して説 明してくれることで、 人間としては直感的に理解しやすくなります。

SHAPもLIMEも複雑な機械学習モデルの予測を人間が理解しやすくするた めに便利ですが、 使う目的が異なります。 SHAPは特徴量の全体的な影響を公 平に評価するのに対し、 LIMEは個別の予測に焦点を当てて、 直感的に説明し てくれます。 目的に応じて使い分けることが必要です。

2害予測と根本原因分析への応用

6

271

行動分析とパターン認識

3

役立つのはこんなとき

利用者の行動を把握するための手法を知りたい パターンとして捉えるための手法を知りたい

ECサイトで顧客がどのような遷移でWebサイト内を進んだのか、 攻撃者が どのような手順で不正アクセスを実施したのか、といったことを検出するには、 どのような操作をしたのか、その履歴をたどる必要があります。 このとき、AI を使うとログに記録されている利用者の操作履歴から効率よく分析できます。

利用者の行動を把握する

Webサイトにアクセスした多くの利用者の中から、 異常な行動を取った利用 者を特定するときは、一般的な行動とその他の行動の違いを把握しなければな りません。 そこで、 ログを使って分類することを考えます。

セッションを把握する

ログデータから利用者の操作や利用傾向を抽出し、 理解することを行動分析 といいます。 たとえば、 ECサイトでの購入行動、 Web サービスでのページ遷 移、アプリの利用頻度などを分析します。 これにより、利用者が何を求めてい るのかを把握できるだけでなく、 既存のサービスを改善し、 不正を検知するこ とにつなげられる可能性があります。

-

第6章 AIによるログ分析

272

利用者の行動を1つのまとまりとして扱うとき、第5章でも解説した「セッシ ョン」 という単位で区切って分析する方法がわかりやすいでしょう。

ログイン時に発行されるセッションIDなどを使って、ログアウトするまでの 間を1つのセッションとすることもできますが、 ログアウトしてからもなんら かの操作が続くかもしれません。

そこで、ログのデータを整形したものから、 Python を使ってセッションを 識別することを考えます。 ここでは、30分以上ログに記録されていない、つま り何も操作されていない場合は、別のセッションとして区切ることにします。 たとえば、次のようなプログラムが考えられます。

import pandas as pd

#サンプルログデータ (ユーザーID、タイムスタンプ、操作内容) data = { 'user_id': [1, 1, 1, 2, 2, 11, 'timestamp': [ '2025-06-15 10:00:00', '2025-06-15 10:05:00', '2025-06-15 11:30:00', '2025-06-15 10:10:00', '2025-06-15 10:20:00', ], '2025-06-15 11:40:00' 'action': ['login' 'view', 'purchase', 'login', 'view', 'logout'] } df = pd.DataFrame(data) df['timestamp'] = pd.to_datetime (df['timestamp'])

#セッションの区切り (例:30分以上あいたら新セッション) df = df.sort_values(['user_id', 'timestamp']) df['prev_time'] = df.groupby('user_id')['timestamp'].shift (1) df['diff'] = (df ['timestamp'] df['prev_time']) dt.total_seconds(div(60)

df['new_session']= (df ['diff'] > 0) (df['diff'].isnull()) df ['session_id'] = df.groupby('user_id')['new_session'].cumsum ( )

).

6-3 行動分析とパターン認識

273

print(df[['user_id', 'timestamp', 'action', 'session_id']])

これを実行すると次のような内容が出力され、データを並べ替えるとともに、 同じユーザーID であっても30分以上経過したものについては、 新しいセッシ ョンIDを付与していることがわかります。

user_id timestamp action session_id 1 2 1 2025-06-15 10:00:00 1 2025-06-15 10:05:00 12025-06-15 11:30:00 login. 1 view purchase 2 5 1 2025-06-15 11:40:00 logout 2 3 2 2025-06-15 10:10:00 login 1 4 2 2025-06-15 10:20:00 view 1

このようなデータを使うことで、ログをセッションとして捉えられます。

パターン認識のための特徴量作成

セッションで区切ったあとは、 それぞれのセッションごとに、 どのような行 動を取っているのか、 その特徴を数値化します。 たとえば、 行動を数値化する ときは、表6-5 のような値を求めることが考えられます。

表6-5 特徴量の例

特徵量 取得方法 セッション内の操作回数 特定のアクションの頻度 セッション時間 ユーザー単位でのログの件数で集計 ある操作をセッション内で実施した回数 セッションの開始から終了までの時間

そして、この特徴量を使って、 複数のセッションのデータをグループに分け ることができれば、 何か異常な行動を取っているセッションを識別できる可能 性があります。 つまり、他のセッションでの行動と大きく違う行動を取ってい

第6章 AIによるログ分析

274

るものを識別できるのです。

クラスタリングによる行動パターンの抽出

グループに分けるときは、セッションをクラスタリングすることを考えます。 代表的な手法として、第5章でも解説したk-means 法や階層型クラスタリング があります。

ここでは、k-means法でクラスタリングします。そうすると次のようなプロ グラムが考えられます。

from sklearn.cluster import KMeans import numpy as np

#例としてセッション特徴量の作成 (仮のデータ) X = np.array([ [5,1,30], # 操作回数5回、 購入1回、30分セッション [3, 0, 10], [10, 2, 60], [4, 0, 20] ])

kmeans = KMeans (n_clusters=2, random_state) labels = kmeans.fit_predict(X)

print("クラスタリング結果:", labels)

これを実行すると、 次のような結果が表示されます。

クラスタリング結果 [000]

この場合、 3つ目のデータが異常であることがわかります。 これにより、 般的な利用者の行動パターンと、不正な利用者の行動パターンなどを分けられ る可能性があります。 -

3行動分析とパターン認識

I

275

4

AIログ分析の自動化

役立つのはこんなとき

ログの分析を自動的に実行する方法を知りたい

AIでログを分析するときの課題を知りたい

AI を使ってログを分析するときは、単にモデルを作って終わるのではなく、 実務で運用を続けなければなりません。 特に、 手作業でログを監視するのでは なく、自動的に動かし続けて、 リアルタイムで監視することが求められます。

ログ分析自動化の目的

自動的に動かす目的は、大量のログをリアルタイムで監視し、 異常や障害が 発生したときに速やかに検知し、 対応できる体制を作ることです。 また、 手動 で分析をしていると担当者への負担が大きくなり、 属人化してしまう可能性が あります。

Python での自動化例: ログの定期分析

ログの分析を自動化する理由として、新しいログを取り込み続けることで、モ デルを更新することが挙げられます。 古いモデルを使っていると、高い精度で 動作を続けることは難しく、 結果として誤検知が発生してしまいます。

そこで、ログを定期的に読み込んで、 簡単な異常検知モデルで分析し、 結果 を通知する処理を作成してみます。

第6章AIによるログ分析

276

import time import pandas as pd from sklearn.ensemble import IsolationForest

def load_log(file_path): #簡単なログ読み込み例 (CSV形式想定) return pd.read_csv(file_path)

def preprocess(df): #必要な前処理(例:数値化) df['level_code'] = df['level'].map({'INFO': 0, 'WARN': 1, 'ERROR':2}) 例 return df[['level_code', 'value']] # 'value' (

def analyze (df): model Isolation Forest (contamination-0.05, random_state=model.fit(df) preds = model.predict(df) anomalies df[preds == -1] return anomalies

42)

def alert(anomalies): if not anomalies.empty: print("異常検知! 以下のログを確認してください:") print (anomalies) else: print ( " 異常なし”)

def main loop(log_path, interval_sec=60):

while True:

df load_log(log_path)

df_pre preprocess (df)

anomalies analyze (df_pre)

alert(anomalies)

time.sleep(interval_sec)

#実行例(実際はファイルパスを指定) #main loop('system_log.csv', interval_sec=300)

AIログ分析の自動化

277

このコードは指定した間隔でログを読み込み、 異常があったときにはそれを 検知して警告メッセージとしてコンソールに出力します。 実際の運用では管理 者にメールを送信したり、 ダッシュボードに統計的なデータを集計して出力し たりするような連携も考えられます。

cronの設定

自動的に実行することを考えたとき、 上記のようにプログラムをループさせ る方法のほか、定期的に起動するように設定する方法が使われます。 定期的に 起動する場合、 UNIX系のOSであればcron Windows系のOSであればタス クスケジューラを使うことが一般的です。

ここでは、 cron を使った設定を紹介します。 cron では、 cron デーモンとい うプログラムが「/etc/crontab」 というファイルや 「crontab -e」 というコマ ンドで作成されたファイルを1分ごとにチェックしており、 このファイルに書 かれている条件と一致したときに、 処理が実行されます。

この「crontab-e」 を使う場合は、 実行するタイミングについての条件と、実 行したいコマンドを 図 6-7 のように並べて指定します。

図6-7 crontab-e」 での指定方法 * * * * * Is 分 時 日 月 曜日 実行したいコマンド

この「*」はすべてに該当することを意味し、図6-7 のように指定すると、 1 分ごとに 「1s」というコマンドを実行することを意味します。

また、毎時0分に 「1s」 というコマンドを実行するときは、次のように書き ます。

0 ★★★★ 1s

一章 AIによるログ分析

6

第

278

同様に、毎日10時30分に 「ls」 というコマンドを実行するときは、次のよう に書きます。

3010 * * * 15

これによって、 上記で作成したようなプログラムを自動的に実行し、 異常が あれば通知できます。

AIログ分析の課題と対策

AI を使ってログを分析する方法は、 人間では気づかないような異常に気づけ るなどのメリットがありますが、 実際にはそれほど容易に高い精度が得られる わけではありません。 具体的にどのような課題があるのか、 そしてその対策と してできることについて解説します。

データの偏り (バイアス)

AI を使ってログを学習させようと考えたとき、すぐに発生する問題が、 ログ のデータに偏りがあることです。 一般的なシステムでは、ログに出力される内 容は正常なものがほとんどです。 つまり、 異常が発生したときのログはそれほ ど多くありません (異常が頻繁に発生しているのであれば、システムの根本的 な修正が必要なため)。

このため、ログとして出力される内容を学習させても、あまりよい結果が得 られません。 特に、 管理者など特定の利用者による操作だけが過剰に含まれた り、特定の状況でしか発生しないエラーだけが収集されたりすると、そのデー タをAIが学習したときに、 誤検知が頻繁に発生します。

これを防ぐためには、データの分布を考慮し、偏りを把握しておく必要があ ります。 そのうえで、アンダーサンプリングやオーバーサンプリングといった 方法が使われます。

-

6

AIログ分析の自動化

4

279

アンダーサンプリングは、与えられたデータの中で特定のクラスに分類され るデータが圧倒的に多い場合に、一部のデータを取り除くことで全体のバラン スを取る方法です。 つまり、 全体で1000件のログがあり、 そのうち正常データ が950件、異常データが50件であると大きな差があります。 このため、正常デ ータの一部を削除し、 正常データを200件、 異常データを50件のように調整す ることで、ある程度バランスが取れて、 判定の精度が向上する可能性がありま す。

逆に、 オーバーサンプリングは特定のクラスに分類されるデータが少ないと きに、少数派のデータを増やすことで全体のバランスを取る方法です。 簡単に 言えば、 データの水増しをする方法で、 既存のデータを使って、少数派のデー タに近い値を作成します。

また、クロスバリデーション (交差検証) を用いてモデルの汎化性能を評価 する方法もよく使われます。

過学習 (オーバーフィッティング)

AIのモデルが学習データに対してのみ過剰に適合し、 新しいデータに対して は精度が低下してしまう現象を過学習といいます。 データにノイズが含まれて いたり、モデルが複雑すぎたりすることが原因で発生します。

過学習を防ぐには、 学習データの量を増やす方法のほか、 正則化や早期終了 (Early Stopping) などの手法がよく使われます。

正則化は、モデルの複雑さを表す指標を追加した損失関数を最小化するよう に学習させる方法です。 たとえば、 モデルがy=a1x1+α2x2のように与えられ たとき、一般に損失関数として次のような式が使われます。

L= (a1x11+a2x12))² f-1

これは、予測値 41Xa+α2x1zに対して実測値y, が得られたとき、その誤差を 合計するものです。 ここで、や42に極端な値が設定されてしまうことを防ぎ たいものです。

第6章AIによるログ分析

280

この正則化には、L1正則化やL2正則化といった手法がよく使われます。 L1 正則化(ラッソ回帰) は次のように正則化項を追加します。

L=

(y- (x+α2xc2))+入(a1+a2 D

また、 L2正則化 (リッジ回帰) では、次のような正則化項を追加します。

L=2(-4xn+azxc2))+入(a1+a2)

f=1

このような正則化項を追加した損失関数を使うことで、 複雑なモデルであれ ば、追加した正則化項の値が大きくなるため、 損失関数の値が小さくならず過 学習を防げるという考え方です。

また、早期終了とは、 学習を続ける中で学習データでは精度が向上しても、検 証データで精度が向上しなくなるような段階で学習を打ち切る方法です。

説明可能性の問題

ニューラルネットワークをはじめとしたAIのモデルでは、高い精度が得られ たとしても、なぜそのような結果が得られるのかわからない状況が発生します。 特にディープラーニングのようにパラメータ数が多くなると、 人間にとって理 解が難しいブラックボックスになってしまいます。

しかし、ログを使って異常を検知したり、 障害を予測したりするときには、そ の原因を取引先などに説明することが求められます。 このとき、 「AIが通知し たから」という説明ではなく、根拠を持って説明できる必要があります。

そこで最近では、「説明可能AI」 という言葉が使われるように、 人間が解釈で きるモデルが求められています。 具体的には、 決定木やランダムフォレストの ような説明可能性の高いモデルを使うほか、上記でも解説したSHAPやLIME といった手法を活用する方法が挙げられます。

64-ログの自動化

281

あとがき

ログというテーマは、一見すると地味な印象を受けるかもしれま せん。 しかし、実際にはシステムの安定運用やトラブル解決、 さら にはサービスの改善に欠かすことのできない重要な役割を担ってい ることを、感じていただけたのではないでしょうか。

本書では、ログの基本的な仕組みや種類、 設計や運用のポイント、 そして実際の活用方法まで、幅広く解説してきました。 ログの世界 は非常に奥が深く、 知識を得ただけで運用できるものではありませ ん。 実際にシステムから出力されるログを見て、 適切に管理し、 活 使用していくことを考える過程で、さらに多くの気づきが得られます。 ぜひ、 本 書で得た知識を土台にして、 実際のシステムや仕事の現場でログに触れ、 経験 を積んでいただければと思います。

「はじめに」 でも触れたように、ログは単なる記録ではなく、システムが語 る「日記」や「声」 のようなものです。 その声を聞き逃さず、正しく読み解く 力を身につけることが、 トラブル対応のスピードアップやサービスの質の向上 につながります。 ログを活用することで、問題を早期に発見し、迅速に対処で きるようになるだけでなく、 ユーザー体験の向上やシステムの効率化にも貢献 できるのです。

もちろん、技術の進歩に伴い、 ログの扱い方も日々変化しています。 クラウ ド環境でのログ管理や、 従来のテキスト形式にとどまらない多様なログ形式の 登場、そしてAIを活用した高度なログ分析技術など、新しい技術が次々と生ま れています。 こうした最新の技術や手法を継続的に学び続けることで、ログ分 析の効率と精度は飛躍的に向上し、システムの価値をさらに高めることができ るでしょう。

普段はあまり意識しないログでも、頭の片隅に置いておくことで、よりよい システムの開発や運用につながるはずです。 本書が皆さまにとって、ログの重 要性を再認識し、 実際の業務でログを最大限に活かすきっかけとなれば幸いで す。 これからもログの世界に興味を持ち続け、 技術を深めていっていただける と嬉しいです。

282

INDEX 数字 2段階認證 184 2要素認證 184 3-2-1ルール 113 Ajax A-H A/Bテスト ACID 特性 Adobe Analytics Aggregation Alma Linux Apache APM ツール 207 40 30 154 60 64 57 228 Average Session Duration 205 BERT CDN Chain ChatGPT. Cookie 265 59, 182, 233 189 166 242 Core Web Vitals cron Datadog DBMS 235 278 228 40 DLP dmesg Elastic APM 28 48 228 Elastic Stack 83, 137 Elasticsearch 137, 154 ELK Stack 83 EUC-JP 17 Fluentd 77 GDPR 242 Google Analytics 30, 58, 201 Grafana 110, 144, 228 Graylog 95 grep 124, 155 grok 86

1 ICMP I-P 94 IDS 45 IPA 46 IPS 45 iptables. 189 Isolation Forest 260 ISO 8601 16 IT資産管理ツール 72 ITAM 72 ITOM 72 ITSM 72 Journal. 50 k-means 217 Kibana 141 L1正則化 281 L2正則化 281 LANSCOPE 72 Lighthouse 235 LIME 271 Logstash. 83 LSTM 261 LSTM オートエンコーダ 263 macOS 51 MongoDB 95 MyLogStar. 72 n-gram 135 Nagios 103 Nagios Agent. 109 NCSA combined. 38 New Relic 228 nginx. 57, 226 NIST 46 NRPE 109 NTP 175 OpenSearch 95 pandas 253 PHP 70 PowerShell. 153, 155 print デバッグ Prometheus. 43 110, 228 Python 71

283

R-Z | | あ RCA RDBMS Recall 177 アーカイブ 41. アイトラッキング 32 アクセス解析 116 241 29, 200 RFM分析 212 アクセス監査ログ 41 RNN 261 アクセス制限 117 rsyslog. 49 アクセスログ 13, 37 SEO 215 アクティビティモニタ 24 SHAP 271 アソシエーション分析 209 Shift JIS 17 アップセル 212 SIEM 75 圧力センサー 241 SKYSEA Client View 72 アプリケーションログ 34 SNMP 94 アンダーサンプリング 279 SP800-92. 46 異常検知 162 Splunk. 147, 154 異常終了 36 SVM 250 イベントビューアー 47 sysdiagnose 52 イベントログ 35 syslog- 48, 64 印刷ログ 35 System Support Lab 72 インデックス 133 TF-IDF. 249 インポート 87 TLS. 77 ウォード法 221 Tripwire. 197 映像解析 240 ULS 51 エージェント 93 Unixtime 16 エージェントレス 94 Urchin Tracking Module 215 エクスポート 90 USBメモリ 195 エラーメッセージ 22 UTF-8 17 エラーログ 39 UTM パラメータ解析 215 オートエンコーダ 263 Variety 31 オートスケーリング 237 Velocity. 31 オーバーサンプリング 279 Visual Studio Code. 126 Volume 31 - VPS 65 VS Code 126 改ざん 118 W3C 拡張ログ形式 38 改ざん検出 196 WAF 28 階層型クラスタリング 217 Web センサー 61 過学習 280 Webビーコン型 57 可逆压缩 115 Wi-Fi トラッキング 240 可視化ツール 158 Wireshark 55 仮名化 242 WMI 94 間隔尺度 253 WSL 125 監査ポリシー 192 Zabbix 96 監査ログ 41

284

監視カメラ

偽陰性

機械学習

160, 173

26

240

163

164, 246

キャッシュ

59,233 システム障害

246

強化学習

教師あり学習

20

教師なし学習

偽陽性

246

自然言語处理

シャーディング

246

163

協調フィルタリング

クエリログ

クラスタリング

212

41

217, 275

クラッシュログ

クロス集計

クロスセル

36

障害予測

266

208

クロスバリデーション

形態素解析

ゲートウェイ型

閾值

事後対応

システム管理

システムログ

160

視線追跡

33

31

241

135

真陰性

真陽性

163

163

221

280

スケーラビリティ

ステータスコード

134

スロークエリログ

73

決定木

交差検証

行動データ

236

正規表現

生成 Al

258

249

行動分析

行動ログ

行動ログデータ

42

正則化

39

129

166

セキュリティパッチ

240

280

セッション

204

272

240

顧客行動分析

故障の木解析

240

239

179

故障モード影響解析

個人情報保護法

コレスポンデンス分析

179

242

コンソールアプリ

コンテンツベースフィルタリング

混同行列

221

51

212

コンバージョン

250

コンバージョン率

設定変更ログ

全文検索エンジン

総当たり攻撃

207

207

コンピュータセキュリティログ管理ガイド 根本原因分析

46

177, 268

1

さ

サードパーティCookie

サーバーログ型

再帰型ニューラルネットワーク

再構成誤差

最短距離法

相関分析

261

263

チャネル

57

再発防止策

221

27

177

36

285

135

早期終了

181

174

た

タイムスタンプ

21.121

タグ

タスクスケジューラ

280

278

69

タスクマネージャー

チャットログ

24

31

214

直

直率。

205

1

データサイエンス

243 デシジョンツリー

205

29

データセキュリティ管理基準

113

データベース管理システム

40

デジタル署名

258

デバイス

デバッグログ

121

216

電話応対ログ

統計処理

43

31

161

統計的手法 統合ログ管理ツール 164 プロキシサーバー 59 75 平均セッション時間 204 特徵量 252 ページ/セッション 208 特徴量エンジニアリング 252 ページビュー 202 匿名化 242 変更監査ログ 41 トラブルシューティング 21 ホスト型 72 トランザクションログ 40 保存期間 113 トランスフォーマー 265 ポット 244 な ま なぜなぜ分析 178 マーケットバスケット分析 209 入退室ログ 42 前处理 251 ニューラルネットワーク 250 マルウェア 26, 185 認証ログ 36 身代金要求型ウイルス 185 ノーウェアランサム 186 名義尺度。 252 メモリダンプ 36 メモリ不足 は 22 バイアス 279 1 や パケットキャプチャ 54 パケットキャプチャ型 57 ユーザー 203 パケットキャプチャツール 55 ユーザーエージェント 37 パスワードリスト攻撃 183 ユニークユーザー 203 ハッシュ値 120 予兆検知 27 パフォーマンス解析 24 パフォーマンスモニター 24 1 ら ビーコン 59, 240 非可逆圧縮 非構造化データ 115 ライフログ 32 31 ランサムウェア 185 ビッグデータ 29 ランダムフォレスト 250, 258 表計算ソフト 152 リアルタイム解析 56 標準出力 43 離脫率 206 比例尺度 253 リバースブルートフォース攻撃 181 ファイアウォール 188 リバースプロキシ 227 ファシリティ 66 リファラー 37 フィッシュボーン図 179 ルールベース 164 フィルタリング 154 レベル 66 フォレンジック 164 連鎖障害 174 負荷分散装置 227, 236 ローカルストレージ 111 不正抑止 28 ローテーション 18 ブルートフォース攻撃 27, 181 ローテート 18 振る舞い検知 186 フローチャート 179

286

本書内容に関するお問い合わせについて

このたびは翔泳社の書籍をお買い上げいただき、誠にありがとうございます。 弊社では、読者の 皆様からのお問い合わせに適切に対応させていただくため、以下のガイドラインへのご協力をお 願いしております。 下記項目をお読みいただき、手順に従ってお問い合わせください。

お問い合わせされる前に

弊社Webサイトの「正誤表」をご参照ください。 これまでに判明した正誤や追加情報を掲載し ています。

正誤表 https://www.shoeisha.co.jp/book/errata/

お問い合わせ方法

弊社Webサイトの 「書籍に関するお問い合わせ」 をご利用ください。

書籍に関するお問い合わせ https://www.shoeisha.co.jp/book/qa/

インターネットをご利用でない場合は、FAXまたは郵便にて、下記”(株)翔泳社愛読者サービ スセンター”までお問い合わせください。 電話でのお問い合わせは、お受けしておりません。

回答について

回答は、お問い合わせいただいた手段によってご返事申し上げます。 お問い合わせの内容によ っては、回答に数日ないしはそれ以上の期間を要する場合があります。

お問い合わせに際してのご注意 本書の対象を超えるもの、 記述個所を特定されないもの、また読者固有の環境に起因するお問 い合わせ等にはお答えできませんので、予めご了承ください。

郵便物送付先およびFAX番号 送付先住所 〒160-0006 東京都新宿区舟町5 FAX 番号 宛先 03-5362-3818 (株)翔泳社愛読者サービスセンター

※本書に記載された URL等は予告なく変更される場合があります。

※本書の出版にあたっては正確な記述につとめていますが、 著者および株式会社翔泳社のいずれも、本書の 内容に対してなんらかの保証をするものではなく、内容やサンプルに基づくいかなる運用結果に関しても いっさいの責任を負いません。

※本書に掲載されているサンプルプログラムやスクリプト、および実行結果を記した画面イメージなどは、 特定の設定に基づいた環境にて再現される一例です。

※本書に記載されている会社名、製品名はそれぞれ各社の商標および登録商標です。

287

PROFILE 著者プロフィール

増井 敏克(ますいとしかつ)

增井技術士事務所 代表

技術士(情報工学部門)

1979年奈良県生まれ。 大阪府立大学大学院修了。 テクニカルエンジニア(ネットワーク、 情報セキュ リティ)、その他情報処理技術者試験にも多数合格。

また、ビジネス数学検定1級に合格し、 公益財団法人日本数学検定協会認定トレーナーとしても活動。 「ビジネス」×「数学」 × 「IT」 を組み合わせ、 コンピュータを 「正しく」 「効率よく」 使うためのス キルアップ支援や、 各種ソフトウェアの開発を行っている。

著書に「実務で使えるメール技術の教科書』、 『実務で役立つバックアップの教科書』、 『図解まるわ かりセキュリティのしくみ』、 『図解まるわかり データサイエンスのしくみ」、 「IT用語図鑑』 (以上、 翔泳社)、『基礎からのWeb開発リテラシー」、 「Obsidian で " 育てる"最強ノート術」 (以上、 技術評 論社) 『1週間でシステム開発の基礎が学べる本』 データ分析に強くなるSQLレシピ」 (以上、イ ンプレス) などがある。

装丁本文デザイン 大下賢一郎 DTP BUCH+

実務で役立つログの教科書

基礎知識から収集方法 分析手法 トラブルシューティング・パフォーマン ス最適化 機械学習での活用まで

2025年11月25日 初版第1刷発行

著者 增井 敏克 発行人 臼井かおる 発行所 株式会社翔泳社(https://www.shoeisha.co.jp) 印刷製本 日経印刷株式会社

© 2025 Toshikatsu Masui

本書は著作権法上の保護を受けています。 本書の一部または全部について (ソフトウェアおよびプログラムを含む)、 株式会社翔泳社から文書による許諾を得ずに、いかなる方法においても無断で複写、複製することは禁じられて います。 本書へのお問い合わせについては、287ページに記載の内容をお読みください。 本には細心の注意を払っておりますが、 万一、 乱丁 (ページの順序違い)や落丁 (ページの抜け) がござい ましたら、 お取り替えいたします。 03-5362-3705 までご連絡ください。 ISBN 978-4-7981-9212-3

Printed in Japan

サーバー管理者、 セキュリティ担当者、開発者、

情報システム担当者、 Webサイト運営者にオススメ!

知っているようで知らない基本を学びなおす

実務で役立つ教科書シリーズ

メール技術

システム

実務で使える

メール技術の教科書

メールの

增井敏克 著

定価:本体2,380円+税

し

ISBN: 978-4-7981-8393-0

メール技術の歴史や、メールサーバーを構築するための手 順、 迷惑メールを防ぐための取り組みまで、体系的に解説。

バックアップ 実務で役立つ

教科書

バックアップの教科書

增井敏克著

定価:本体2,380円+税

ISBN: 978-4-7981-8838-6

データが消えてから後悔しないために。 基本の考え方やしくみから設定、 ツールまでを網羅。

ってのは? イメージ等に 社会貢献・サステナビリティへの 取り組み 「社では持続可能な社会への貢献のくわしくは 環境や人に対してさまざまな 取り組みをおこなっています。 https://www.shoeisha.co.jp/sustainability